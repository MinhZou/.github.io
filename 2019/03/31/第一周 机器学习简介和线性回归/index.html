<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next1.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next1.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习,学习笔记,人工智能,线性回归,">





  <link rel="alternate" href="/atom.xml" title="Minh's Blog" type="application/atom+xml">






<meta name="description" content="1.1 一个Kaggle竞赛优胜解决方案 一个Kaggle竞赛优胜解决方案 任务：Avazu点击率预估竞赛 Rank 2nd Owen Zhang的解法">
<meta name="keywords" content="机器学习,学习笔记,人工智能,线性回归">
<meta property="og:type" content="article">
<meta property="og:title" content="第一周 机器学习简介与线性回归">
<meta property="og:url" content="http://minhzou.top/2019/03/31/第一周 机器学习简介和线性回归/index.html">
<meta property="og:site_name" content="Minh&#39;s Blog">
<meta property="og:description" content="1.1 一个Kaggle竞赛优胜解决方案 一个Kaggle竞赛优胜解决方案 任务：Avazu点击率预估竞赛 Rank 2nd Owen Zhang的解法">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-04-06T04:03:56.736Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第一周 机器学习简介与线性回归">
<meta name="twitter:description" content="1.1 一个Kaggle竞赛优胜解决方案 一个Kaggle竞赛优胜解决方案 任务：Avazu点击率预估竞赛 Rank 2nd Owen Zhang的解法">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://minhzou.top/2019/03/31/第一周 机器学习简介和线性回归/">





  <title>第一周 机器学习简介与线性回归 | Minh's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/minhzou" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Minh's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">成长的路上每一步都算数</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://minhzou.top/2019/03/31/第一周 机器学习简介和线性回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Minh">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Minh's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">第一周 机器学习简介与线性回归</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-31T20:37:15+08:00">
                2019-03-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-04-06T12:03:56+08:00">
                2019-04-06
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/31/第一周 机器学习简介和线性回归/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/31/第一周 机器学习简介和线性回归/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  7.9k 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  33 min
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h5 id="1-1-一个Kaggle竞赛优胜解决方案"><a href="#1-1-一个Kaggle竞赛优胜解决方案" class="headerlink" title="1.1 一个Kaggle竞赛优胜解决方案"></a>1.1 一个Kaggle竞赛优胜解决方案</h5><ul>
<li>一个Kaggle竞赛优胜解决方案<ul>
<li>任务：Avazu点击率预估竞赛</li>
<li>Rank 2nd Owen Zhang的解法<a id="more"></a></li>
<li>优胜算法的特点<ul>
<li>特征工程</li>
<li>融合大法<ul>
<li>多层</li>
<li>多种不同模型的组合</li>
</ul>
</li>
<li>所以：<ul>
<li>基础模型很重要（线性模型）</li>
<li>集成学习模型单模型性能好（GBDT）</li>
<li>特定问题的模型贡献大（FM）</li>
<li>模型融合很重要</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>课程内容安排<ul>
<li>基本模型<ul>
<li>线性模型： 线性回归， logistic回归， SVM</li>
<li>非线性模型： （线性模型核化）、分类回归树</li>
<li>集成学习模型（随机森林、GBDT）</li>
<li>数据预处理：数据清洗，特征工程，降维，聚类</li>
</ul>
</li>
<li>模型融合</li>
<li>推荐系统/点击率预估问题特定解决方案</li>
</ul>
</li>
</ul>
<h5 id="1-2-机器学习任务类型"><a href="#1-2-机器学习任务类型" class="headerlink" title="1.2  机器学习任务类型"></a>1.2  机器学习任务类型</h5><ul>
<li>定义</li>
<li>数据<ul>
<li>数据通常以二维数据表形式给出<ul>
<li>每一行： 一个样本</li>
<li>每一列：一个属性/特征</li>
</ul>
</li>
<li>例：Boston房价预测数据，根据某地区房屋属性，预测该地区预测房价<ul>
<li>506行， 506个样本</li>
<li>14列</li>
</ul>
</li>
</ul>
</li>
<li>机器学习任务类型<ul>
<li>监督学习（Supervised Learning）<ul>
<li>分类（classfication）</li>
<li>回归（regression）</li>
<li>排序（ranking）</li>
</ul>
</li>
<li>非监督学习（unsupervised learning）<ul>
<li>聚类（clustering）</li>
<li>降维（dimensionality reduction）</li>
<li>概率密度估计（density estimation）</li>
</ul>
</li>
<li>增强学习（reinforcement learning）</li>
<li>半监督学习（semi-supervised learning）</li>
<li>迁移学习（transfer learning）</li>
<li>……</li>
</ul>
</li>
<li>监督学习<br>  学习一个x-&gt;y 的映射f, 从而对新输入的x进行预测f（x）<script type="math/tex; mode=display">D = \{X_i,y_i\}^N_{i=1}</script>  D：训练数据集<br>  N：训练样本数目<br>  $X_i$: 第i个样本的输入，亦被称为特征、属性或协变量<br>  $y_i$: 第i个训练样本的输出，亦被称为响应，如类别标签、序号或数值<br>  例：波士顿房价预测<ul>
<li>回归<ul>
<li>若输出y∈R为连续值，则我们称之为一个回归（regression）任务<br>例： 房价预测，预测二手车的价格</li>
<li>假设回归模型：$y = f(\mathbf x|\theta)$<ul>
<li>如在线性回归中，$f(\mathbf x|w) = \mathbf w^T \mathbf x$</li>
</ul>
</li>
<li>训练：根据训练数据 $D = \{\mathbf X_i,y_i\}^N_{i=1}$ 学习映射</li>
<li>预测：对新的测试数据x进行预测：$\hat f = f(x)$ y带帽表示预测</li>
<li>学习目标：训练集上预测值与真值之间的差异最小<ul>
<li>损失函数：度量模型预测值与真值之间的差异，如<script type="math/tex; mode=display">L(f(\mathbf x),y) = \frac 12(f(x) - y)^2</script></li>
<li>目标函数为：$J(\mathbf \theta) = \frac1N \sum_{i = 1}^N L(f(\mathbf x_i|\mathbf \theta), y_i)$</li>
</ul>
</li>
</ul>
</li>
<li>分类<br>   若输出y为离散值，则我们称之为一个分类，标签空间y = {1,2, … C}<br>   例：信用评分<ul>
<li>分类： 学习从输入x到输出y的映射f:概率问题<br>$\hat y = f(\mathbf x) = \underset{c}  {arg\ max} \ p(y = c\mid \mathbf x, D)$</li>
<li>学习目标：<ul>
<li>损失函数：01损失 <script type="math/tex; mode=display">l_{0/1}(y, \hat y) = \begin {cases} 0 & y = \hat y \\ 1 & otherwise \end{cases}</script></li>
</ul>
</li>
<li>需要预测的概率：</li>
<li>预测：最大后验估计（Maximum a Posteriori, MAP）<br>$\hat y = f(\mathbf x) = \underset{c} {arg\ max}\ p(y = c\mid \mathbf x, D)$</li>
</ul>
</li>
<li>排序（Rank）<br>  排序学习是推荐、搜素、广告的核心方法<br>  排序学习中需要首先根据查询q及其文档集合进行标注（data labeling） 和提取特征（feature extraction） 才能得到D = {….}</li>
</ul>
</li>
<li>非监督学习<br>  发现数据中的“有意义的模式”， 亦被称为知识发现<br>  训练数据不包含标签<br>  标签在训练数据中为隐含变量<br>  $ D = \{ \bf X_i\}_{ i= 1}^ N $<ul>
<li>聚类<br>例：人的“类型”<br>分多少类？ 模型选择<br>$ K^* = arg\ max _K\ p(K \mid D)$<br>某个样本属于哪个类？</li>
<li>降维<br>多维特征，有些特征之间会相关而存在冗余<br>很多算法中，降维算法成为了数据预处理的一部分， 如主成分分析（Principal Components Analysis, PCA）</li>
</ul>
</li>
<li>半监督学习<br>  当标注数据“昂贵”时有用<br>  例：标注3D姿态、 蛋白质功能等等</li>
<li>多标签学习</li>
<li>有歧义标签学习</li>
<li>多实例学习</li>
<li>增强学习<br>从行为的反馈(奖励或惩罚)中学习<ul>
<li>设计一个回报函数（reward function）， 如果learning agent(如机器人、围棋ai程序)，在决定一步之后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负</li>
<li>增强学习的任务：找到一条回报值最大的路径</li>
</ul>
</li>
</ul>
<h5 id="1-3-一个典型的机器学习案例-对鱼进行分类"><a href="#1-3-一个典型的机器学习案例-对鱼进行分类" class="headerlink" title="1.3 一个典型的机器学习案例-对鱼进行分类"></a>1.3 一个典型的机器学习案例-对鱼进行分类</h5><ul>
<li>根据一些光学传感器对传送带上的鱼进行分类</li>
<li>形式化为机器学习问题<ul>
<li>训练数据<ul>
<li>每条鱼的测量向量</li>
<li>每条鱼的标签</li>
</ul>
</li>
<li>测试<ul>
<li>给定一个新的特征向量x</li>
<li>预测对应的标签y </li>
</ul>
</li>
<li>将长度作为特征进行分类（直方图）<ul>
<li>需要先做一个决策边界<ul>
<li>最小化平均损失</li>
</ul>
</li>
</ul>
</li>
<li>将亮度作为特征进行分类 （直方图）</li>
<li>将长度和亮度一起作为特征（二维散点图）<ul>
<li>线性决策函数</li>
<li>二次决策函数</li>
<li>更复杂的决策边界<br>训练集上的误差 ≠ 测试集上的误差<br>数据过拟合（overfitting）<br>推广性（generalization）差</li>
</ul>
</li>
</ul>
</li>
<li>小结：设计一个鱼的分类器<ul>
<li>选择特征<ul>
<li>可能是最重要的步骤！（收集训练数据）</li>
</ul>
</li>
<li>选择模型（如决策边界的形状）</li>
<li>根据训练数据估计模型</li>
<li>利用模型对新样本进行分类</li>
</ul>
</li>
</ul>
<h5 id="1-4-机器学习算法的组成部分"><a href="#1-4-机器学习算法的组成部分" class="headerlink" title="1.4 机器学习算法的组成部分"></a>1.4 机器学习算法的组成部分</h5><ul>
<li>机器学习任务的一般步骤<ul>
<li>确定特征<ul>
<li>可能是最重要的步骤！（收集训练数据）</li>
</ul>
</li>
<li>确定模型<ul>
<li>目标函数/决策边界形状</li>
</ul>
</li>
<li>模型训练：根据训练数据估计模型参数<ul>
<li>优化计算</li>
</ul>
</li>
<li>模型评估：在校验集上评估模型预测性能</li>
<li>模型应用/预测 </li>
</ul>
</li>
<li>模型<ul>
<li>监督学习任务：$D = \{X_i, y_i\} _{i = 1} ^ N $</li>
<li>模型：对给定的输入x, 如何预测其标签$ \hat y$<ul>
<li>不同模型对数据的假设不同</li>
<li>最简单的模型：线性模型$ f(x) = \sum_j w_j x_j = \bf w^T \bf x$</li>
</ul>
</li>
<li>确定模型类别后，模型训练转化为求解模型参数<ul>
<li>如对线性模型参数为$\theta = \{w_j \mid j = 1,…, D\}$,其中D为特征维数</li>
</ul>
</li>
<li>求解模型参数：目标函数最小化</li>
</ul>
</li>
<li>非线性模型<ul>
<li>基函数： $x^2$, log, exp, 样条函数，决策树….</li>
<li>核化：将原问题转化为对偶问题，将对偶问题中的向量积$\langle x_i, x_j\rangle$ 换成核函数$k(x_i,x_j)$</li>
</ul>
</li>
<li>目标函数：通常包含两项：损失函数和正则项<script type="math/tex; mode=display">J(\theta) = \frac 1N \sum_{i=1}^N\ L(f(x_i; \theta), y_i) + R(\theta)</script><ul>
<li>损失函数<ul>
<li>损失函数 - 回归<ul>
<li>损失函数：度量模型预测值与真值之间的差异</li>
<li>对回归问题：令残差 $r = f(\bf x) - y$<ul>
<li>L2损失：连续，但对噪声敏感<script type="math/tex; mode=display">L_2 (r) = \frac 12 r ^2</script></li>
<li>L1损失：不连续，对噪声不敏感<script type="math/tex; mode=display">L_1(r) = |r|</script></li>
<li>Huber 损失： 连续，对噪声不敏感<script type="math/tex; mode=display">L_\delta (r) = \begin{cases}  \frac 12 r^2 & if|r| \le \delta\\ \delta |r| - \frac 12 \delta^2 & if|r| \ge \delta\end{cases}</script></li>
</ul>
</li>
</ul>
</li>
<li>损失函数 - 分类<ul>
<li>损失函数：度量模型预测值与真值之间的差异</li>
<li>对分类问题<ul>
<li>0-1损失：$l_{0/1}(y,f(x)) = \begin{cases} 1 &amp; yf(x) \lt 0 \\ 0 &amp; othereise\end{cases}$</li>
<li>logistic损失：亦称负log似然损失<br>  $l_{log}(y,f(x)) = log(1 + exp(-yf(x)))$</li>
<li>指数损失：$l_{exp}(y,f(x)) = exp(-yf(x))$</li>
<li>合页损失：$l_{hinge}(y,f(x)) = max(0, 1 - yf(x))$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>正则项<br>  复杂模型（预测）不稳定：方差大<br>  正则项对复杂模型施加惩罚<ul>
<li>正则项的必要性<br>例：sin曲线拟合</li>
<li>增加L2正则<br>岭回归：最小化RSS</li>
<li>欠拟合：模型太简单/对复杂性惩罚太多</li>
<li>样本数目增多时，可以考虑更复杂的模型</li>
<li>常见正则项<ul>
<li>L2正则: $R(\theta) = \lambda ||\theta||^2_2 = \lambda \sum^D_{j=1} \theta_j^2$</li>
<li>L1正则: $R(\theta) = \lambda |\theta| = \lambda \sum ^D_{j=1}|\theta_j|$</li>
<li>L0正则: $R(\theta) = \lambda||\theta||_ 0$<ul>
<li>非0参数的数目</li>
<li>不好优化，通常用L1正则近似</li>
</ul>
</li>
</ul>
</li>
<li>常见线性模型的损失和正则项组合</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>L2损失</th>
<th>L1损失</th>
<th>Huber损失</th>
<th>Logistic损失</th>
<th>合页损失</th>
<th>e-insensitive损失</th>
</tr>
</thead>
<tbody>
<tr>
<td>L2正则</td>
<td>岭回归</td>
<td></td>
<td></td>
<td>L2正则 Logistic回归</td>
<td>SVM</td>
<td>SVR</td>
</tr>
<tr>
<td>L1正则</td>
<td>LASSO</td>
<td></td>
<td></td>
<td>L1正则 Logistic回归</td>
<td></td>
<td></td>
</tr>
<tr>
<td>L2+L1正则</td>
<td>Elastic</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>模型训练<ul>
<li>在训练数据上求目标函数极小值：优化</li>
<li>简单目标函数直接求解<ul>
<li>如小数据集上的线性回归</li>
</ul>
</li>
<li>更复杂问题：凸优化<ul>
<li>（随机）梯度下降</li>
<li>牛顿法/拟牛顿法</li>
<li>… </li>
</ul>
</li>
</ul>
</li>
<li>梯度下降（Gradient Descent）算法<ul>
<li>梯度下降/最速下降算法：快速寻找函数局部极小值</li>
<li>梯度下降算法：求函数J（θ）的最小值<ul>
<li>给定初始值$θ^0$</li>
<li>更新θ，使得J（θ）越来越小<ul>
<li>$θ^t = θ^{t-1} - \eta\nabla J(θ)$ ( $\eta$ : 学习率 )</li>
</ul>
</li>
<li>直到收敛到 / 达到预先设定的最大迭代次数</li>
<li>下降的步伐太小（学习率）非常重要：如果太小，收敛速度慢； 如果太大，可能会出现overshoot the minimum的现象</li>
<li>梯度下降求得的只是局部最小值<ul>
<li>二阶导数 &gt; 0, 则目标函数为凸函数，局部极小值即为全局最小值</li>
<li>随机选择多个初始值，得到函数的多个局部极小值点。多个局部极小值点的最小值为函数的全局最小值</li>
</ul>
</li>
<li>梯度下降算法每次学习都使用整个训练集，这样对大的训练数据集合，每次学习时间过长，对大的训练集需要消耗大量的内存。此时可采用随机梯度下降（Stochastic gradient descent, SGD), 每次从训练集中随机选择一部分样本进行学习。</li>
<li>更多（随机）梯度下降算法的改进版<ul>
<li>动量（Momentum）</li>
<li>Nesterov accelerated gradient (NAG)</li>
<li>Adagrad</li>
<li>RMSprop</li>
<li>Adaptive Moment Estimation (Adam)…</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>模型选择与模型评估<ul>
<li>同一个问题有不同的解决方案<br>  如线性回归 vs. 决策树</li>
<li>哪个更好？ 模型评估与模型选择<ul>
<li>在新数据点的预测误差最小</li>
</ul>
</li>
<li>模型评估：已经选定最终的模型，估计它在新数据上的预测误差</li>
<li>模型选择：估计不同模型的性能，选出最好的模型</li>
</ul>
</li>
<li>样本足够多：训练集和校验集</li>
<li>样本不够多：重采样技术来模拟校验集：交叉验证和bootstrap<ul>
<li>K-折交叉验证<ul>
<li>交叉验证（Cross Validation, CV）： 将训练数据分成容量大致相等的K份（通常K = 5/10）</li>
<li>交叉验证估计的误差为：<script type="math/tex; mode=display">CV(M)= \frac1K \sum  ^K_{k = 1} E_k(M)</script></li>
</ul>
</li>
</ul>
</li>
<li>模型选择<ul>
<li>对多个不同模型，计算其对应的误差CV（M）， 最佳模型为CV（M）最小的模型</li>
<li>模型复杂度和泛化误差的关系通常是U形曲线：</li>
</ul>
</li>
</ul>
<h5 id="1-5-学习环境简介"><a href="#1-5-学习环境简介" class="headerlink" title="1.5 学习环境简介"></a>1.5 学习环境简介</h5><ul>
<li>编程语言 Python</li>
<li>数据处理工具包<ul>
<li>Numpy</li>
<li>SciPy</li>
<li>pandas</li>
</ul>
</li>
<li>数据可视化工具包<ul>
<li>Matplotlib</li>
<li>Seaborn</li>
</ul>
</li>
<li>机器学习工具包<ul>
<li>scikit learn</li>
</ul>
</li>
<li>示例代码：INotebook </li>
<li>NumPy<ul>
<li>NumPy(Numeric Python)是Python的开源数值计算扩展，可用来存储和处理大型矩阵</li>
<li>Numpy包括：<ul>
<li>N维数组(ndarray)</li>
<li>实用的线性代数、傅里叶变换和随机数生成函数</li>
</ul>
</li>
<li>Numpy和稀疏矩阵运算包SciPy配合使用更加方便</li>
</ul>
</li>
<li>SciPy<ul>
<li>SciPy是建立在NumPy的基础上、是科学和工程设计的Python工具包，提供统计、优化和数值微积分计算等功能</li>
<li>NumPy 处理$10^6$级别的数据通常没有大问题，但当数据量达到$10^7$级别时速度开始发慢，内存受到限制（具体情况取决于实际内存的大小）</li>
<li>当处理超大规模数据集，比如$10^{10}$级别，且数据中包含大量的0时，可采用稀疏矩阵可显著的提高速度和效率</li>
</ul>
</li>
<li>Pandas(<strong>Pan</strong>del <strong>da</strong>ta structures)<ul>
<li>Pandas是Python语言的“关系型数据库”数据结构和数据分析工具，非常高效且易于使用<ul>
<li>基于NumPy补充了大量数据操作功能，能实现统计、分组、排序、透视表(SQL语句的大部分功能)</li>
<li>Pandas主要有2种重要的数据类型<ul>
<li>series：一维序列</li>
<li>DataFrame：二维表(机器学习数据的常用数据结构)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Matplotlib<ul>
<li>Matplotlib是Python语言的2D图形绘制工具</li>
</ul>
</li>
<li>Seaborn<ul>
<li>Seaborn是一个基于Matplotlib的Python可视化工具包，提供更高层次的用户接口，可以给出漂亮的数据统计</li>
</ul>
</li>
<li>Scikit - Learn<ul>
<li>Machine Learning in Python</li>
<li>Scikit-Learn是基于Python的开源机器学习模块，最早于2007年由David Cournapeau发起</li>
<li>基本功能有六部分：分类（Classification），回归（Regression），聚类（Clustering），数据降维（Dimensionality reduction），模型选择（Model Selection），数据预处理（Preprocessing）</li>
<li>对于具体的机器学习问题，通常可以分为三个步骤<ul>
<li>数据准备与预处理（Preprocessing, Dimensionality reduction）</li>
<li>模型选择与训练（Classification, Regression, Clustering）</li>
<li>模型验证与参数调优（Model Selection）</li>
</ul>
</li>
</ul>
</li>
<li>各种机器学习模型有统一的接口</li>
<li>模型既有默认参数，也提供多种参数调优方法</li>
<li>卓越的文档</li>
<li>丰富的随附任务功能集合</li>
<li>活跃的社区提供开发和支持</li>
</ul>
<h5 id="1-6-线性回归模型"><a href="#1-6-线性回归模型" class="headerlink" title="1.6 线性回归模型"></a>1.6 线性回归模型</h5><ul>
<li>目标函数通常包含两项：损失函数和正则项<script type="math/tex; mode=display">J(\bf \theta) = \frac1N \sum_{i = 1}^N L(f(\bf x_i|\bf \theta), y_i) + \lambda R(\bf \theta)</script></li>
<li>对回归问题，损失函数可以采用L2损失，得到<script type="math/tex; mode=display">\begin{eqnarray}J(\theta) 
   &=&\sum_{i=1}^NL(y_i,\hat y_i) \\
   &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\
   &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2
  \end{eqnarray}</script>  残差平方和（residual sum of squares, RSS）</li>
<li>由于线性模型比较简单，实际应用中有时正则项为空，得到最小二乘线性回归（Ordinary Least Square, OLS）<script type="math/tex; mode=display">\begin{eqnarray}J(\theta) 
  &=&\sum_{i=1}^NL(y_i,\hat y_i) 
  &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\
  &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2
  \end{eqnarray}</script></li>
<li>正则项可以为L2正则，得到岭回归（Ridge Regression）<script type="math/tex; mode=display">J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda ||w||^2_2</script></li>
<li><p>正则项也可以选L1正则，得到Lasso模型：</p>
<script type="math/tex; mode=display">J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda |w|</script><ul>
<li>当$\lambda$取合适值时，Lasso（Least absolute shrinkage and selection operator）的结果是稀疏的（w的某些元素系数为0），起到特征选择作用</li>
</ul>
</li>
<li><p>为什么L1正则的解是稀疏的？</p>
</li>
<li>线性回归模型的概率解释<ul>
<li>最小二乘（线性）回归等价于极大似然估计<ul>
<li>假设：$ y = f(\bf x) + \epsilon = w^Tx + \epsilon $<br>其中$\epsilon$为线性预测和真值之间的残差<br>我们通常假设残差的分布为$\epsilon \sim N(0,\sigma ^2)$,因此线性回归可写成：$p(y|x,\theta) \sim N(y| \bf w^T \bf x, \sigma^2)$,其中$ \bf \theta = (\bf w, \sigma ^2)$</li>
</ul>
</li>
<li>正则（线性）回归等价于高斯先验（L2正则）或Laplace先验下（L1正则）的贝叶斯估计</li>
</ul>
</li>
<li>Recall：极大似然估计<ul>
<li>极大似然估计（Maximize Likelihood Estimator, MLE）定义为<script type="math/tex; mode=display">\hat \theta = \underset {\theta}  {arg\ max}\ log\ p(D\mid \theta)</script></li>
<li>其中（log）似然函数为<script type="math/tex; mode=display">l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta)</script></li>
<li>表示在参数为$\theta$的情况下，数据$D ={\bf x_i,y_i}^N_{i=1}$</li>
<li>极大似然：选择数据出现概率最大的参数</li>
</ul>
</li>
<li>线性回归的MLE<script type="math/tex; mode=display">p(y_i|x_i,\bf w,\sigma ^2) \sim N(y_i\mid \bf w^T \bf x_i, \sigma^2) = \frac 1{\sqrt{2\pi}\sigma} exp(-\frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))</script><ul>
<li>OLS的似然函数为<script type="math/tex; mode=display">l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta)</script></li>
<li>极大似然可等价地写成极小负log似然损失（negative log likelihood, NLL）</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\begin{eqnarray}{NLL(\bf \theta)}
    &=& \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) \\
    &=& - \sum_{i=1}^N log ((\frac 1{2 \pi \sigma^2})^ \frac 12 exp(- \frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))) \\
    &=& \frac N2 log(2\pi \sigma ^2) + \frac 1{2 \sigma^2} \sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2
    \end{eqnarray}</script><ul>
<li>正则回归等价于贝叶斯估计<ul>
<li>假设残差的分布为$\epsilon \sim N(0, \sigma ^2)$,线性回归可写成：<br>$p(y_i \mid \bf x_i, \theta) \sim N(y_i \mid \bf w^T \bf x_i，\sigma ^2)$<br>$p(y\mid \bf X, \bf w, \sigma ^2) = N(\bf y \mid \bf X \bf w, \sigma ^2 \bf I_N) \propto exp(- \frac 1{2\sigma ^2}((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w)))$</li>
<li>若假设参数为w的先验分布为 $w_j \sim N(0, \tau ^2)$<ul>
<li>偏向较小的系数值，从而得到的曲线也比较平滑<br>$p(\bf w) =\prod_{j=1}^{D} N(w_j \mid 0, \tau ^2) \propto exp(- \frac 1{2\tau^2} \sum_{j=1}^D \bf w_j^2 = exp(- \frac 1{2\tau^2} ( \bf w^T \bf w ) )) $</li>
<li>其中$1/\tau ^2$控制先验的强度</li>
</ul>
</li>
<li>根据贝叶斯公式，得到参数的后验分布为<br>$p(y\mid \bf X, \bf w, \sigma ^2) = \propto exp(- \frac 1{2\sigma ^2} ((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w) ) - \frac 1{2 \tau^2} ( w^Tw ) )$</li>
<li>则最大后验估计(MAP)等价于最小目标函数<br>$J(\bf w) = (\bf y - \bf X\bf w)^T(\bf y - \bf X\bf w) + \frac {\sigma ^2}{\tau^2} \bf w^T \bf w $</li>
<li>对比岭回归的目标函数<br>$J(\bf w) = \sum_{i=1}^N(y_i -\bf w^T\bf x_i)^2 + \lambda \Vert \bf w\Vert ^2_2$</li>
</ul>
</li>
<li>小结<ul>
<li>线性回归模型可以放到机器学习一般框架<ul>
<li>损失函数：L2损失，…</li>
<li>正则：无正则， L2正则，L1正则…</li>
</ul>
</li>
<li>正则回归模型可视为先验为正则、似然为高斯分布的贝叶斯估计<ul>
<li>L2正则：先验分布为高斯分布</li>
<li>L1正则：先验分布为Laplace分布</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="1-7-线性回归模型-优化算法"><a href="#1-7-线性回归模型-优化算法" class="headerlink" title="1.7 线性回归模型-优化算法"></a>1.7 线性回归模型-优化算法</h5><ul>
<li>线性回归的目标函数<ul>
<li>无正则的最小二乘线性回归（Ordinary Least Square, OLS）：<script type="math/tex; mode=display">J(w) = \sum_{i=1}^N(y_i - w^Tx_i)^2</script></li>
<li>L2正则的岭回归（Ridge Regression）模型：<script type="math/tex; mode=display">J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D w_j^2</script></li>
<li>L1正则的Lasso模型：<script type="math/tex; mode=display">J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D |w_j|</script></li>
</ul>
</li>
<li>模型训练：<br>  根据训练数据求目标函数取极小值的参数：<br>  $\hat w = \underset {w} {arg\ min} J(\bf w)$<ul>
<li>目标函数的最小值：<ul>
<li>一阶的导数为0：$\frac{\partial J(w)} {\partial w}$</li>
<li>二阶导数&gt;0：$\frac{\partial J^2(w)} {\partial w^2}$</li>
</ul>
</li>
</ul>
</li>
<li>OLS的优化求解：<ul>
<li>OLS的优化求解<ul>
<li>OLS的目标函数写成矩阵形式：<br>$J(w) = \sum ^N_{i=1}(y_i - w^Tx_i)^2 = (y - Xw)^T(y - Xw)$</li>
<li>只取与w有关的项，得到<br>$J(w) = w^T(X^TX)w - 2w^T(X^Ty)$</li>
<li>求导  $\frac{\partial J(w)} {\partial w} = 2X^TXw - 2X^Ty = 0 \Rightarrow X^TXw = X^Ty$`<br>$\hat w_{OLS} = (X^TX)^{-1}X^Ty$</li>
</ul>
</li>
<li>OLS的优化求解 ——SVD<ul>
<li>OLS目标函数：$J(w) = \Vert y - Xw\Vert_2^2$</li>
<li>相当于求 $y = Xw$</li>
<li>如果X为方阵，可求逆：$w = X^{-1}y$</li>
<li>如果𝐗不是方阵，可求Moore-Penrose广义逆：$𝐰 = 𝐗^{\dagger }𝐲$。</li>
<li>Moore-Penrose广义逆可采用奇异值分解(Singular Value Decomposition)<br>实现：<br>奇异值分解：$X = U \Sigma V^T$<br>$X^{\dagger } = V \Sigma ^{\dagger} U^T$<br>其中 $\Sigma = \begin{pmatrix}<br>{\sigma_1}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>{0}&amp;{\sigma_2}&amp;{\cdots}&amp;{0}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{0}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>\end{pmatrix}$,$\Sigma ^{\dagger} = \begin{pmatrix}<br>{\frac {1}{\sigma_1}}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>{0}&amp;{\frac{1}{\sigma_2}}&amp;{\cdots}&amp;{0}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{0}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>\end{pmatrix}$</li>
</ul>
</li>
<li>OLS的优化求解——梯度下降<ul>
<li>OLS目标函数：<br>$J(w) = (y - Xw)^T(y - Xw)$<br>梯度：$\nabla_w = - 2X^T(y - Xw^t)$<br>参数更新：<br>$w^{t+1} = w^t - \eta\nabla_w = w^t + 2\eta X^T(y - Xw^t)$</li>
</ul>
</li>
</ul>
</li>
<li>岭回归的优化求解<ul>
<li>岭回归的目标函数与OLS只相差一个正则项（也是w的二次函数）</li>
<li>岭回归的优化求解——SVD</li>
</ul>
</li>
<li>Lasso的优化条件<ul>
<li>软&amp; 硬阈值</li>
<li>Lasso的优化求解——坐标轴下降法<ul>
<li>为了找到一个函数的局部极小值，在每次迭代中可以在当前点处沿一个坐标方向进行一维搜索。</li>
</ul>
</li>
<li>整个过程中循环使用不同的坐标方向。一个周期的一维搜索迭代过程相当于一个梯度迭代。</li>
<li>注意：<ul>
<li>梯度下降方法是利用目标函数的导数（梯度）来确定搜索方向的，而该梯度方向可能不与任何坐标轴平行。</li>
<li>而坐标轴下降法是利用当前坐标系统进行搜索，不需要求目标函数的导数，只按照某一坐标方向进行搜索最小值。（在稀疏矩阵上的计算速度非常快，同时也是Lasso回归最快的解法）</li>
</ul>
</li>
</ul>
</li>
<li>小结<ul>
<li>线性回归模型比较简单<ul>
<li>当数据规模比较小时，可直接解析求解<ul>
<li>scikit learn中的实现采用SVD分解实现</li>
</ul>
</li>
<li>当数据规模较大时，可采用随机梯度下降<ul>
<li>scikit learn提供一个SGDRegression类</li>
</ul>
</li>
</ul>
</li>
<li>岭回归求解类似OLS，采用SVD分解实现</li>
<li>Lasso优化求解采用坐标轴下降法</li>
</ul>
</li>
</ul>
<h5 id="1-8-线性回归模型-模型选择"><a href="#1-8-线性回归模型-模型选择" class="headerlink" title="1.8 线性回归模型-模型选择"></a>1.8 线性回归模型-模型选择</h5><ul>
<li>模型评估与模型选择<ul>
<li>模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果<ul>
<li>校验集划分（train_test_split、交叉验证）</li>
<li>评价指标（sklearn.metrics）</li>
</ul>
</li>
<li>模型选择：<ul>
<li>模型中通常有一些超参数，需要通过模型选择来确定<ul>
<li>线性回归模型中的正则参数</li>
<li>OLS中的特征的数目</li>
</ul>
</li>
<li>参数搜索范围：网格搜索（GridSearch）</li>
</ul>
</li>
<li>Scikit learn将交叉验证与网格搜索合并为一个函数</li>
</ul>
</li>
<li>评价准则<ul>
<li>模型训练好后，可用一些度量准则检查模型拟合的效果<ul>
<li>开方均方误差（rooted mean squared error，RMSE）:$RMSE = \sqrt{\frac 1N \sum_{i=1}^N(\hat y_i - y_i)^2}$`</li>
<li>平均绝对误差（mean absolute error，MAE）：$MAE = \frac 1N \sum_{i=1}^N|\hat y_i - y_i|$</li>
<li>R2 score：既考虑了预测值与真值之间的差异，也考虑了问题本身真值之<br>间的差异（ scikit learn 线性回归模型的缺省评价准则）$SS_{res} = \sum_{i=1}^N(\hat y_i - y_i)^2, SStot = \sum_{i=1}^N(y_i - \bar{y})^2, R^2 = 1 - \frac {SS_{res}}{SS_{tot}})$</li>
</ul>
</li>
<li>也可以检查残差的分布</li>
<li>还可以打印预测值与真值的散点图</li>
</ul>
</li>
<li>线性回归中的模型选择<br>Scikit learn中的model selection模块提供模型选择功能<ul>
<li>对于线性模型，留一交叉验证（N折交叉验证，亦称为leave-oneout cross-validation，LOOCV）有更简便的计算方式，因此Scikit learn提供了RidgeCV类和LassoCV类实现了这种方式</li>
<li>后续课程将讲述一般模型的交叉验证和参数调优GridSearchCV</li>
<li>RidgeCV<ul>
<li>RidgeCV中超参数λ用alpha表示</li>
<li>RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_c<br>v_values=False)</li>
</ul>
</li>
<li>LassoCV<ul>
<li>LassoCV的使用与RidgeCV类似</li>
<li>Scikit learn还提供一个与Lasso类似的LARS（least angle regression，最小角回归），二者仅仅是优化方法不同，目<br>标函数相同。</li>
<li>当数据集中特征维数很多且存在共线性时，LassoCV更合适。</li>
</ul>
</li>
</ul>
</li>
<li>小结：线性回归之模型选择<ul>
<li>采用交叉验证评估模型预测性能，从而选择最佳模型<ul>
<li>回归性能的评价指标</li>
<li>线性模型的交叉验证通常直接采用广义线性模型的留一交叉验证进行快速模型评估<ul>
<li>Scikit learn中对RidgeCV和LassoCV实现该功能</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="1-9-波士顿房价预测案例详解——数据探索"><a href="#1-9-波士顿房价预测案例详解——数据探索" class="headerlink" title="1.9 波士顿房价预测案例详解——数据探索"></a>1.9 波士顿房价预测案例详解——数据探索</h5><ul>
<li>第一步：理解任务，准备数据<ul>
<li>数据读取<ul>
<li>Pandas支持多种格式的数据</li>
</ul>
</li>
<li>数据探索&amp;特征工程<ul>
<li>数据规模</li>
<li>确定数据类型，是否需要进一步编码<ul>
<li>特征编码</li>
</ul>
</li>
<li>数据是否有缺失值<ul>
<li>数据填补</li>
</ul>
</li>
<li>查看数据分布，是否有异常数据点<ul>
<li>离群点处理</li>
</ul>
</li>
<li>查看两两特征之间的关系，看数据是否有冗余/相关<ul>
<li>降维</li>
</ul>
</li>
</ul>
</li>
<li>数据概览<ul>
<li>pandas:DataFrame<ul>
<li>Head():数据前5行，可查看每一列的名字及数据类型</li>
<li>Info():<ul>
<li>数据规模：行数&amp;列数</li>
<li>每列的数据类型、是否有空值</li>
<li>占用存储量</li>
</ul>
</li>
<li>shape:行数&amp;列数</li>
</ul>
</li>
</ul>
</li>
<li>各属性的统计特性<ul>
<li>直方图<br>  每个取值在数据集中出现的样本数目 </li>
<li>离群点<ul>
<li>离群点：奇异点（outlier）,指远离大多数样本的样本点。通常认为这些点是噪声，对模型有坏影响</li>
</ul>
</li>
<li>相关性<ul>
<li>相关性：相关性可以通过计算相关系数或打印散点图来发现</li>
<li>相关系数：</li>
<li>散点图<ul>
<li>可以通过两个变量之间的散点图直观感受二者的相关性</li>
</ul>
</li>
<li>数据预处理<ul>
<li>数据标准化（ Standardization ）<ul>
<li>某个特征的所有样本取值为0均值、1方差</li>
</ul>
</li>
<li>数据归一化（ Scaling ）<ul>
<li>某个特征的所有样本取值在规定范围内</li>
</ul>
</li>
<li>数据正规化（ Normalization ）<ul>
<li>每个样本模长为1</li>
</ul>
</li>
<li>数据二值化<ul>
<li>根据特征值取值是否大于阈值将特征值变为0或1，可用类Binarizer 实现</li>
</ul>
</li>
<li>数据缺失</li>
<li>数据类型变换<ul>
<li>有些模型只能处理数值型数据。如果给定的数据是不同的类型，必须先将数据<br>变成数值型。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>第二步：模型确定和模型训练<ul>
<li>1、确定模型类型<ul>
<li>目标函数（损失函数、正则）</li>
</ul>
</li>
<li>2、模型训练<ul>
<li>优化算法（解析法，梯度下降、随机梯度下降…）</li>
</ul>
</li>
</ul>
</li>
<li>第三步：模型评估与模型选择<ul>
<li>模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果<ul>
<li>校验集划分（train_test_split、交叉验证）</li>
<li>评价指标 （sklearn.metics）</li>
<li>也可以检查残差的分布</li>
<li>还可以打印预测值与真值的散点图</li>
</ul>
</li>
<li>模型选择：选择预测性能最好的模型<ul>
<li>模型中通常有一些超参数，需要通过模型选择来确定</li>
<li>参数搜索范围：网格搜索（GridSearch）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="1-10-波士顿房价预测-数据探索代码"><a href="#1-10-波士顿房价预测-数据探索代码" class="headerlink" title="1.10 波士顿房价预测-数据探索代码"></a>1.10 波士顿房价预测-数据探索代码</h5><figure class="highlight python"><figcaption><span>python 3.7</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">"boston_housing.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据探索</span></span><br><span class="line">print(data.head())</span><br><span class="line">data.info()</span><br><span class="line">print(data.isnull().sum())</span><br><span class="line">print(data.describe())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标y(房屋价格)的直方图/分布</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">sns.distplot(data.MEDV.values, bins=<span class="number">30</span>, kde=<span class="literal">True</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Median value of owner_occupied homes'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单个特征散点图</span></span><br><span class="line">plt.scatter(range(data.shape[<span class="number">0</span>]), data[<span class="string">"MEDV"</span>].values, color=<span class="string">'purple'</span>)</span><br><span class="line">plt.title(<span class="string">"Distribution of Price"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除y大于50的样本</span></span><br><span class="line">data = data[data.MEDV &lt; <span class="number">50</span>]</span><br><span class="line">print(data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入属性的直方图／分布</span></span><br><span class="line"><span class="comment"># 犯罪率特征</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">sns.distplot(data.CRIM.values, bins=<span class="number">30</span>, kde=<span class="literal">False</span>)</span><br><span class="line">plt.xlabel(<span class="string">'crime rate'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否靠近charles river</span></span><br><span class="line">sns.countplot(data.CHAS, order=[<span class="number">0</span>, <span class="number">1</span>]);</span><br><span class="line">plt.xlabel(<span class="string">'Charles River'</span>);</span><br><span class="line">plt.ylabel(<span class="string">'Number of occurrences'</span>);</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 靠近高速</span></span><br><span class="line">sns.countplot(data.RAD)</span><br><span class="line">plt.xlabel(<span class="string">'index of accessibility to radial highways'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两两特征之间的相关性</span></span><br><span class="line"><span class="comment"># 获得所有列的名字</span></span><br><span class="line">cols = data.columns</span><br><span class="line"><span class="comment"># 计算相关性</span></span><br><span class="line">data_corr = data.corr().abs()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相关性热图</span></span><br><span class="line">plt.subplots(figsize=(<span class="number">13</span>, <span class="number">9</span>))</span><br><span class="line">sns.heatmap(data_corr, annot=<span class="literal">True</span>)</span><br><span class="line">sns.heatmap(data_corr, mask=data_corr &lt; <span class="number">1</span>, cbar=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">'house_coor.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出强相关对</span></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">corr_list = []</span><br><span class="line">size = data_corr.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, size):</span><br><span class="line">        <span class="keyword">if</span> (data_corr.iloc[i,j] &gt;= threshold <span class="keyword">and</span> data_corr.iloc[i, j] &lt; <span class="number">1</span>) <span class="keyword">or</span> (data_corr.iloc[i, j] &lt; <span class="number">0</span> <span class="keyword">and</span> data_corr.iloc &lt;= -threshold):</span><br><span class="line">            corr_list.append([data_corr.iloc[i, j], i, j])</span><br><span class="line">s_corr_list = sorted(corr_list, key=<span class="keyword">lambda</span> x: -abs(x[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> v, i, j <span class="keyword">in</span> s_corr_list:</span><br><span class="line">    print(<span class="string">"%s and %s = %.2f"</span> % (cols[i], cols[j], v))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> v, i, j <span class="keyword">in</span> s_corr_list:</span><br><span class="line">    sns.pairplot(data, height=<span class="number">6</span>, x_vars=cols[i], y_vars=cols[j])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h5 id="1-11-波士顿房价预测案例详解"><a href="#1-11-波士顿房价预测案例详解" class="headerlink" title="1.11 波士顿房价预测案例详解"></a>1.11 波士顿房价预测案例详解</h5><h5 id="1-12-波士顿房价预测案例详解-代码讲解"><a href="#1-12-波士顿房价预测案例详解-代码讲解" class="headerlink" title="1.12 波士顿房价预测案例详解-代码讲解"></a>1.12 波士顿房价预测案例详解-代码讲解</h5><figure class="highlight python"><figcaption><span>python 3.7</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 波士顿房价预测案例——线性回归分析</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 矩阵操作</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># SQL数据处理</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score <span class="comment"># 评价回归预测模型的性能</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">"boston_housing.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、数据准备</span></span><br><span class="line"><span class="comment"># 从原始数据中分离输入特征x和输出y</span></span><br><span class="line">y = data[<span class="string">'MEDV'</span>].values</span><br><span class="line">X = data.drop(<span class="string">'MEDV'</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于后续显示权重系数对应的特征</span></span><br><span class="line">columns = X.columns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据较少，将数据分割训练数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机采样20%的数据构建测试样本，其余作为训练样本</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">33</span>,test_size=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># print(X_train.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、数据预处理/特征工程</span></span><br><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别初始化对特征和目标值的标准化器</span></span><br><span class="line">ss_X = StandardScaler()</span><br><span class="line">ss_y = StandardScaler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别对训练和测试数据的特征以及目标值进行标准化处理</span></span><br><span class="line">X_train = ss_X.fit_transform(X_train)</span><br><span class="line">X_test = ss_X.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对y标准化不是必须</span></span><br><span class="line"><span class="comment"># 对y标准化的好处是不同的问题的w差异不太大，同时正则参数的范围也有限</span></span><br><span class="line">y_train = ss_y.fit_transform(y_train.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">y_test = ss_y.transform(y_test.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、确定模型类型</span></span><br><span class="line"><span class="comment"># 3.1 尝试缺省参数的线性回归</span></span><br><span class="line"><span class="comment"># 线性回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用默认配置初始化</span></span><br><span class="line">lr = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型参数</span></span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_test_pred_lr = lr.predict(X_test)</span><br><span class="line">y_train_pred_lr = lr.predict(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看各特征的权重系数，系数的绝对值大小可视为该特征的重要性</span></span><br><span class="line">fs = pd.DataFrame(&#123;<span class="string">"columns"</span>: list(columns), <span class="string">"coef"</span>: list((lr.coef_.T))&#125;)</span><br><span class="line">fs.sort_values(by=[<span class="string">'coef'</span>], ascending=<span class="literal">False</span>)</span><br><span class="line">print(fs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">print(<span class="string">'The r2 score of LinearRegression on test is'</span>, r2_score(y_test, y_test_pred_lr))</span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">print(<span class="string">'The r2 score of LinearRegression on train is'</span>, r2_score(y_train, y_train_pred_lr))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练集上观察残差的分布，看是否符合模型假设：噪声为0均值的高斯噪声</span></span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">7</span>, <span class="number">5</span>))</span><br><span class="line">f.tight_layout()</span><br><span class="line">ax.hist(y_train - y_train_pred_lr, bins=<span class="number">40</span>, label=<span class="string">'Residuals Linear'</span>, color=<span class="string">'b'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">ax.set_title(<span class="string">"Histogram of Residuals"</span>)</span><br><span class="line">ax.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 还可以观察预测值与真值的散点图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">plt.scatter(y_train, y_train_pred_lr)</span><br><span class="line">plt.plot([<span class="number">-3</span>, <span class="number">3</span>],[<span class="number">-3</span>, <span class="number">3</span>], <span class="string">'--k'</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'True price'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Predicted price'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性模型，随机梯度下降优化模型参数</span></span><br><span class="line"><span class="comment"># 随机梯度下降一般在大数据集上应用，其实本项目不适合用</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment">#  使用默认配置初始化线</span></span><br><span class="line">sgdr = SGDRegressor(max_iter=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练：参数估计</span></span><br><span class="line">sgdr.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">sgdr.coef_</span><br><span class="line">print(<span class="string">'The value of default measurement of SGDRegressor on test is'</span>, sgdr.score(X_test, y_test))</span><br><span class="line">print(<span class="string">'The value of default measurement of SGDRegressor on train is'</span>, sgdr.score(X_train, y_train))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 正则化的线性回归（L2正则--&gt;岭回归）</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置超参数（正则参数）范围</span></span><br><span class="line">alphas = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个RidgeCV</span></span><br><span class="line">ridge = RidgeCV(alphas=alphas, store_cv_values=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">ridge.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_test_pred_ridge = ridge.predict(X_test)</span><br><span class="line">y_train_pred_ridge = ridge.predict(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估，使用r2_score评价模型在测试集和训练集上的性能</span></span><br><span class="line">print(<span class="string">'The r2 score of RidgeCV on test is'</span>, r2_score(y_test, y_test_pred_ridge))</span><br><span class="line">print(<span class="string">'The r2 score of RidgeCV on test is'</span>, r2_score(y_train, y_train_pred_ridge))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">mse_mean = np.mean(ridge.cv_values_, axis=<span class="number">0</span>)</span><br><span class="line">plt.plot(np.log10(alphas), mse_mean.reshape(len(alphas), <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])</span></span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'log(alpha)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'mse'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'alpha is:'</span>, ridge.alpha_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看各特征的权重系数，系数的绝对值大小可视为该特制的重要性</span></span><br><span class="line">fs = pd.DataFrame(&#123;<span class="string">"columns"</span>: list(columns), <span class="string">"coef_lr"</span>: list(lr.coef_.T), <span class="string">"coef_ridge"</span>: list(ridge.coef_.T)&#125;)</span><br><span class="line">fs.sort_values(by=[<span class="string">'coef_lr'</span>], ascending=<span class="literal">False</span>)</span><br><span class="line">print(fs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.3 正则化的线性回归（L1正则--&gt;Lasso）</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个LassoCV实例</span></span><br><span class="line">lasso = LassoCV()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练（内含CV）</span></span><br><span class="line">lasso.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">y_test_pred_lasso = lasso.predict(X_test)</span><br><span class="line">y_train_pred_lasso = lasso.predict(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估， 使用r2_score评价模型在测试集和训练集上的性能</span></span><br><span class="line">print(<span class="string">'The r2 score of LassoCV on test is'</span>, r2_score(y_test, y_test_pred_lasso))</span><br><span class="line">print(<span class="string">'The r2 score of LassoCV on train is'</span>, r2_score(y_train, y_train_pred_lasso))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">mses = np.mean(lasso.mse_path_, axis=<span class="number">1</span>)</span><br><span class="line">plt.plot(np.log10(lasso.alphas_), mses)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])</span></span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'log(alpha)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'mse'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'alpha is:'</span>, lasso.alpha_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看各特征的权重系数，系数的绝对值大小可视为该特制的重要性</span></span><br><span class="line">fs = pd.DataFrame(&#123;<span class="string">"columns"</span>: list(columns), <span class="string">"coef_lr"</span>: list(lr.coef_.T), <span class="string">"coef_ridge"</span>: list(lasso.coef_.T)&#125;)</span><br><span class="line">fs.sort_values(by=[<span class="string">'coef_lr'</span>], ascending=<span class="literal">False</span>)</span><br><span class="line">print(fs)</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    
    <div>
  		
   		  <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">------ 本文结束------</div>
    
</div>
 		
	</div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"> <i class="fa fa-tag"></i> 机器学习</a>
          
            <a href="/tags/学习笔记/" rel="tag"> <i class="fa fa-tag"></i> 学习笔记</a>
          
            <a href="/tags/人工智能/" rel="tag"> <i class="fa fa-tag"></i> 人工智能</a>
          
            <a href="/tags/线性回归/" rel="tag"> <i class="fa fa-tag"></i> 线性回归</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/30/计算机视觉基础入门/" rel="next" title="计算机视觉基础入门 学习笔记">
                <i class="fa fa-chevron-left"></i> 计算机视觉基础入门 学习笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/01/Hexo搭建博客/" rel="prev" title="Hexo + GitHub Pages + Next在windows下搭建个人博客">
                Hexo + GitHub Pages + Next在windows下搭建个人博客 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Minh">
            
              <p class="site-author-name" itemprop="name">Minh</p>
              <p class="site-description motion-element" itemprop="description">既然相遇，不如同行</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/minhzou" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.miraclewk.top" title="Miraclewk" target="_blank">Miraclewk</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-一个Kaggle竞赛优胜解决方案"><span class="nav-number">1.</span> <span class="nav-text">1.1 一个Kaggle竞赛优胜解决方案</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-机器学习任务类型"><span class="nav-number">2.</span> <span class="nav-text">1.2  机器学习任务类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-一个典型的机器学习案例-对鱼进行分类"><span class="nav-number">3.</span> <span class="nav-text">1.3 一个典型的机器学习案例-对鱼进行分类</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4-机器学习算法的组成部分"><span class="nav-number">4.</span> <span class="nav-text">1.4 机器学习算法的组成部分</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-5-学习环境简介"><span class="nav-number">5.</span> <span class="nav-text">1.5 学习环境简介</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-6-线性回归模型"><span class="nav-number">6.</span> <span class="nav-text">1.6 线性回归模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-7-线性回归模型-优化算法"><span class="nav-number">7.</span> <span class="nav-text">1.7 线性回归模型-优化算法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-8-线性回归模型-模型选择"><span class="nav-number">8.</span> <span class="nav-text">1.8 线性回归模型-模型选择</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-9-波士顿房价预测案例详解——数据探索"><span class="nav-number">9.</span> <span class="nav-text">1.9 波士顿房价预测案例详解——数据探索</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-10-波士顿房价预测-数据探索代码"><span class="nav-number">10.</span> <span class="nav-text">1.10 波士顿房价预测-数据探索代码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-11-波士顿房价预测案例详解"><span class="nav-number">11.</span> <span class="nav-text">1.11 波士顿房价预测案例详解</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-12-波士顿房价预测案例详解-代码讲解"><span class="nav-number">12.</span> <span class="nav-text">1.12 波士顿房价预测案例详解-代码讲解</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Minh</span>

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 总访客
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: '6EPnyXCThXwcbw7bdMdV8mEW-gzGzoHsz',
        appKey: 'pLe1UvLJvlysvxS7vy8aJgqz',
        placeholder: '来~~ 快活呀',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('3');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'manual') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
