<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next1.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next1.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="学习笔记,人工智能,Logistic回归,SVM,">





  <link rel="alternate" href="/atom.xml" title="Minh's Blog" type="application/atom+xml">






<meta name="description" content="1、Logistic回归基本原理 分类 给定训练数据$D =\{\mathbf x_i, y_i\}^N_{i=1}$，分类任务学习一个从输入x到输出y的映射f ：$\hat y = f(\mathbf x) = \underset{c}{arg\ max}\ p(y = c \mid \mathbf x, D)$ 其中y为离散值，其取值范围称为标签空间:$Y =\{1,2,…,C\}$ 当C=2">
<meta name="keywords" content="学习笔记,人工智能,Logistic回归,SVM">
<meta property="og:type" content="article">
<meta property="og:title" content="第二周 Logistic回归、SVM">
<meta property="og:url" content="http://minhzou.top/2019/04/06/第二周 Logistic回归、SVM/index.html">
<meta property="og:site_name" content="Minh&#39;s Blog">
<meta property="og:description" content="1、Logistic回归基本原理 分类 给定训练数据$D =\{\mathbf x_i, y_i\}^N_{i=1}$，分类任务学习一个从输入x到输出y的映射f ：$\hat y = f(\mathbf x) = \underset{c}{arg\ max}\ p(y = c \mid \mathbf x, D)$ 其中y为离散值，其取值范围称为标签空间:$Y =\{1,2,…,C\}$ 当C=2">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://minhzou.top/2019/04/06/第二周%20Logistic回归、SVM/ROC曲线.png">
<meta property="og:image" content="http://minhzou.top/2019/04/06/第二周%20Logistic回归、SVM/PR曲线.png">
<meta property="og:updated_time" content="2019-04-13T09:12:47.667Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第二周 Logistic回归、SVM">
<meta name="twitter:description" content="1、Logistic回归基本原理 分类 给定训练数据$D =\{\mathbf x_i, y_i\}^N_{i=1}$，分类任务学习一个从输入x到输出y的映射f ：$\hat y = f(\mathbf x) = \underset{c}{arg\ max}\ p(y = c \mid \mathbf x, D)$ 其中y为离散值，其取值范围称为标签空间:$Y =\{1,2,…,C\}$ 当C=2">
<meta name="twitter:image" content="http://minhzou.top/2019/04/06/第二周%20Logistic回归、SVM/ROC曲线.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://minhzou.top/2019/04/06/第二周 Logistic回归、SVM/">





  <title>第二周 Logistic回归、SVM | Minh's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/minhzou" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Minh's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">成长的路上每一步都算数</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://minhzou.top/2019/04/06/第二周 Logistic回归、SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Minh">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Minh's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">第二周 Logistic回归、SVM</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-06T20:37:26+08:00">
                2019-04-06
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-04-13T17:12:47+08:00">
                2019-04-13
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/06/第二周 Logistic回归、SVM/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/06/第二周 Logistic回归、SVM/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.8k 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  9 min
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h5 id="1、Logistic回归基本原理"><a href="#1、Logistic回归基本原理" class="headerlink" title="1、Logistic回归基本原理"></a>1、Logistic回归基本原理</h5><ul>
<li>分类<ul>
<li>给定训练数据$D =\{\mathbf x_i, y_i\}^N_{i=1}$，分类任务学习一个从输入x到输出y的映射f ：<br>$\hat y = f(\mathbf x) = \underset{c}{arg\ max}\ p(y = c \mid \mathbf x, D)$</li>
<li>其中y为离散值，其取值范围称为标签空间:$Y =\{1,2,…,C\}$</li>
<li>当C=2时，为两类分类问题，计算出$p(y = 1 \mid \mathbf x)$即可。此时分布为Bernoulli分布: <script type="math/tex; mode=display">p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))</script>其中$\mu (\mathbf x) = \mathbb{E}(y \mid \mathbf x) = p(y = 1 \mid \mathbf x)$<a id="more"></a></li>
</ul>
</li>
<li>Recall:Bernouili分布<ul>
<li>Bernoulli分布又名两点分布或者0-1分布。若Bernoulli试验成功，则Bernoulli随机变量X取值为1，否则X为0。记试验成功概率为θ， 我们称X服从参数为θ的Bernoulli分布，记为: 𝑋~𝐵𝑒𝑟(θ), 概率函数（pmf）为：<script type="math/tex; mode=display">p(x) = \theta ^x(1- \theta)^{1-x} = \begin{cases} \theta & if\ x = 1\\ 1 - \theta & if\ x = 0 \end{cases}</script></li>
<li>Bernoulli分布的均值：$\mu = \theta $</li>
<li>方差：$\sigma^2 = \theta \times (1-\theta)$</li>
</ul>
</li>
<li>Logistic回归模型<ul>
<li>Logistic回归模型同线性回归模型类似，也是一个线性模型，只是条件概率𝑝(𝑦|𝐱)的形式不同：<script type="math/tex; mode=display">p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf  x))</script><script type="math/tex; mode=display">\mu (\mathbf x) = \sigma(\mathbf w^T\mathbf x)</script></li>
<li>其中sigmoid函数（S形函数）定义为<script type="math/tex; mode=display">\sigma(a) = \frac{1}{1+exp(-a)} = \frac{exp(-a)}{exp(-a)+1}</script></li>
<li>亦被称为logistic函数或logit函数，将实数a变换到[0,1]区间。<ul>
<li>因为概率取值在[0,1]区间</li>
<li>Logistic回归亦被称为logit回归</li>
</ul>
</li>
</ul>
</li>
<li>为什么用logistic函数？<ul>
<li>在神经科学中<ul>
<li>神经元对其输入进行加权和：$f(x) = w^Tx$</li>
<li>如果该和大于某阈值 $f(x) &gt; \tau $，神经元发放脉冲</li>
</ul>
</li>
<li>在Logistic回归，定义Log Odds Ratio:<script type="math/tex; mode=display">\begin{eqnarray} LOR(x) &=& log \frac {p(y=1 \mid \mathbf  x, \mathbf f w)}{p(y = 0 \mid \mathbf x, \mathbf w)} 
  &=& log [\frac{1}{1+exp(-\mathbf  w^T\mathbf x)} \frac {1+exp(-\mathbf w^T\mathbf x)}{exp(-\mathbf w^T\mathbf x)}] \\
  &=& log [exp(\mathbf w^T \mathbf x)]\\
  &=& \mathbf w^T\mathbf x \end{eqnarray}</script></li>
<li>因此，$iff LOR(\mathbf x) = \mathbf w^T \mathbf x &gt; 0$神经元发放脉冲，即<br>$p(y=1 \mid \mathbf x, \mathbf w) &gt; p(y=0 \mid \mathbf x, \mathbf w)$</li>
</ul>
</li>
<li>线性决策函数<ul>
<li>在Logistic回归中<ul>
<li>$LOR(\bf x) = w^Tx &gt; 0, \hat y = 1$</li>
<li>$LOR(\bf x) = w^Tx &lt;0, \hat y = 0$</li>
<li>$\bf w^T \bf x = 0$:决策面</li>
</ul>
</li>
<li>因此$a(\bf x) = w^Tx$分类决策面<ul>
<li>因此Logistic回归是一个线性分类器</li>
</ul>
</li>
</ul>
</li>
<li>极大似然估计<ul>
<li>Logistic回归：$p(y \mid \mathbf {x,w}) = Ber(y \mid \mu (x)),  \mu (\mathbf x) = \sigma (\mathbf w^T\mathbf x)$</li>
<li>令$\mu_i = \mu(\mathbf x_i)$，则负log似然为<br>$\begin{eqnarray} J(w) = NLL(\mathbf w) &amp;=&amp; - \sum_{i=1}^N log \left[(\mu_i)^{y_i} \times (1-\mu_i)^{1-y_i}\right]\\<br>  &amp;=&amp; \sum_{i=1}^N- \left[y_i log(\mu_i)+(1-y_i)log(1-u_i) \right]<br>  \end{eqnarray}$</li>
<li>极大似然估计等价于最小Logistic损失</li>
<li>优化求解：梯度下降／牛顿法</li>
</ul>
</li>
<li>梯度<ul>
<li>目标函数为<br>$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$</li>
<li>梯度为<br>$\begin{eqnarray} g(\bf w) &amp;=&amp; \frac{\partial J(\bf w)}{\partial \bf w} = \frac{\partial}{\partial \bf w}[\sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]]\\<br>&amp;=&amp; \sum_{i=1}^N[\mu(\mathbf x_i) - y_i] \mathbf x_i \\<br>&amp;=&amp; \bf X^T(\mu - y)<br>\end{eqnarray}$</li>
<li>二阶Hessian矩阵为<br>$\begin{eqnarray} H(w) &amp;=&amp; \frac{\partial}{\partial \mathbf w}[\mathbf g( \mathbf w)^T] = \sum_{i=1}^N(\frac {\partial}{\partial \mathbf w}\mu_i) \mathbf x_i^T\\<br>&amp;=&amp; \sum_{i=1}{N}\mu_i(1-\mu_i)\bf x_ix_i^T = X^T\underset{S}{ \underbrace{diag(\mu_i(1-\mu_i)}}X = X^TSX<br>\end{eqnarray}$</li>
</ul>
</li>
<li>牛顿法<ul>
<li>亦称牛顿-拉夫逊（ Newton-Raphson ）方法<ul>
<li>牛顿在17世纪提出的一种近似求解方程的方法</li>
<li>使用函数f(x)的泰勒级数的前面几项来寻找方程f(x) = 0的根</li>
</ul>
</li>
<li>在求极值问题中，求$g(\mathbf w) = \frac {\partial J(\mathbf w)}{\partial w} = 0$的根<ul>
<li>对应$J(\mathbf w)$处取极值</li>
</ul>
</li>
<li>将导数$g(\mathbf w)$在 $w^t$处进行Taylor展开：<br>$0 = \bf g(\hat w) = g(w^t)+(\hat w - w^t)H(w^t) + Op(\hat w - w^t)$</li>
<li>去掉高阶无穷小$Op(\bf \hat w - w^t)$，从而得到<br>$g(\bf w^t)+(\hat w - w^t)H(w^t) = 0 \Rightarrow \hat w = w^t - H^{-1}(w^t)g(w^t)$</li>
<li>因此迭代机制为：<br>$\bf w^{t+1} = w^t - H^{-1}(w^t)g(w^t)$<ul>
<li>也被称为二阶梯度下降法，移动方向:$\bf d = -(H(w^t))^{-1}g(w^t)$</li>
<li>Vs. 一阶梯度法，移动方向:$\bf d = -g(w^t)$移动</li>
</ul>
</li>
</ul>
</li>
<li>Iteratively Reweighted Least Squares（IRLS）<ul>
<li>引入记号：<br>$\bf g^t(w) = X^T(\mu^t - y), \mu_i^t = \sigma((w^t)^Tx_i)$<br>$\bf H^t(w) = X^TS^tX$,$ S^t:diag(\mu_i^t(1-\mu_1^t),…,\mu_N^t(1-\mu_N^t))$</li>
<li>根据牛顿法的结果：<br>$w^{t+1} = w^t - (H^t)^{-1}g^t = (X^TS^tX)^{-1}X^TS^tz$</li>
<li>回忆最小二乘问题：<ul>
<li>目标函数：$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T(y - Xw)$</li>
<li>解：$\hat w = (X^TX)^{-1}X^Ty$</li>
</ul>
</li>
<li>回忆加权最小二乘问题：<ul>
<li>目标函数:$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T\Sigma^{-1}(y - Xw)$</li>
<li>解：$\hat w = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}y$</li>
</ul>
</li>
<li>IRLS中，$\bf w^{t+1} = (X^TS^tX)^{-1}X^TS^t[Xw^t + (S^t)^{-1}(y - \mu ^t)]$<ul>
<li>相当于权重矩阵为 $\Sigma^{-1} = \bf S^t$</li>
<li>由于$S^t$是对角阵，$S^t$相当于给每个样本的权重$S_{ii}^t = \mu_i^t(1-\mu_i^t), \mathbf z_i^t = (\mathbf w^t)^T\mathbf x_i + \frac {y_i - \mu_i^t}{S_{ii}^t}$</li>
</ul>
</li>
</ul>
</li>
<li>拟牛顿法<ul>
<li>牛顿法比一般的梯度下降法收敛速度快，但是在高维情况下，计算目标函数的二阶偏导数的复杂度很大，而且有时候目标函数的海森矩阵无法保持正定，不存在逆矩阵，此时牛顿法将不再能使用。</li>
<li>因此，人们提出了拟牛顿法。其基本思想是：不用二阶偏导数而构造出可以近似Hessian矩阵(或Hessian矩阵的逆矩阵)的正定对称矩阵，进而再逐步优化目标函数。不同的构造方法就产生了不同的拟牛顿法（Quasi-Newton Methods）<ul>
<li>BFGS／LBFGS／Newton-CG</li>
</ul>
</li>
</ul>
</li>
<li>正则化的Logistic回归<ul>
<li>若损失函数取logistic损失，则Logistic回归的目标函数为<br>$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$</li>
<li>同线性回归类似，Logistic回归亦可加上L2正则<br>$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \Vert \mathbf w\Vert ^2_2$</li>
<li>或L1正则<br>$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \vert \mathbf w\vert $</li>
<li>L2正则的Logistic回归求解<ul>
<li>梯度为:<br>$g_{I 2}(\mathbf{w})=g(\mathbf{w})+\lambda \mathbf{w}=\sum_{i=1}^{N}\left(\mu\left(\mathbf{x}_{i}\right)-y_{i}\right) \mathbf{x}_{i}+\lambda \mathbf{w}=\mathbf{X}^{T}(\mathbf{\mu}-\mathbf{y})+\lambda \mathbf{w}$</li>
<li>Hessian矩阵为：$\mathbf{H}_{L 2}(\mathbf{w})=\mathbf{H}(\mathbf{w})+\lambda \mathbf{I}=\mathbf{X}^{T} \mathbf{S} \mathbf{X}+\lambda \mathbf{I}$</li>
<li>类似不带正则的Logistic回归，可采用（随机）梯度下降、牛顿法或拟牛顿法求解。</li>
</ul>
</li>
<li>L1正则的Logistic回归求解<ul>
<li>L1正则项的在0处不可导</li>
<li>在此我们L1正则的Logistic回归的牛顿法（IRLS）求解<ul>
<li>随机梯度下降（在线学习）在CTR预估部分讲解</li>
</ul>
</li>
<li>Recall：IRLS<script type="math/tex; mode=display">\mathbf{w}^{t+1}=\left(\mathbf{X}^{T} \mathbf{S}^{t} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{S}^{t} \mathbf{z}=\underset{\mathbf{w}}{\arg \min }\left\|\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{X} \mathbf{w}-\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{z}\right\|_{2}^{2}</script></li>
<li>L1正则的Logistic回归在每次迭代中可视为一个再加权的Lasso问题：<script type="math/tex; mode=display">\mathbf{w}^{t+1}=\underset{\mathbf{w}}{\arg \min }\left\|\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{X} \mathbf{w}-\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{z}\right\|_{2}^{2}, s . t .\|\mathbf{w}\|_{1}<t</script></li>
</ul>
</li>
</ul>
</li>
<li>小结<ul>
<li>Logistic回归：<ul>
<li>损失函数：负log似然损失</li>
<li>正则：L2/L1正则</li>
<li>优化：梯度下降／牛顿法／拟牛顿法</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="2、Softmax分类器"><a href="#2、Softmax分类器" class="headerlink" title="2、Softmax分类器"></a>2、Softmax分类器</h5><ul>
<li>多类分类任务<ul>
<li>一对所有(One-vs-all /One-vs-rest)：<br>$f_{w}^{c}(x) = p(y=c \mid \mathbf x, \mathbf W), c =1, 2, 3$<br>如果是正则LR，每类的模型都有自己正则参数</li>
<li>One-vs-all<ul>
<li>对每个类别c，训练一个logistic回归分类器$f_w^c(x)$，预测概率$y=c$</li>
<li>对新的输入x，选择使得$f_w^c(x)$最大的类别作为其预测：$\underset{c}{max} f_w^c(\mathbf x)$</li>
</ul>
</li>
</ul>
</li>
<li>Softmax分类器<ul>
<li>从sigmoid（对应二项分布）扩展为softmax函数（对应多项分布Cat）：<br>$p(y=c \mid \mathbf x, \mathbf W) = Cat(y \mid S(\bf W^Tx))$</li>
<li>Softmax 函数类似取最大函数：<br>$S(\mathbf \eta)_c = \frac {exp(\eta_c)}{\sum^C_{c\prime = 1}\  exp(\eta_{c\prime})}$</li>
<li>综合起来：<br>$p(y = c \mid \mathbf x, \mathbf W) = \frac {exp(\mathbf w_c^T\mathbf x)}{\sum^C_{c\prime = 1}\  exp(\mathbf w_{c\prime}^T\mathbf x)}$</li>
</ul>
</li>
<li>Softmax回归<ul>
<li>引入记号：<br>$\mu_{ic} = p(y_i = c \mid \mathbf x_i, \mathbf W) = S(\eta_i)_c$<br>$\eta_i = \mathbf W^T\mathbf x_i$  C $\times$ vector<br>$y_{ic} = \prod (y_i =c)$</li>
<li>则负似然函数为：$ \begin{eqnarray}J(\mathbf W)<br>&amp;=&amp;NLL(\mathbf W) \\<br>&amp;=&amp; -l(\mathbf W) \\<br>&amp;=&amp; -log\prod_{i=1}^N \prod_{c=1}^C \mu_{ic}^{y_{ic}} \\<br>&amp;=&amp; - \sum_{i=1}^N\sum_{c=1}^Cy_{ic}log \mu_{ic}\\<br>&amp;=&amp; -\sum_{i=1}^N[(\sum_{c=1}^Cy_{ic}\mathbf w_c^T \mathbf x_i) - log(\sum_{c \prime = 1}^C exp(\mathbf w^T_{c \prime} \mathbf x_i))]<br>\end{eqnarray}$<ul>
<li>梯度:$g = [\nabla J(\mathbf w_i),…,\nabla J(\mathbf w_c)] = [\mathbf g_1,…,\mathbf g_c]$<br>$\mathbf g_c = \sum_{i=1}^N(\mu_{ic}-y_{ic})\mathbf x_i$</li>
<li>Hessian矩阵为正定：$\mathbf H = \sum_{i=1}^N(diag(\mathbf \mu_i)-\mathbf \mu_i \mathbf \mu_i^T)\otimes \mathbf x_i \mathbf x_i^T$</li>
</ul>
</li>
</ul>
</li>
<li>小结：Softmax分类器Logistic回归<ul>
<li>Softmax分类器能实现多类分类，是对Logistic回归在两类分类任务上的扩展</li>
<li>优化算法和正则与两类分类Logistic回归类似</li>
</ul>
</li>
</ul>
<h5 id="3、Scikit-learn中的Logistic回归实现"><a href="#3、Scikit-learn中的Logistic回归实现" class="headerlink" title="3、Scikit learn中的Logistic回归实现"></a>3、Scikit learn中的Logistic回归实现</h5><ul>
<li>Scikit learn 中的LogisticRegression实现<ul>
<li>Scikit learn提供的LogisticRegression实现为：<br>LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’liblinear’, max_iter=100, multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1)<ul>
<li>Logistic回归的正则参数：penalty、C</li>
<li>优化求解参数： dual、solver、max_iter、tol、warm_start</li>
<li>模型参数： multi_class、fit_intercept、intercept_scaling</li>
<li>样本均衡参数：class_weight</li>
</ul>
</li>
</ul>
</li>
<li>LogisticRegression参数列表</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>penalty</td>
<td>惩罚函数／正则函数，支持L2正则和L1正则，缺省：L2</td>
</tr>
<tr>
<td>dual</td>
<td>原问题（primal）还是对偶问题求解。对偶只支持L2正则和liblinear solver。当样本数n_samples&gt;特征数目n_features时，缺省：False</td>
</tr>
<tr>
<td>tol</td>
<td>迭代终止判据的误差范围。缺省:1e-4</td>
</tr>
<tr>
<td>C</td>
<td>C=1/$\lambda$ , 缺省：1</td>
</tr>
<tr>
<td>fit_intercept</td>
<td>是否在决策函数中加入截距项。如果数据已经中心化，可以不用。缺省：True</td>
</tr>
<tr>
<td>intercept_scaling</td>
<td>截距缩放因子，当fit_intercept为True且liblinear solver有效所以还是对y做标准化预处理</td>
</tr>
<tr>
<td>class_weight</td>
<td>不同类别样本的权重，用户指定每类样本权重或‘balanced’（每类样本权重与该类样本出现比例成反比）。缺省：None</td>
</tr>
<tr>
<td>random_state</td>
<td>混合数据的伪随机数。缺省：None</td>
</tr>
<tr>
<td>solver</td>
<td>优化求解算法，可为‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’。缺省：liblinear</td>
</tr>
<tr>
<td>max_iter</td>
<td>最大迭代次数，当newton-cg, sag and lbfgs solvers时有效。缺省：100</td>
</tr>
<tr>
<td>multi_class</td>
<td>多类分类处理策略，可为‘ovr’, ‘multinomial’。‘ovr’为1对多，将多类分类转化为多个两类分类问题，multinomial为softmax分类。缺省：‘ovr’</td>
</tr>
<tr>
<td>verbose</td>
<td>是否详细输出</td>
</tr>
<tr>
<td>warm_start</td>
<td>是否热启动（用之前的结果作为初始化），对liblinear solver无效。缺省：False</td>
</tr>
<tr>
<td>n_jobs</td>
<td>多线程控制。缺省值-1，算法自动检测可用CPU核，并使用全部核</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>多分类问题<ul>
<li>multi_class参数决定了多类分类的实现方式</li>
<li>‘ovr’ ：即1对其他（one-vs-rest，OvR），将多类分类转化为多个二类分类任务。为了完成第c类的分类决策，将所有第c类的样本作为正例，除了第c类样本以外的所有样本都作为负例。</li>
<li>‘multinomial’ ：多对多（many-vs-many，MvM），即softmax回归模型。</li>
<li>OvR相对简单，但分类效果相对略差<ul>
<li>大多数情况，不排除某些情况下OvR更好</li>
</ul>
</li>
<li>MvM分类相对精确，但分类速度较OvR慢</li>
<li>multi_class选择会影响优化算法solver参数的选择<ul>
<li>OvR：可用所有的slover</li>
<li>Multinomial： 只能选择newton-cg, lbfgs和sag／saga</li>
</ul>
</li>
</ul>
</li>
<li>优化求解算法solver<ul>
<li>liblinear：使用了开源的liblinear库实现，使用坐标轴下降法来迭代优化损失函数</li>
<li>sag：随机平均梯度下降（Stochastic Average Gradient），是梯度下降法的变种，每次迭代仅用一部分的样本来计算梯度，适合于样本多的情况</li>
<li>saga： sag的增强版本</li>
<li>lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵（Hessian矩阵）来迭代优化损失函数</li>
<li>newton-cg：牛顿法家族的一种（ 共轭梯度）</li>
<li>对小数据集，‘liblinear’ 是一个很好的选择，而‘sag’ 和‘saga’ 对大数据集更快</li>
<li>对多类分类问题，只有‘newton-cg’, ‘sag’, ‘saga’ 和‘lbfgs’支持MvM（multinomial）， ‘liblinear’ 只支持OvR（one-versus-rest） 的方式</li>
<li>‘newton-cg’, ‘lbfgs’ 和‘sag’ 支持L2正则，而‘liblinear’ 和‘saga’ 支持L1正则</li>
<li>注意： ‘sag’ 和‘saga’ 只有当特征有类似的尺度（scale）时能保证快速收敛。（对数据做标准化预处理）</li>
</ul>
</li>
<li>优化求解算法solver选择</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>正则</th>
<th>求解算法</th>
<th>应用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1</td>
<td>liblinear</td>
<td>如果模型的特征非常多，希望一些不重要的特征系数归零，从而让模型系数稀疏的话，可以使用L1正则化。liblinear适用于小数据集</td>
</tr>
<tr>
<td>L1</td>
<td>saga</td>
<td>当数据量较大，且选择L1，只能采用saga</td>
</tr>
<tr>
<td>L2</td>
<td>liblinear</td>
<td>libniear只支持多元逻辑回归的OvR，不支持多项分布损失（MvM），但MVM相对精确</td>
</tr>
<tr>
<td>L2</td>
<td>lbfgs/newton-cg/sag</td>
<td>较大数据集，支持OvR和MvM两种多元logit回归</td>
</tr>
<tr>
<td>L2</td>
<td>sag／saga</td>
<td>如果样本量非常大，sag／sga是第一选择</td>
</tr>
</tbody>
</table>
</div>
<p>对于大数据集，可以考虑使用SGDClassifier，并使用logloss</p>
<ul>
<li>类别权重class_weight<ul>
<li>class_weight用于不同类别样本数目不均衡的情况</li>
</ul>
</li>
</ul>
<h5 id="4、不平衡数据分类学习"><a href="#4、不平衡数据分类学习" class="headerlink" title="4、不平衡数据分类学习"></a>4、不平衡数据分类学习</h5><ul>
<li>不平衡数据的出现场景<ul>
<li>搜索引擎的点击预测<ul>
<li>点击的网页往往占据很小的比例</li>
</ul>
</li>
<li>电子商务领域的商品推荐<ul>
<li>推荐的商品被购买的比例很低</li>
</ul>
</li>
<li>信用卡欺诈检测</li>
<li>信用卡欺诈检测</li>
<li>…</li>
</ul>
</li>
<li>解决方案<ul>
<li>从数据的角度：抽样，从而使得数据相对均衡</li>
<li>从算法的角度：考虑不同误分类情况代价的差异性对算法进行优化</li>
</ul>
</li>
<li>采样<ul>
<li>随机欠采样：从多数类中随机选择少量样本再合并原有少数类样本作为新的训练数据集<ul>
<li>有放回采样</li>
<li>无放回采样</li>
<li>会造成一些信息缺失，选取的样本可能有偏差</li>
</ul>
</li>
<li>随机过采样：随机复制少数类来样本<ul>
<li>扩大了数据集，造成模型训练复杂度加大，另一方面也容易造成模型的过拟合问题</li>
</ul>
</li>
</ul>
</li>
<li>集成学习算法<ul>
<li>EasyEnsemble算法：<ul>
<li>对于多数类样本，通过n次有放回抽样生成n份子集</li>
<li>少数类样本分别和这n份样本合并训练一个模型：n个模型</li>
<li>最终模型：n个模型预测结果的平均值</li>
</ul>
</li>
<li>BalanceCascade（级联）算法：<ul>
<li>从多数类中有效地选择一些样本与少数类样本合并为新的数据集进行训练</li>
<li>训练好的模型每个多数类样本进行预测。若预测正确，则不考虑将其作为下一轮的训练样本</li>
<li>依次迭代直到满足某一停止条件，最终的模型是多次迭代模型的组合</li>
</ul>
</li>
</ul>
</li>
<li>SMOTE: Synthetic Minority Over-sampling Technique<ul>
<li>基本思想：基于“插值”来为少数类合成新的样本</li>
<li>对少数类的一个样本$i$ ，其特征向量为$x_i$,：<ul>
<li><ol>
<li>从少数类的全部N 个样本中找到样本$x_i$的K个近邻（如欧氏距离），记为$x_{i(near)}, near \in \{1, …, K\}$</li>
</ol>
</li>
<li><ol>
<li>从这K个近邻中随机选择一个样本$x_{i(nn)}$，再生成一个0到1之间的随机数$\zeta$ ，从而合成一个新样本$x_{i1}$：<ul>
<li>$x_{i1} = (1-\zeta)x_i + \zeta x_{inn}$</li>
<li>新样本$x_{i1}$相当于是表示样本xi和表示样本$x_{i(nn)}$ 的点之间所连线段上的一个点： 插值</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>SMOTE算法摒弃了随机过采样复制样本的做法，可以防止随机过采样易过拟合的问题。实践证明此方法可以提高分类器的性能</li>
<li>SMOTE 对高维数据不是很有效</li>
<li>当生成合成性实例时，SMOTE 并不会把来自其他类的相邻实例考虑进来，这导致了类重叠的增加，并会引入额外的噪音。为了解决SMOTE算法的这一缺点提出一些改进算法，如Borderline-SMOTE算法</li>
</ul>
</li>
<li>代价敏感学习<ul>
<li>在算法层面上解决不平衡数据学习的方法主要是基于代价敏感学习算法(Cost-Sensitive Learning)</li>
<li>代价敏感学习方法的核心要素是代价矩阵：不同类型的误分类情况导致的代价不一样</li>
<li>基于代价矩阵分析，代价敏感学习方法主要有以下三种实现方式：</li>
<li>从贝叶斯风险理论出发，把代价敏感学习看成是分类结果的一种后处理，按照传统方法学习到一个模型，以实现损失最小为目标对结果进行调整<ul>
<li>不依赖所用具体的分类器</li>
<li>但是缺点要求分类器输出值为概率</li>
</ul>
</li>
<li>从学习模型出发，对具体学习方法的改造，使之能适应不平衡数据下的学习<ul>
<li>代价敏感的支持向量机，决策树，神经网络</li>
<li>从预处理的角度出发，将代价用于权重的调整，使得分类器满足代价敏感的特性</li>
</ul>
</li>
</ul>
</li>
<li>Scikit learn中的不均衡样本分类处理<ul>
<li>类别权重class_weight<ul>
<li>class_weight参数用于标示分类模型中各类别样本的权重</li>
<li><ol>
<li>不考虑权重，即所有类别的权重相同</li>
</ol>
</li>
<li><ol>
<li>balanced：自动计算类别权重<ul>
<li>某类别的样本量越多，其权重越低；样本量越少，则权重越高</li>
<li>类权重计算方法为：n_samples / (n_classes * np.bincount(y))<ul>
<li>n_samples为样本数，n_classes为类别数量，np.bincount(y)输出每个类的样本数</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>手动指定各个类别的权重<ul>
<li>如对于0,1二类分类问题，可以定义class_weight={0:0.9, 1:0.1}，即类别0的权重为90%，而类别1的权重为10%</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>样本权重sample_weight<ul>
<li>模型训练：$fit(X, y, sample_weight=None)$<ul>
<li>其中参数sample_weight为样本权重参数</li>
</ul>
</li>
<li>当样本高度失衡时，样本不是总体样本的无偏估计，可能导致模型预测能力下降</li>
<li>解决方案：调节样本权重<ul>
<li>一种是在class_weight使用balanced</li>
<li>另一种是在调用fit函数时，通过sample_weight来调节每个样本权重</li>
<li>如果两种方法都用了，那么样本的真正权重是class_weight*sample_weight</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>小结：Logistic回归<ul>
<li>不均衡样本分类<ul>
<li>样本采样：过采样、欠采样</li>
<li>分类器：代价敏感函数<ul>
<li>样本权重、类别权重</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="5、分类模型的评价"><a href="#5、分类模型的评价" class="headerlink" title="5、分类模型的评价"></a>5、分类模型的评价</h5><ul>
<li>分类模型性能评价<ul>
<li>损失函数可以作为评价指标(log_loss、zero_one_loss、hinge_loss)</li>
<li>logistic／负log似然损失（log_loss）：<ul>
<li>logloss $=-\frac{1}{N} \sum_{i=0}^{N} \sum_{j=0}^{M} y_{i j} \log p_{i j}$<ul>
<li>M为类别数，$y_{ij}$为二值，当第i个样本为第j类时$y_{ij}$ = 1，否则取0；$p_{ij}$为模型预测的第i个样本为第j类的概率</li>
<li>当M=2时, $\operatorname{logloss}=-\frac{1}{N} \sum_{i=0}^{N}\left(y_{i} \log p_{i}+\left(1-y_{i}\right) \log \left(1-p_{i}\right)\right)$<ul>
<li>$y_{i}$为第i个样本类别，$p_{i}$为模型预测的第i个样本为第1类的概率</li>
</ul>
</li>
</ul>
</li>
<li>0-1损失(zero_one_loss) （错误率、正确率评价指标均与此有关）<ul>
<li>$\mathrm{MCE}=-\frac{1}{N} \sum_{\hat{y}_{i} \neq y_{i}} 1$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>两类分类任务中更多评价指标<ul>
<li>ROC／AUC</li>
<li>PR曲线</li>
<li>MAP@n</li>
</ul>
</li>
<li>False Positive &amp; False Negative<ul>
<li>0-1损失：假设两种错误的代价相等<ul>
<li>False Positive （FP） &amp; False Negative（FN）</li>
</ul>
</li>
<li>有些任务中可能某一类错误的代价更大<ul>
<li>如医疗诊断中将病例误分为正常，错过诊疗时机</li>
<li>因此单独列出每种错误的比例：混淆矩阵</li>
</ul>
</li>
<li>混淆矩阵（confusion matrix）<ul>
<li>真正的正值（true positives）</li>
<li>假的正值（false positives）</li>
<li>真正的负值（true negatives）</li>
<li>假的负值（false negatives ）</li>
<li>Scikit-learn实现了多类分类任务的混淆矩阵</li>
<li>sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)<ul>
<li>y_true： N个样本的标签观测值／真值</li>
<li>y_pred： N个样本的预测标签值</li>
<li>labels：C个类别在矩阵的索引顺序<ul>
<li>缺省为y_true或y_pred类别出现的顺序</li>
</ul>
</li>
<li>sample_weight： N个样本的权重</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>$\hat y = 1$</th>
<th>$\hat y = 0$</th>
<th>$\Sigma$</th>
</tr>
</thead>
<tbody>
<tr>
<td>y = 1</td>
<td>#TP</td>
<td>#FN</td>
<td>$N_{+}$</td>
</tr>
<tr>
<td>y = 0</td>
<td>#FP</td>
<td>#TN</td>
<td>$N_{-}$</td>
</tr>
<tr>
<td>$\Sigma$</td>
<td>$\hat N_{+}$</td>
<td>$\hat N_{-}$</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Receiver Operating Characteristic (ROC)<br>  $\operatorname{accuracy}=\frac{T P+T N}{N}$<br>  error rate $=\frac{F P+F N}{N}$<ul>
<li>PPV - positive predictive value, precision 预测结果为真的样本中真正为真的比例</li>
<li>TPR - true positive rate, sensitivity, recall, hit rate 预测结果召回了多少真正的真样本</li>
<li>FPR – False positive rate, false alarm, fallout 预测结果将多少假的样本预测预测成了真</li>
<li>下面我们讨论给定阈值τ的TPR和FPR</li>
<li>如果不是只考虑一个阈值，而是在一些列阈值上运行检测器，并画出TPR和FPR为阈值τ的隐式函数，得到ROC曲线。<br><img src="/2019/04/06/第二周 Logistic回归、SVM/ROC曲线.png" alt="ROC曲线"><ul>
<li>$T P R=\frac{T P}{N_{+}}$</li>
<li>$F P R=\frac{F P}{N_{-}}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>$\hat y = 1$</th>
<th>$\hat y = 0$</th>
<th>$\Sigma$</th>
</tr>
</thead>
<tbody>
<tr>
<td>y = 1</td>
<td>#TP</td>
<td>#FN</td>
<td>$N_{+}$</td>
</tr>
<tr>
<td>y = 0</td>
<td>#FP</td>
<td>#TN</td>
<td>$N_{-}$</td>
</tr>
<tr>
<td>$\Sigma$</td>
<td>$\hat N_{+}$</td>
<td>$\hat N_{-}$</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>y = 1</th>
<th>y = 0</th>
<th>y = 1</th>
<th>y = 0</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\hat y = 1$</td>
<td>$T P / \hat{N}_{+}=$ precision</td>
<td>$F P / \hat{N}_{+}=\mathrm{FDP}$</td>
<td>$T P / N_{+}=\mathrm{TPR}$</td>
<td>$F P / N_{-}=\mathrm{FPR}$</td>
</tr>
<tr>
<td>$\hat y = 0$</td>
<td>$F N / \hat{N}_{-}$</td>
<td>$T N / \hat{N}_{-}=\mathrm{NPV}$</td>
<td>$F N / N_{+}=\mathrm{FNR}$</td>
<td>$T N / N_{-}=\mathrm{TNR}$</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>PR曲线<ul>
<li>Precision and Recall (PR曲线)：用于稀有事件检测，如目标检测、信息检索<ul>
<li>负样本非常多，因此$F P R=F P / N_{-}$很小，比较TPR和FPR不是很有信息（ROC曲线中只有左边很小一部分有意义）$\rightarrow$ 只讨论正样本</li>
<li>Precision（精度，查准率）：以信息检索为例，对于一个查询，返回了一系列的文档，正确率指的是返回结果中相关文档占的比例<ul>
<li>Precision=返回结果中相关文档的数目/返回结果的数目</li>
</ul>
</li>
<li>Recall（召回率，查全率）：返回结果中相关文档占所有相关文档的比例<ul>
<li>Recall=返回结果中相关文档的数目/所有相关文档的数目</li>
</ul>
</li>
</ul>
</li>
<li>Precision and Recall (PR曲线)<ul>
<li>阈值变化时的P和R</li>
<li>Precsion $=T P / \hat{N}_{+}$ ：检测结果真正为正的比例</li>
<li>$\mathrm{Recall}=T P / N_{+}$：被正确检测到的正样本的比例<br><img src="/2019/04/06/第二周 Logistic回归、SVM/PR曲线.png" alt="PR曲线"></li>
</ul>
</li>
</ul>
</li>
<li>AP<ul>
<li>Precision只考虑了返回结果中相关文档的个数，没有考虑文档之间的序。</li>
<li>对一个搜索引擎或推荐系统而言，返回的结果是有序的，且越相关的文档越靠前越好，于是有了AP的概念。</li>
<li>AP: Average Precision，对不同召回率点上的精度进行平均<ul>
<li>$A P=\int_{0}^{1} p(k) d r=\sum_{k=0}^{n} p(k) \Delta r(k)$</li>
<li>即PR曲线下的面积（Recall： AUC为ROC下的面积）</li>
<li>其中k为返回文档中的序位，n为返回文档的数目，$p(k)$ 为列表中k截止点的precision， $\Delta r(k)$ 表示从k-1到k Recall的变化。</li>
</ul>
</li>
<li>上述离散求和表示等价于：$A P=\sum_{k=0}^{n} p(k) r e l(k) /$ 相关文档数目，其中<br>$r e l(k)$为示性函数，即第k个位置为相关文档取1，否则取0.<ul>
<li>计算每个位置上的precision，如果该位置的文档是不相关的则该位置precision=0</li>
<li>然后对所有的位置的precision再求平均</li>
</ul>
</li>
</ul>
</li>
<li>Mean Average Precision<ul>
<li>多个查询的AP平均：</li>
<li>$M A P=\left(\sum_{q=0}^{Q} A P(q)\right) /(Q)$</li>
<li>其中Q为查询的数目，n为文档数目</li>
</ul>
</li>
<li>MAP@K （MAPK）<ul>
<li>在现代web信息检索中，recall其实已经没有意义，因为相关文档有成千上万个，很少有人会关心所有文档。</li>
<li>Precision@K：在第K个位置上的Precision<ul>
<li>对于搜索引擎，考虑到大部分作者只关注前一、两页的结果，所以Precision @10， Precision @20对大规模搜索引擎非常有效</li>
</ul>
</li>
<li>MAP@K：多个查询Precision@K的平均</li>
</ul>
</li>
<li>F1 分数<ul>
<li>亦被称为F1 score, balanced F-score or F-measure</li>
<li>Precision 和Recall 调和平均：<br>$F 1=\frac{(2 \ast  Precision   \ast   Recall) }{ (Precision + Recall) }$<ul>
<li>最好为1，最差为0</li>
<li>多类：每类的F1平均值</li>
</ul>
</li>
</ul>
</li>
<li>模型性能评价<ul>
<li>Scikit learn 提供3 不同的API，用于评估模型预测的性能：<ul>
<li>Estimator score method: 模型自带的分数方法（score函数）提供一个缺省的评估准则。</li>
<li>Scoring parameter: 采用交叉验证的模型评估工具（ model_selection.cross_val_score and model_selection.GridSearchCV、以及一些xxxCV类）有scoring 参数（最佳参数为最大scoring模型对应的参数）</li>
<li>Metric functions: metrics模块提供评价预测性能的功能<ul>
<li>Classification metrics,</li>
<li>Multilabel ranking metrics</li>
<li>Regression metrics</li>
<li>Clustering metrics</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>分类模型性能评价<ul>
<li>对分类模型，缺省的score函数返回的是正确率（Mean accuracy）</li>
<li>scoring参数<ul>
<li>交叉验证中可设置scoring参数，规定模型性能的评价指标</li>
<li>注意：scoring越大的模型性能越好，所以如果采用损失／误差，需要加neg，如‘neg_log_loss’</li>
</ul>
</li>
<li>可以自定义评价函数<ul>
<li>有些指标还需要额外的参数，而没有在scoring出现，或者某个任务需要特殊的指标，scikit learn支持自定义scoring函数</li>
</ul>
</li>
<li>Scikit learn中Classification metrics 模块针对两类分类问题提供的性能评价指标有</li>
</ul>
</li>
</ul>
<h5 id="6、Logistic回归之模型选择-参数调优"><a href="#6、Logistic回归之模型选择-参数调优" class="headerlink" title="6、Logistic回归之模型选择_参数调优"></a>6、Logistic回归之模型选择_参数调优</h5><ul>
<li>网格搜索（Grid Search）<ul>
<li>不同超参数下的模型性能不同。</li>
<li>为了找到最佳模型，通常对这些超参数设定搜索范围</li>
<li>多个超参数可以联合一起优化，得到超参数的搜索网格<ul>
<li>如：LogisticRegression中的超参数penalty和C一起优化<ul>
<li>penalty可取‘l2’或‘l1’</li>
<li>C假设取值范围为： 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000</li>
</ul>
</li>
<li>则搜索网格为：</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>‘l2’</th>
<th>0.001</th>
<th>0.01</th>
<th>0.1</th>
<th>1</th>
<th>10</th>
<th>100</th>
<th>1000</th>
</tr>
</thead>
<tbody>
<tr>
<td>‘l1’</td>
<td>0.001</td>
<td>0.01</td>
<td>0.1</td>
<td>1</td>
<td>10</td>
<td>100</td>
<td>1000</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>LogisticRegression超参数调优<ul>
<li>超参数调优需先确定超参数的搜索网格，然后对每个可能的超参数组合评估其性能</li>
<li>对LogisticRegression的超参数调优，scikit learn提供给两种实现方式：<ul>
<li><ol>
<li>同其他estimator一样，调用GridSearchCV （集成了网格搜索和交叉验证）：设置候选参数集合、根据候选参数集合构造GridSearchCV、调用GridSearchCV 的fit函数；</li>
</ol>
</li>
<li><ol>
<li>LogisticRegressionCV 类内置的LR的交叉验证，用于找到最优的C参数</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>LogisticRegressionCV<ul>
<li>LogisticRegressionCV 使用了内置的Logistic回归的交叉验证，用于找到最优的C参数。（正则参数penalty可设为‘l1’ 或‘l2’ ）</li>
<li>对于多分类问题<ul>
<li>如果multi_class参数设置为“ovr”，对于每个类都获得一个最优的C；</li>
<li>如果multi_class设置为”multinomial”, 将获得一个最优的C，它使得交叉熵的loss（corss-entropy loss）最小。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="7、Logistic回归-Otto商品分类代码"><a href="#7、Logistic回归-Otto商品分类代码" class="headerlink" title="7、Logistic回归-Otto商品分类代码"></a>7、Logistic回归-Otto商品分类代码</h5><h5 id="8、支持向量机"><a href="#8、支持向量机" class="headerlink" title="8、支持向量机"></a>8、支持向量机</h5><ul>
<li>SVM基本原理<ul>
<li>SVM as 最大间隔分类器<ul>
<li>最大间隔原则：最大化两个类最近点之间的距离<ul>
<li>这个距离被称为间隔(margin)</li>
<li>边缘上的点被称为支持向量(support vectors)</li>
</ul>
</li>
<li>我们先假设分类器是线性可分的</li>
</ul>
</li>
<li>间隔<ul>
<li>线性分类面：$f(\mathbf{x}) = \mathbf {w}^{\mathrm{T}} \mathbf {x}+w_{0}$</li>
<li>则有 $\mathbf{x}=\mathbf{x}_{\mathrm{p}}+r \frac{\mathbf{w}}{|\mathbf{w}|}$<ul>
<li>其中x到分类面的距离r</li>
</ul>
</li>
<li>代入得到 $f(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \mathbf{x}+w_{0}=\mathbf{w}^{\mathrm{T}}\left(x_{\mathrm{p}}+r \frac{\mathbf{w}}{|\mathbf{w}|}\right)+w_{0}$<br>$=\mathbf{w}^{\mathrm{T}} x_{\mathrm{p}}+r \frac{\mathbf{w}^{\mathrm{T}} \mathbf{w}}{|\mathbf{w}|}+w_{0}$<br>$\Rightarrow r=\frac{f(\mathbf{x})}{|\mathbf{w}|}$</li>
<li>当x=0时，原点到分类面的距离<br>$r_{0}=\frac{f(\mathbf{0})}{|\mathbf{w}|}=\frac{w_{0}}{|\mathbf{w}|}$</li>
</ul>
</li>
<li>线性判别函数<ul>
<li>线性判别函数利用一个超平面把特征空间分隔成两个区域。</li>
<li>超平面的方向由法向量w确定，它的位置由阈值$w_{0}$确定。</li>
<li>判别函数f(x)正比于x点到超平面的代数距离（带正负号）<ul>
<li>当x点在超平面的正侧时， f(x) &gt;0；</li>
<li>当x点在超平面的负侧时， f(x) &lt;0</li>
<li>x点到超平面的距离$r y_{i}=\frac{y_{i} f(\mathbf{x})}{|\mathbf{w}|}$可视为对x判别的“置信度”<br>$y_{i} \in\{1,-1\}$</li>
</ul>
</li>
</ul>
</li>
<li>SVM 符号表示</li>
<li>间隔计算</li>
<li>SVM：最大间隔<ul>
<li>最大化间隔的超平面为<br>$\max _{w_{0}, \mathbf{w}} \frac{2}{|\mathbf{w}|}, \quad$ subject to $\quad y_{i}\left(w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right) \geq 1, \quad \forall i$</li>
<li>等价于<br>$\min _{w_{0}, \mathbf{w}} \frac{1}{2}|\mathbf{w}|^{2}, \quad$ subject to $\quad y_{i}\left(w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right) \geq 1, \quad \forall i$<ul>
<li>二次规划问题(目标函数为二次函数，约束为线性约束)</li>
<li>变量数为D+1，约束项的数目为N</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>对偶表示(Dual Representation)<ul>
<li>凸优化理论告诉我们可以将该优化问题等价地写成其对偶形式(dual formulation) 。</li>
<li>定义拉格朗日函数<br>$L\left(\boldsymbol{a}, w_{0}, \mathbf{w}\right)=\frac{1}{2} \mathbf{w}^{T} \mathbf{w}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right)-1\right), \quad \alpha_{i} \geq 0$</li>
<li>求使得目标$L\left(\boldsymbol{\alpha}, w_{0}, \mathbf{w}\right)$最小的对w0和w：<br>$\frac{\partial L}{\partial \mathbf{w}}=0 \Rightarrow \mathbf{w}=\sum_{i=1}^{N} \alpha_{i} y_{i} \mathbf{x}_{i}$<br>$\frac{\partial L}{\partial w_{0}}=0 \Rightarrow \sum_{i=1}^{N} \alpha_{i} y_{i}=0$</li>
<li>将$w_{0}, \mathbf{w}$从$L\left(\boldsymbol{\alpha}, w_{0}, \mathbf{w}\right)$消去，得到对偶表示</li>
</ul>
</li>
<li>Karush-Kuhn-Tucker (KKT) Conditions<ul>
<li>如果强对偶条件成立，则对最优的  $\mathbf{x}^{ \ast } , \lambda^{ \ast } , \mathbf{\mu}^{ \ast } $，必须满足下述KKT条件</li>
<li>原问题的可行域：$f_{i}\left(\mathbf{x}^{ \ast } \right) \leq 0, h_{j}\left(\mathbf{x}^{ \ast }\right)=0$</li>
<li>对偶问题的可行域：$\lambda^{\ast } \geq 0$</li>
<li>平稳条件：$\Delta_{x} L(\mathbf{x}, \lambda, \boldsymbol{\mu})=0$</li>
<li>互补松弛条件：$\lambda_{i}^{\ast } f_{i}\left(\mathbf{x}^{\ast }\right)=0$</li>
<li>如果$\mathbf{x}^{+}, \lambda^{+}, \mathbf{\mu}^{+}$满足凸问题的KKT条件，则其是最优的。</li>
</ul>
</li>
<li>SVM – Duality<ul>
<li>原问题：$P=\min _{w} \frac{1}{2} \mathbf{w}^{T} \mathbf{w}$<br>s.t. $y_{i}\left(w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right) \geq 1$</li>
<li>拉格朗日函数:$L(\mathbf{x}, \boldsymbol{a})=\frac{1}{2} \mathbf{w}^{T} \mathbf{w}-\sum_{i} \alpha_{i}\left(y_{i}\left(w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right)-1\right)$</li>
<li>对偶问题：$D=\max _{\alpha}\left(\mathbf{1}^{T} \boldsymbol{\alpha}-\boldsymbol{\alpha}^{T} \mathbf{y K y}\right),$ where $K_{i j}=\left\langle x_{i}, x_{j}\right\rangle$<br>S.t. $\alpha_{i} \geq 0$</li>
</ul>
</li>
<li>对偶性<ul>
<li>拉格朗日对偶通常是凹的（即使原问题非凸） ，可能更容易优化求解</li>
<li>弱对偶性：$\mathrm{P} \geq \mathrm{D}$<ul>
<li>总是成立</li>
</ul>
</li>
<li>强对偶性：P = D<ul>
<li>并不总是成立</li>
<li>对凸问题通常成立</li>
<li>对SVM QP成立</li>
</ul>
</li>
</ul>
</li>
<li>SVM – KKT Conditions<ul>
<li>拉格朗日函数:$L(\mathbf{x}, \boldsymbol{\alpha})=\frac{1}{2} \mathbf{w}^{T} \mathbf{w}-\boldsymbol{\alpha}^{T}\left(\mathbf{y}\left(\mathbf{x}^{T} \mathbf{w}+w_{0} \mathbf{1}\right)-1\right)$</li>
<li>对偶问题的可行域：$\alpha_{i}^{\ast } \geq 0$</li>
<li>原问题的可行域：$y_{i}\left(w_{0}^{\ast }+\mathbf{w}^{\ast_{T}} \mathbf{x}_{i}\right)-1 \geq 0$</li>
<li>互补松弛条件：$\alpha_{i}^{\ast }\left[y_{i}\left(w_{0}^{\ast }+\mathbf{w}^{\ast T} \mathbf{x}_{i}\right)-1\right]=0$</li>
<li>平稳条件：$\begin{aligned} \Delta L_{\mathrm{w}} &amp;=0 \Rightarrow \mathbf{w}^{\ast }=\mathbf{x} y \boldsymbol{\alpha} \\ \Delta L_{w_{0}} &amp;=0 \Rightarrow \boldsymbol{\alpha}^{\ast } y \mathbf{1}=0 \end{aligned}$</li>
</ul>
</li>
<li>α的稀疏性<ul>
<li>根据KKT条件，对每个点<br>$\alpha_{i}^{\ast}=0 \quad$ or $\quad y_{i}\left(w_{0}^{\ast }+\mathbf{w}^{\ast } x_{i}\right)=1$</li>
<li>当$\alpha_{i}^{\ast }=0$时，该点在决策函数中不起作用<br>$f(\mathbf{x})=w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}=w_{0}+\sum_{i} \alpha_{i} y_{i}\left\langle\mathbf{x}, \mathbf{x}_{i}\right\rangle$</li>
<li>其他点称为支持向量，满足$y_{i}\left(w_{0}^{\ast }+\mathbf{w}^{\ast T} \mathbf{x}_{i}\right)=1$</li>
<li>对应位于最大间隔超平面上的点<ul>
<li>模型训练好后，大多数点可以抛掉，只需保留支持向量</li>
</ul>
</li>
</ul>
</li>
<li>w0的计算<ul>
<li>由于支持向量满足 $y_{i}\left(w_{0}+\mathbf{w}^{T} \mathbf{x}_{i}\right)=1$</li>
<li>将 $f(\mathbf{x})=w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}=w_{0}+\sum \alpha_{i} y_{i}\left\langle\mathbf{x}, \mathbf{x}_{i}\right\rangle$</li>
<li>代入，得到<br>$y_{i}\left[\sum_{m \in S} \alpha_{m} y_{m}\left\langle\mathbf{x}_{i}, \mathbf{x}_{m}\right\rangle+ w_{0}\right]=1$<ul>
<li>用任意一个支持向量即可求得$w_{0}$</li>
</ul>
</li>
<li>为了得到更稳定的解，两边同乘以$y_{i}, y_{i}^{2}=1$</li>
<li>并对所有的支持向量求平均，得到<br>$w_{0}=\frac{1}{N_{S}} \sum_{m \in \mathcal{S}}\left[y_{i}-\sum_{m \in \mathcal{S}} \alpha_{m} y_{m}\left\langle\mathbf{x}_{i}, \mathbf{x}_{m}\right\rangle\right]$</li>
</ul>
</li>
<li>小结<ul>
<li>SVM基本原理<ul>
<li>最大间隔原则</li>
<li>对偶表示(Dual Representation)</li>
<li>KKT条件<h5 id="9、带松弛因子的C-SVM"><a href="#9、带松弛因子的C-SVM" class="headerlink" title="9、带松弛因子的C-SVM"></a>9、带松弛因子的C-SVM</h5><h5 id="10、核方法"><a href="#10、核方法" class="headerlink" title="10、核方法"></a>10、核方法</h5><h5 id="11、支持向量回归（SVR）"><a href="#11、支持向量回归（SVR）" class="headerlink" title="11、支持向量回归（SVR）"></a>11、支持向量回归（SVR）</h5><h5 id="12、sklearn中的SVM实现"><a href="#12、sklearn中的SVM实现" class="headerlink" title="12、sklearn中的SVM实现"></a>12、sklearn中的SVM实现</h5><h5 id="13、SVM-Otto"><a href="#13、SVM-Otto" class="headerlink" title="13、SVM-Otto"></a>13、SVM-Otto</h5></li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    
    
    
    <div>
  		
   		  <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">------ 本文结束------</div>
    
</div>
 		
	</div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/学习笔记/" rel="tag"> <i class="fa fa-tag"></i> 学习笔记</a>
          
            <a href="/tags/人工智能/" rel="tag"> <i class="fa fa-tag"></i> 人工智能</a>
          
            <a href="/tags/Logistic回归/" rel="tag"> <i class="fa fa-tag"></i> Logistic回归</a>
          
            <a href="/tags/SVM/" rel="tag"> <i class="fa fa-tag"></i> SVM</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/01/Hexo搭建博客/" rel="next" title="Hexo + GitHub Pages + Next在windows下搭建个人博客">
                <i class="fa fa-chevron-left"></i> Hexo + GitHub Pages + Next在windows下搭建个人博客
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/14/机器学习的数学基础/" rel="prev" title="机器学习的数学基础">
                机器学习的数学基础 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Minh">
            
              <p class="site-author-name" itemprop="name">Minh</p>
              <p class="site-description motion-element" itemprop="description">既然相遇，不如同行</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/minhzou" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.miraclewk.top" title="Miraclewk" target="_blank">Miraclewk</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#1、Logistic回归基本原理"><span class="nav-number">1.</span> <span class="nav-text">1、Logistic回归基本原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2、Softmax分类器"><span class="nav-number">2.</span> <span class="nav-text">2、Softmax分类器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3、Scikit-learn中的Logistic回归实现"><span class="nav-number">3.</span> <span class="nav-text">3、Scikit learn中的Logistic回归实现</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4、不平衡数据分类学习"><span class="nav-number">4.</span> <span class="nav-text">4、不平衡数据分类学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5、分类模型的评价"><span class="nav-number">5.</span> <span class="nav-text">5、分类模型的评价</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6、Logistic回归之模型选择-参数调优"><span class="nav-number">6.</span> <span class="nav-text">6、Logistic回归之模型选择_参数调优</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7、Logistic回归-Otto商品分类代码"><span class="nav-number">7.</span> <span class="nav-text">7、Logistic回归-Otto商品分类代码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8、支持向量机"><span class="nav-number">8.</span> <span class="nav-text">8、支持向量机</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#9、带松弛因子的C-SVM"><span class="nav-number">9.</span> <span class="nav-text">9、带松弛因子的C-SVM</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#10、核方法"><span class="nav-number">10.</span> <span class="nav-text">10、核方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#11、支持向量回归（SVR）"><span class="nav-number">11.</span> <span class="nav-text">11、支持向量回归（SVR）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#12、sklearn中的SVM实现"><span class="nav-number">12.</span> <span class="nav-text">12、sklearn中的SVM实现</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#13、SVM-Otto"><span class="nav-number">13.</span> <span class="nav-text">13、SVM-Otto</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Minh</span>

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 总访客
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: '6EPnyXCThXwcbw7bdMdV8mEW-gzGzoHsz',
        appKey: 'pLe1UvLJvlysvxS7vy8aJgqz',
        placeholder: '来~~ 快活呀',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('3');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'manual') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
