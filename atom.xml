<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Minh&#39;s Blog</title>
  
  <subtitle>成长的路上每一步都算数</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://minhzou.top/"/>
  <updated>2019-07-19T14:25:16.045Z</updated>
  <id>http://minhzou.top/</id>
  
  <author>
    <name>Minh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《机器学习》第7章 朴素贝叶斯 公式推导</title>
    <link href="http://minhzou.top/2019/07/15/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC7%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%20%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://minhzou.top/2019/07/15/《机器学习》第7章 朴素贝叶斯 公式推导/</id>
    <published>2019-07-15T14:21:12.985Z</published>
    <updated>2019-07-19T14:25:16.045Z</updated>
    
    <content type="html"><![CDATA[<h3 id="第7章-朴素贝叶斯"><a href="#第7章-朴素贝叶斯" class="headerlink" title="第7章 朴素贝叶斯"></a>第7章 朴素贝叶斯</h3><h4 id="贝叶斯判定准则"><a href="#贝叶斯判定准则" class="headerlink" title="贝叶斯判定准则"></a>贝叶斯判定准则</h4><p>贝叶斯判定准则：<br>为最小化总体风险，只需在每个样本上选择那个能使条件风险$R(c | \boldsymbol{x})$最小的类别标记，即</p><script type="math/tex; mode=display">h^{\star}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \min } R(c | \boldsymbol{x})</script><p>此时，$h^{\star}$称为贝叶斯最优分类器。</p><p>已知条件风险$R(c | \boldsymbol{x})$的计算公式为</p><script type="math/tex; mode=display">R\left(c_{i} | \boldsymbol{x}\right)=\sum_{j=1}^{N} \lambda_{i j} P\left(c_{j} | \boldsymbol{x}\right)</script><p>若目标是最小化分类错误率，则误判损失$\lambda_{i j}$对应为0/1损失，也即</p><script type="math/tex; mode=display">\lambda_{i j}=\left\{\begin{array}{l}{0, \text { if } i=j} \\ {1, \text { otherwise }}\end{array}\right.</script><p>那么条件风险$R(c | \boldsymbol{x})$的计算公式可以进一步展开为</p><script type="math/tex; mode=display">\begin{aligned} R\left(c_{i} | \boldsymbol{x}\right) &=1 \times P\left(c_{1} | \boldsymbol{x}\right)+\ldots+1 \times P\left(c_{i-1} | \boldsymbol{x}\right)+0 \times P\left(c_{0} | \boldsymbol{x}\right)+1 \times P\left(c_{i+1} | \boldsymbol{x}\right)+\ldots+1 \times P\left(c_{N} | \boldsymbol{x}\right) \\ &=P\left(c_{1} | \boldsymbol{x}\right)+\ldots+P\left(c_{i-1} | \boldsymbol{x}\right)+P\left(c_{i+1} | \boldsymbol{x}\right)+\ldots+P\left(c_{N} | \boldsymbol{x}\right) \end{aligned}</script><p>由于$\sum_{j=1}^{N} P\left(c_{j} | \boldsymbol{x}\right)=1$，所以</p><script type="math/tex; mode=display">R\left(c_{i} | \boldsymbol{x}\right)=1-P\left(c_{i} | \boldsymbol{x}\right)</script><p>于是，最小化错误率的贝叶斯最优分类器为</p><script type="math/tex; mode=display">h^{\star}(x)=\underset{c \in \mathcal{Y}}{\arg \min } R(c | \boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \min }(1-P(c | \boldsymbol{x}))=\underset{c \in \mathcal{Y}}{\arg \max } P(c | \boldsymbol{x})</script><h4 id="多元正态分布参数的极大似然估计"><a href="#多元正态分布参数的极大似然估计" class="headerlink" title="多元正态分布参数的极大似然估计"></a>多元正态分布参数的极大似然估计</h4><p>已知对数似然函数为</p><script type="math/tex; mode=display">L L\left(\boldsymbol{\theta}_{c}\right)=\sum_{\boldsymbol{x} \in D_{e}} \log P\left(\boldsymbol{x} | \boldsymbol{\theta}_ {c}\right)</script><p>为了便于后续计算，我们令1og的底数为e，则对数似然函数可化为</p><script type="math/tex; mode=display">L L\left(\boldsymbol{\theta}_{c}\right)=\sum_{\boldsymbol{x} \in D_{c}} \ln P\left(\boldsymbol{x} | \boldsymbol{\theta}_ {c}\right)</script><p>由于$P\left(\boldsymbol{x} | \boldsymbol{\theta}_ {c}\right)=P(\boldsymbol{x} | c) \sim \mathcal{N}\left(\boldsymbol{\mu}_{c}, \boldsymbol{\sigma}_{c}^{2}\right)$，所以</p><script type="math/tex; mode=display">P\left(\boldsymbol{x} | \boldsymbol{\theta}_{c}\right)=\frac{1}{\sqrt{(2 \pi)^{d}\left|\boldsymbol{\Sigma}_{c}\right|}} \exp \left(-\frac{1}{2}\left(\boldsymbol{x}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}_ {c}\right)\right)</script><p>其中，d表示$\boldsymbol{x}$的维数，$\boldsymbol{\Sigma}_{c}=\boldsymbol{\sigma}_{c}^{2}$为对称正定协方差矩阵，$\left|\boldsymbol{\Sigma}_ {c}\right|$表示$\sum_{c}$的行列式，将上式代入对数似然函数可得</p><script type="math/tex; mode=display">L L\left(\boldsymbol{\theta}_{c}\right)=\sum_{\boldsymbol{x} \in D_{e}} \ln \left[\frac{1}{\sqrt{(2 \pi)^{d}\left|\boldsymbol{\Sigma}_{c}\right|}} \exp \left(-\frac{1}{2}\left(\boldsymbol{x}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}_{c}\right)\right)\right]</script><p>令$\left|D_{c}\right|=N$，则对数似然函数可化为</p><script type="math/tex; mode=display">\begin{aligned} L L\left(\boldsymbol{\theta}_{c}\right) &=\sum_{i=1}^{N} \ln \left[\frac{1}{\sqrt{(2 \pi)^{d}\left|\boldsymbol{\Sigma}_{c}\right|}} \exp \left(-\frac{1}{2}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right)\right] \\ &=\sum_{i=1}^{N} \ln \left[\frac{1}{\sqrt{(2 \pi)^{d}}} \cdot \frac{1}{\sqrt{\left|\boldsymbol{\Sigma}_{c}\right|}} \cdot \exp \left(-\frac{1}{2}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right)\right] \\&=\sum_{i=1}^{N}\left\{\ln \frac{1}{\sqrt{(2 \pi)^{d}}}+\ln \frac{1}{\sqrt{\left|\Sigma_{c}\right|}}+\ln \left[\exp \left(-\frac{1}{2}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right)\right]\right\}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} L L\left(\boldsymbol{\theta}_{c}\right) &=\sum_{i=1}^{N}\left\{-\frac{d}{2} \ln (2 \pi)-\frac{1}{2} \ln \left|\boldsymbol{\Sigma}_{c}\right|-\frac{1}{2}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right.\\ &=-\frac{N d}{2} \ln (2 \pi)-\frac{N}{2} \ln \left|\mathbf{\Sigma}_{c}\right|-\frac{1}{2} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right) \end{aligned}</script><p>由于参数$\boldsymbol{\theta}_ {c}$的极大似然估$\hat{\boldsymbol{\theta}}_ {c}$为</p><script type="math/tex; mode=display">\hat{\boldsymbol{\theta}}_{c}=\underset{\boldsymbol{\theta}_{c}}{\arg \max } L L\left(\boldsymbol{\theta}_ {c}\right)</script><p>所以接来下只需要求出使得对数似然函数$L L\left(\boldsymbol{\theta}_ {c}\right)$取到最大值的$\hat{\boldsymbol{\mu}}_ {c}$和$\hat{\mathbf{\Sigma}}_ {c}$，也就求出了$\hat{\boldsymbol{\theta}}_{c}$</p><p>对$L L\left(\boldsymbol{\theta}_ {c}\right)$关于$\boldsymbol{\mu}_ {c}$，求偏导</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\mu}_{c}} &=\frac{\partial}{\partial \boldsymbol{\mu}_{c}}\left[-\frac{N d}{2}-\ln (2 \pi)-\frac{N}{2} \ln \left|\mathbf{\Sigma}_{c}\right|-\frac{1}{2} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right] \\ &=\frac{\partial}{\partial \boldsymbol{\mu}_{c}}\left[-\frac{1}{2} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right] \\&=-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol \mu_{c}}\left[\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right] \\&=-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol{\mu}_{c}}\left[\left(\boldsymbol{x}_{i}^{T}-\boldsymbol{\mu}_{c}^{T} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right]\right. \\&=-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol\mu_{c}}\left[\left(\boldsymbol{x}_{i}^{T}-\boldsymbol{\mu}_{c}^{T}\right)\left(\boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}-\boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}\right)\right] \\&=-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol{\mu}_{c}}\left[\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}-\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}-\boldsymbol{\mu}_{c}^{T} \mathbf{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}+\boldsymbol{\mu}_{c}^{T} \mathbf{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}\right]\end{aligned}</script><p>由于$\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_ {c}$的计算结果为标量，所以</p><script type="math/tex; mode=display">\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}=\left(\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}\right)^{T}=\boldsymbol{\mu}_{c}^{T}\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T} \boldsymbol{x}_{i}=\boldsymbol{\mu}_{c}^{T}\left(\boldsymbol{\Sigma}_{c}^{T}\right)^{-1} \boldsymbol{x}_{i}=\boldsymbol{\mu}_{c}^{T} \mathbf{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}</script><p>于是上式可以进一步化为</p><script type="math/tex; mode=display">\frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\mu}_{c}}=-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol{\mu}_{c}}\left[\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}-2 \boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}+\boldsymbol{\mu}_{c}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}\right]</script><p>由矩阵微分公式$\frac{\partial \boldsymbol{a}^{T} \boldsymbol{x}}{\partial \boldsymbol{x}}=\boldsymbol{a}, \frac{\partial \boldsymbol{x}^{T} \boldsymbol{B} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\boldsymbol{B}+\boldsymbol{B}^{T}\right) \boldsymbol{x}$可得</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\mu}_{c}} &=-\frac{1}{2} \sum_{i=1}^{N}\left[0-\left(2 \boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1}\right)^{T}+\left(\boldsymbol{\Sigma}_{c}^{-1}+\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T}\right) \boldsymbol{\mu}_{c}\right] \\ &=-\frac{1}{2} \sum_{i=1}^{N}\left[-\left(2\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T} \boldsymbol{x}_{i}\right)+\left(\boldsymbol{\Sigma}_{c}^{-1}+\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T}\right) \boldsymbol{\mu}_{c}\right] \\ &=-\frac{1}{2} \sum_{i=1}^{N}\left[-\left(2 \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}\right)+2 \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}\right] \\ &=\sum_{i=1}^{N} \Sigma_{c}^{-1} \boldsymbol{x}_{i}-N \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c} \end{aligned}</script><p>令偏导数等于0可得</p><script type="math/tex; mode=display">\begin{array}{c}{\frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\mu}_{c}}=\sum_{i=1}^{N} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}-N \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}=0} \\ {N \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}=\sum_{i=1}^{N} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}} \\ {N \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}=\boldsymbol{\Sigma}_{c}^{-1} \sum_{i=1}^{N} \boldsymbol{x}_{i}} \\ {N \boldsymbol{\mu}_{c}=\sum_{i=1}^{N} \boldsymbol{x}_{i}} \\ \boldsymbol{\mu}_{c}=\frac{1}{N} \sum_{i=1}^{N} \boldsymbol{x}_{i} \Rightarrow \hat{\boldsymbol{\mu}}_{c}=\frac{1}{N} \sum_{i=1}^{N} \boldsymbol{x}_{i} \end{array}</script><p>对$L L\left(\boldsymbol{\theta}_{c}\right)$关于$\Sigma_{c}$求偏导</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\Sigma}_{c}} &=\frac{\partial}{\partial \boldsymbol{\Sigma}_{c}}\left[-\frac{N d}{2} \ln (2 \pi)-\frac{N}{2} \ln \left|\boldsymbol{\Sigma}_{c}\right|-\frac{1}{2} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right] \\ &=\frac{\partial}{\partial \boldsymbol{\Sigma}_{c}}\left[-\frac{N}{2} \ln \left|\boldsymbol{\Sigma}_{c}\right|-\frac{1}{2} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right] \\&=-\frac{N}{2} \cdot \frac{\partial}{\partial \boldsymbol{\Sigma}_{c}}\left[\ln \left|\boldsymbol{\Sigma}_{c}\right|\right]-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol{\Sigma}_{c}}\left[\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right]\end{aligned}</script><p>由矩阵微分公式$\frac{\partial|\mathbf{X}|}{\partial \mathbf{X}}=|\mathbf{X}| \cdot\left(\mathbf{X}^{-1}\right)^{T}, \frac{\partial \boldsymbol{a}^{T} \mathbf{X}^{-1} \boldsymbol{b}}{\partial \mathbf{X}}=-\mathbf{X}^{-T} \boldsymbol{a} \boldsymbol{b}^{T} \mathbf{X}^{-T}$可得</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\Sigma}_{c}} &=-\frac{N}{2} \cdot \frac{1}{\left|\boldsymbol{\Sigma}_{c}\right|} \cdot\left|\boldsymbol{\Sigma}_{c}\right| \cdot\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T}-\frac{1}{2} \sum_{i=1}^{N}\left[-\boldsymbol{\Sigma}_{c}^{-T}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-T}\right] \\ &=-\frac{N}{2} \cdot\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T}-\frac{1}{2} \sum_{i=1}^{N}\left[-\boldsymbol{\Sigma}_{c}^{-T}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-T}\right] \\ &=-\frac{N}{2} \boldsymbol{\Sigma}_{c}^{-1}+\frac{1}{2} \sum_{i=1}^{N}\left[\boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-1}\right] \end{aligned}</script><p>令偏导数等于0可得</p><script type="math/tex; mode=display">\frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\Sigma}_{c}}=-\frac{N}{2} \boldsymbol{\Sigma}_{c}^{-1}+\frac{1}{2} \sum_{i=1}^{N}\left[\boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-1}\right]=0</script><p>$-\frac{N}{2} \boldsymbol{\Sigma}_{c}^{-1}=-\frac{1}{2} \sum_{i=1}^{N}\left[\boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-1}\right]$<br>$N \boldsymbol{\Sigma}_{c}^{-1}=\sum_{i=1}^{N}\left[\boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-1}\right]$<br>$N \boldsymbol{\Sigma}_{c}^{-1}=\boldsymbol{\Sigma}_{c}^{-1}\left[\sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T}\right] \boldsymbol{\Sigma}_{c}^{-1}$<br>$N=\mathbf{\Sigma}_{c}^{-1}\left[\sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T}\right]$<br>$\boldsymbol{\Sigma}_{c} N=\sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T}$<br>$\mathbf{\Sigma}_{c}=\frac{1}{N} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \Rightarrow \hat{\boldsymbol{\Sigma}}_{c}=\frac{1}{N} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T}$</p><h4 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h4><p>已知最小化分类错误率的贝叶斯最优分类器为</p><script type="math/tex; mode=display">h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c | \boldsymbol{x})</script><p>又由贝叶斯定理可知</p><script type="math/tex; mode=display">P(c | \boldsymbol{x})=\frac{P(\boldsymbol{x}, c)}{P(\boldsymbol{x})}=\frac{P(c) P(\boldsymbol{x} | c)}{P(\boldsymbol{x})}</script><p>所以</p><script type="math/tex; mode=display">h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } \frac{P(c) P(\boldsymbol{x} | c)}{P(\boldsymbol{x})}=\underset{c \in \mathcal{Y}}{\arg \max } P(c) P(\boldsymbol{x} | c)</script><p>已知属性条件独立性假设为</p><script type="math/tex; mode=display">P(\boldsymbol{x} | c)=P\left(x_{1}, x_{2}, \ldots, x_{d} | c\right)=\prod_{i=1}^{d} P\left(x_{i} | c\right)</script><p>所以</p><script type="math/tex; mode=display">h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c) \prod_{i=1}^{d} P\left(x_{i} | c\right)</script><p>此即为朴素贝叶斯分类器的表达式</p><p>对于P（c）：它表示的是样本空间中各类样本所占的比例，根据大数定律，当训练集包含充足的独立同分布样本时，P（c）可通过各类样本出现的频率来进行估计，也即</p><script type="math/tex; mode=display">P(c)=\frac{\left|D_{c}\right|}{|D|}</script><p>其中，D表示训练集，$|D|$表示D中的样本个数，$D_{c}$表示训练集D中第c类样本组成的集合，$\left|D_{c}\right|$表示集合$D_{c}$中的样本个数。</p><p>对于$P\left(x_{i} | c\right) :$<br>若样本的第i个属性$x_{i}$取值为连续值<br>我们假设该属性的取值服从正态分布，也即</p><script type="math/tex; mode=display">P\left(x_{i} | c\right) \sim \mathcal{N}\left(\mu_{c, i}, \sigma_{c, i}^{2}\right) \Rightarrow P\left(x_{i} | c\right)=\frac{1}{\sqrt{2 \pi} \sigma_{c, i}} \exp \left(-\frac{\left(x_{i}-\mu_{c, i}\right)^{2}}{2 \sigma_{c, i}^{2}}\right)</script><p>其中正态分布的参数可以用极大似然估计法推得：$\mu_{C, i}$和$\sigma_{c, i}^{2}$即为第c类样本在第i个属性上取值的均值和方差</p><p>对于$P\left(x_{i} | c\right) :$<br>若样本的第i个属性$x_{i}$取值为离散值：<br>同样根据极大似然估计法，我们用其频率值作为其概率值的估计值，也即</p><script type="math/tex; mode=display">P\left(x_{i} | c\right)=\frac{\left|D_{c, x_{i}}\right|}{\left|D_{c}\right|}</script><p>其中，$D_{c, x_{i}}$表示$D_{c}$中在第个属性上取值为$x_{i}$的样本组成的集合。</p><p>例：现将一枚6面骰子抛掷10次，抛掷出的点数分别为2、3、2、5、4、.6、1、3、4、2，试基于此抛掷结果估计这枚骰子抛掷出各个点数的概率。<br>解：设这枚骰子抛掷出点数的概率为$P_{i}$，根据极大似然估计法可以写出似然函数为</p><script type="math/tex; mode=display">L(\theta)=P_{1} \times P_{2}^{3} \times P_{3}^{2} \times P_{4}^{2} \times P_{5} \times P_{6}</script><p>其对数似然函数即为</p><script type="math/tex; mode=display">\begin{aligned} L(\theta) & :=\ln L(\theta)=\ln \left(P_{1} \times P_{2}^{3} \times P_{3}^{2} \times P_{4}^{2} \times P_{5} \times P_{6}\right) \\ &=\ln P_{1}+3 \ln P_{2}+2 \ln P_{3}+2 \ln P_{4}+\ln P_{5}+\ln P_{6} \end{aligned}</script><p>由于$P_{i}$之间满足如下约束</p><script type="math/tex; mode=display">P_{1}+P_{2}+P_{3}+P_{4}+P_{5}+P_{6}=1</script><p>所以此时最大化对数似然函数属于带约束的最优化问题，也即</p><script type="math/tex; mode=display">\begin{array}{ll}{\underset{\theta}{max}} & {L(\theta)=\ln P_{1}+3 \ln P_{2}+2 \ln P_{3}+2 \ln P_{4}+\ln P_{5}+\ln P_{6}} \\ {\text {s.t.}} & {P_{1}+P_{2}+P_{3}+P_{4}+P_{5}+P_{6}=1}\end{array}</script><p>由拉格朗日乘子法可得拉格拉格朗日函数为</p><script type="math/tex; mode=display">\mathcal{L}(\theta)=\ln P_{1}+3 \ln P_{2}+2 \ln P_{3}+2 \ln P_{4}+\ln P_{5}+\ln P_{6}+\lambda\left(P_{1}+P_{2}+P_{3}+P_{4}+P_{5}+P_{6}-1\right)</script><p>对拉格朗日函数$\mathcal{L}(\theta)$分别关于$P_{i}$求偏导，然后令其等于0可得</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \mathcal{L}(\theta)}{\partial P_{1}}&=\frac{\partial}{\partial P_{1}}\left[\ln P_{1}+3 \ln P_{2}+2 \ln P_{3}+2 \ln P_{4}+\ln P_{5}+\ln P_{6}+\lambda\left(P_{1}+P_{2}+P_{3}+P_{4}+P_{5}+P_{6}-1\right)\right]=0\\&=\frac{\partial}{\partial P_{1}}\left(\ln P_{1}+\lambda P_{1}\right)=0\\&=\frac{1}{P_{1}}+\lambda=0\\&\Rightarrow \lambda=-\frac{1}{P_{1}}\end{aligned}</script><p>同理可求得：</p><script type="math/tex; mode=display">\lambda=-\frac{1}{P_{1}}=-\frac{3}{P_{2}}=-\frac{2}{P_{3}}=-\frac{2}{P_{4}}=-\frac{1}{P_{5}}=-\frac{1}{P_{6}}</script><p>又因为</p><script type="math/tex; mode=display">P_{1}+P_{2}+P_{3}+P_{4}+P_{5}+P_{6}=1</script><p>所以最终解得</p><script type="math/tex; mode=display">P_{1}=\frac{1}{10}, P_{2}=\frac{3}{10}, P_{3}=\frac{2}{10}, P_{4}=\frac{2}{10}, P_{5}=\frac{1}{10}, P_{6}=\frac{1}{10}</script><p>此时抛掷出各个点数的概率值与其频率值相等</p><h4 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h4><h5 id="1-EM算法的引入"><a href="#1-EM算法的引入" class="headerlink" title="1.EM算法的引入"></a>1.EM算法的引入</h5><h6 id="为什么需要EM算法"><a href="#为什么需要EM算法" class="headerlink" title="为什么需要EM算法"></a>为什么需要EM算法</h6><p>概率模型有时既含有观测变量，又含有隐变量或者潜在变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或者贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。<br>EM算法就是含有隐变量的概率模型参数的极大似然估计法。</p><h5 id="2-EM算法的例子"><a href="#2-EM算法的例子" class="headerlink" title="2.EM算法的例子"></a>2.EM算法的例子</h5><h6 id="三硬币模型"><a href="#三硬币模型" class="headerlink" title="三硬币模型"></a>三硬币模型</h6><p>《统计学习方法》例9.1（三硬币模型）：<br>假设有3枚硬币，分别记作A，B，c.这些硬币正面出现的概率分别 T，р和q。进行如下掷硬币试验：先掷硬币A，根据其结果选出硬币B或硬币C，正面选硬币B，反面选硬币C；然后掷选出的硬币，掷硬币的结果，出现E面记作1，出现反面记作0；独立地重复n此实验（这里，n=10），观测结果如下：<br>1，1，0，1，0，0，1，0，1，1<br>假设只能观测到掷硬币的结果，不能观测掷硬币的过程。问如何估计三硬币正面出现的概率，即三硬币模型的参数。</p><p>解：对每一次试验可以进行如下建模</p><script type="math/tex; mode=display">\begin{aligned} P(y | \theta) &=\sum_{z} P(y, z | \theta)=\sum_{z} P(z | \theta) P(y | z, \theta) \\ &=P(z=1 | \theta) P(y | z=1, \theta)+P(z=0 | \theta) P(y | z=0, \theta) \\ &=\left\{\begin{array}{ll}{\pi p+(1-\pi) q,} & {\text { if } y=1} \\ {\pi(1-p)+(1-\pi)(1-q),} & {\text { if } y=0} \end{array}\right.\\ &=\pi p^{y}(1-p)^{1-y}+(1-\pi) q^{y}(1-q)^{1-y}\end{aligned}</script><p>其中，随机变量y是观测变量，表示一次试验观测的结果是1或0；随机变量z是隐变量，表示未观测到的掷硬币A的结果；<br>$\theta=(\pi, p, q)$是模型参数。将观测数据表示为$Y=\left(Y_{1}, Y_{2}, \ldots, Y_{n}\right)^{T}$，未观测数据表示为$Z=\left(Z_{1}, Z_{2}, \ldots, Z_{n}\right)^{T}$则观测数据的似然函数为：</p><script type="math/tex; mode=display">P(Y | \theta)=\sum_{Z} P(Z | \theta) P(Y | Z, \theta)=\prod_{j=1}^{n} P\left(y_{j} | \theta\right)=\prod_{j=1}^{n}\left[\pi p_{j}^{y_{j}}(1-p)^{1-y_{j}}+(1-\pi) q^{y_{j}}(1-q)^{1-y_{j}}\right]</script><p>考虑求模型参数$\theta=(\pi, p, q)$的极大似然估计，即使用对数似然函数来进行参数估计可得：</p><script type="math/tex; mode=display">\begin{aligned} \hat{\theta} &=\arg \max _{\theta} \ln P(Y | \theta) \\ &=\arg \max _{\theta} \ln \prod_{j=1}^{n}\left[\pi p^{y_{j}}(1-p)^{1-y_{j}}+(1-\pi) q^{y_{j}}(1-q)^{1-y_{j}}\right] \\ &=\arg \max _{\theta} \sum_{j=1}^{n} \ln \left[\pi p^{y_{j}}(1-p)^{1-y_{j}}+(1-\pi) q^{y_{j}}(1-q)^{i-y_{j}}\right] \end{aligned}</script><p>上式没有解析解，也就是没办法直接解出$\pi, p, q$恰好等于某个常数，只能用送代的方法来进行求解。</p><h5 id="3-EM算法的导出"><a href="#3-EM算法的导出" class="headerlink" title="3.EM算法的导出"></a>3.EM算法的导出</h5><h6 id="Jensen（琴生）不等式"><a href="#Jensen（琴生）不等式" class="headerlink" title="Jensen（琴生）不等式"></a>Jensen（琴生）不等式</h6><p>若f是凸函数，则：</p><script type="math/tex; mode=display">f\left(t x_{1}+(1-t) x_{2}\right) \leq t f\left(x_{1}\right)+(1-t) f\left(x_{2}\right)</script><p>ps:$t x_{1}+(1-t) x_{2}$ 代表取遍$x_{1}$和$x_{2}$之间所有的点<br>其中，$t \in[0,1]$。同理，若f是凹函数，则只需将上式中的$\leq$换成$\geq$即可。<br>将上式中的t推广到n个同样成立，也即：</p><script type="math/tex; mode=display">f\left(t_{1} x_{1}+t_{2} x_{2}+\ldots+t_{n} x_{n}\right) \leq t_{1} f\left(x_{1}\right)+t_{2} f\left(x_{2}\right)+\ldots+t_{n} f\left(x_{n}\right)</script><p>其中，$t_{1}, t_{2}, \dots, t_{n} \in[0,1], t_{1}+t_{2}+\ldots+t_{n}=1$。在概率论中常以以下形式出现：</p><script type="math/tex; mode=display">\varphi(E[X]) \leq E[\varphi(X)]</script><p>其中，$X$是随机变量，$\varphi$是凸函数，$E[X]$表示X的期望。</p><h6 id="EM算法的推导"><a href="#EM算法的推导" class="headerlink" title="EM算法的推导"></a>EM算法的推导</h6><p>我们面对一个含有隐变量的概率模型，目标是极大化观测数据Y关于参数日的对数似然函数，即极大化：</p><script type="math/tex; mode=display">L(\theta)=\ln P(Y | \theta)=\ln \sum_{Z} P(Y, Z | \theta)=\ln \left(\sum_{Z} P(Y | Z, \theta) P(Z | \theta)\right)</script><p>注意到这一极大化的主要困难是上式中有未观测数据Z并有包含和（Z为离散型时）或者积分（Z为连续型时）的对数。EM算法采用的是通过迭代逐步近似极大化$L(\theta)$：假设在第次迭代后$\theta$的估计值$\theta^{(i)}$，我们希望新的估计值$\theta$能使 $L(\theta)$增加，即$L(\theta)&gt;L\left(\theta^{(i)}\right)$，并逐步达到极大值。为此，我们考虑两者的差：</p><script type="math/tex; mode=display">\begin{aligned} L(\theta)-L\left(\theta^{(i)}\right) &=\ln \left(\sum_{Z} P(Y | Z, \theta) P(Z | \theta)\right)-\ln P\left(Y | \theta^{(i)}\right) \\ &=\ln \left(\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right)}\right)-\ln P\left(Y | \theta^{(i)}\right) \end{aligned}</script><p>套用琴生不等式可得：</p><script type="math/tex; mode=display">\geq \sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right)}-\ln P\left(Y | \theta^{(i)}\right)</script><script type="math/tex; mode=display">\begin{aligned} L(\theta)-L\left(\theta^{(i)}\right) & \geq \sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right)}-\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \cdot \ln P\left(Y | \theta^{(i)}\right) \\ &=\sum_{Z} P\left(Z | Y, \theta^{(i)}\right)\left(\ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right)}-\ln P\left(Y | \theta^{(i)}\right)\right) \\ &=\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right) P\left(Y | \theta^{(i)}\right)} \end{aligned}</script><script type="math/tex; mode=display">L(\theta) \geq L\left(\theta^{(i)}\right)+\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right) P\left(Y | \theta^{(i)}\right)}</script><p>令：</p><script type="math/tex; mode=display">B\left(\theta, \theta^{(i)}\right)=L\left(\theta^{(i)}\right)+\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right) P\left(Y | \theta^{(i)}\right)}</script><p>则：</p><script type="math/tex; mode=display">L(\theta) \geq B\left(\theta, \theta^{(i)}\right)</script><p>即函数$B\left(\theta, \theta^{(i)}\right)$是$L(\theta)$的一个下界，此时若设$\theta^{(i+1)}$使得$B\left(\theta, \theta^{(i)}\right)$达到极大也即</p><script type="math/tex; mode=display">B\left(\theta^{(i+1)}, \theta^{(i)}\right) \geq B\left(\theta^{(i)}, \theta^{(i)}\right)</script><p>由于$B\left(\theta^{(i)}, \theta^{(i)}\right)=L\left(\theta^{(i)}\right)$，所以可以进一步推得：</p><script type="math/tex; mode=display">L\left(\theta^{(i+1)}\right) \geq B\left(\theta^{(i+1)}, \theta^{(i)}\right) \geq B\left(\theta^{(i)}, \theta^{(i)}\right)=L\left(\theta^{(i)}\right)</script><script type="math/tex; mode=display">L\left(\theta^{(i+1)}\right) \geq L\left(\theta^{(i)}\right)</script><p>因此，任何可以使$B\left(\theta, \theta^{(i)}\right)$增大的$\theta$，也可以使$L(\theta)$增大，于是问题就转化为了求解能使得$B\left(\theta, \theta^{(i)}\right)$达到极大的$\theta^{(i+1)}$，即</p><script type="math/tex; mode=display">\begin{aligned} \theta^{(i+1)} &=\underset{\theta}{\arg \max } B\left(\theta, \theta^{(i)}\right) \\ &=\underset{\theta}{\arg \max }\left(L\left(\partial^{(i)}\right)+\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right) P\left(Y | \theta^{(i)}\right)}\right) \\ &=\arg \max _{\theta}\left(\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln (P(Y | Z, \theta) P(Z | \theta))\right) \\ &=\underset{\theta}{\arg \max }\left(\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln P(Y, Z | \theta)\right) \\ &=\underset{\theta}{\arg \max } Q\left(\theta, \theta^{(i)}\right) \end{aligned}</script><p>到此即完成了EM算法的一次迭代，求出的$\theta^{(i+1)}$作为下一次迭代的初始$\theta(i)$。综上可以总结出EM算法的”E步”和”M步”分别为：<br>E步：计算完全数据的对数似然函数$\ln P(Y, Z | \theta)$关于在给定观测数据Y和当前参数$\theta$下对未观测数据Z的条件概率分布$P\left(Z | Y, \theta^{(i)}\right)$期望，即Q函数：</p><script type="math/tex; mode=display">Q\left(\theta, \theta^{(i)}\right)=E_{Z}\left[\ln P(Y, Z | \theta) | Y, \theta^{(i)}\right]=\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln P(Y, Z | \theta)</script><p>M步：求使得Q函数到达极大的$\theta^{(i+1)}$。</p><h5 id="4-EM算法求解例子"><a href="#4-EM算法求解例子" class="headerlink" title="4.EM算法求解例子"></a>4.EM算法求解例子</h5><h6 id="用EM算法求解三硬币模型"><a href="#用EM算法求解三硬币模型" class="headerlink" title="用EM算法求解三硬币模型"></a>用EM算法求解三硬币模型</h6><p>《统计学习方法》例9.1（三硬币模型）：<br>假设有3枚硬币，分别记作A，B，c.这些硬币正面出现的概率分别 T，р和q。进行如下掷硬币试验：先掷硬币A，根据其结果选出硬币B或硬币C，正面选硬币B，反面选硬币C；然后掷选出的硬币，掷硬币的结果，出现E面记作1，出现反面记作0；独立地重复n此实验（这里，n=10），观测结果如下：<br>1，1，0，1，0，0，1，0，1，1<br>假设只能观测到掷硬币的结果，不能观测掷硬币的过程。问如何估计三硬币正面出现的概率，即三硬币模型的参数。</p><p>解：对每一次试验可以进行如下建模</p><script type="math/tex; mode=display">\begin{aligned} P(y | \theta) &=\sum_{z} P(y, z | \theta)=\sum_{z} P(z | \theta) P(y | z, \theta) \\ &=P(z=1 | \theta) P(y | z=1, \theta)+P(z=0 | \theta) P(y | z=0, \theta) \\ &=\left\{\begin{array}{ll}{\pi p+(1-\pi) q,} & {\text { if } y=1} \\ {\pi(1-p)+(1-\pi)(1-q),} & {\text { if } y=0} \end{array}\right.\\ &=\pi p^{y}(1-p)^{1-y}+(1-\pi) q^{y}(1-q)^{1-y}\end{aligned}</script><p>其中，随机变量y是观测变量，表示一次试验观测的结果是1或0；随机变量z是隐变量，表示未观测到的掷硬币A的结果；$\theta=(\pi, p, q)$是模型参数。</p><p>求解思路：<br>E步：导出Q函数<br>M步：求使得Q函数达到极大的$\theta^{(i+1)}=\left(\pi^{(i+1)}, p^{(i+1)}, q^{(i+1)}\right)$</p><p>E步：导出Q函数</p><script type="math/tex; mode=display">\begin{aligned} Q\left(\theta | \theta^{(i)}\right) &=\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln P(Y, Z | \theta) \\ &=\sum_{z_{1}, z_{2}, \ldots, z_{N}}\left\{\prod_{i=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right) \ln \left[\prod_{j=1}^{N} P\left(y_{j}, z_{j} | \theta\right)\right]\right\} \\ &=\sum_{z_{1}, z_{2}, \ldots, z_{N}}\left\{\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right)\left[\sum_{j=1}^{N} \ln P\left(y_{j}, z_{j} | \theta\right)\right]\right\} \\ &=\sum_{z_{1}, z_{2}, \ldots, z_{N}}\left\{\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right)\left[\ln P\left(y_{1}, z_{1} | \theta\right)+\sum_{j=2}^{N} \ln P\left(y_{j}, z_{j} | \theta\right)\right]\right\} \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} Q\left(\theta | \theta^{(i)}\right) &=\sum_{z_{i}, z_{2}, \ldots, z_{N}}\left\{\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right) \cdot \ln P\left(y_{1}, z_{1} | \theta\right)+\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right)\left[\sum_{j=2}^{N} \ln P\left(y_{j}, z_{j} | \theta\right)\right]\right\} \\ &=\sum_{z_{1}, z_{2}, \ldots, z_{N}}\left\{\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right) \cdot \ln P\left(y_{1}, z_{1} | \theta\right)\right\}+\sum_{z_{i}, z_{2}, \ldots, z_{N}}^{N}\left\{\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right)\left[\sum_{j=2}^{N} \ln P\left(y_{j}, z_{j} | \theta\right)\right]\right\} \end{aligned}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;第7章-朴素贝叶斯&quot;&gt;&lt;a href=&quot;#第7章-朴素贝叶斯&quot; class=&quot;headerlink&quot; title=&quot;第7章 朴素贝叶斯&quot;&gt;&lt;/a&gt;第7章 朴素贝叶斯&lt;/h3&gt;&lt;h4 id=&quot;贝叶斯判定准则&quot;&gt;&lt;a href=&quot;#贝叶斯判定准则&quot; class=&quot;he
      
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://minhzou.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》第6章 支持向量模型 公式推导</title>
    <link href="http://minhzou.top/2019/07/12/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC6%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%20%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://minhzou.top/2019/07/12/《机器学习》第6章 支持向量模型 公式推导/</id>
    <published>2019-07-12T15:15:11.753Z</published>
    <updated>2019-07-17T12:40:09.451Z</updated>
    
    <content type="html"><![CDATA[<h3 id="第6章-支持向量模型"><a href="#第6章-支持向量模型" class="headerlink" title="第6章 支持向量模型"></a>第6章 支持向量模型</h3><h4 id="支持向量机公式推导"><a href="#支持向量机公式推导" class="headerlink" title="支持向量机公式推导"></a>支持向量机公式推导</h4><h5 id="SVM问题的原始形式推导"><a href="#SVM问题的原始形式推导" class="headerlink" title="SVM问题的原始形式推导"></a>SVM问题的原始形式推导</h5><h6 id="1-常见的几何性质（欧氏空间）"><a href="#1-常见的几何性质（欧氏空间）" class="headerlink" title="1. 常见的几何性质（欧氏空间）"></a>1. 常见的几何性质（欧氏空间）</h6><script type="math/tex; mode=display">\left\{\begin{array}{l}{w^{T} x_{1}+b=0} \\ {w^{T} x_{2}+b=0} \\ {w^{T}\left(x_{1}-x_{2}\right)=0}\end{array}\right.</script><p>$w$是法向量</p><script type="math/tex; mode=display">\left\{\begin{array}{l}{w^{T} x_{1}+b=0} \\ {\lambda w^{T} \frac{w}{\|w\|}+b=0} \\ {\text { offset }} =\frac{-b}{\|w\|}\end{array}\right.</script><p>$\frac{-b}{|\boldsymbol{w}|}$是原点到平面的”距离”</p><script type="math/tex; mode=display">r=\frac{\left|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right|}{\|\boldsymbol{w}\|}</script><p>r是点到平面的距离</p><script type="math/tex; mode=display">\begin{aligned} r &=\left\|\frac{\boldsymbol{w}^{T} \boldsymbol{x}}{\|\boldsymbol{w}\|^{2}} \boldsymbol{w}-\frac{-b}{\|\boldsymbol{w}\|^{2}} \boldsymbol{w}\right\| \\ &=\left\|\frac{\boldsymbol{w}^{T} \boldsymbol{x}}{\|\boldsymbol{w}\|^{2}}+\frac{b}{\|\boldsymbol{w}\|^{2}}\right\|\|\boldsymbol{w}\| \\ &=\frac{\left\|\boldsymbol{w}^{T} \boldsymbol{x}+b\right\|}{\|\boldsymbol{w}\|} \\ &=\frac{\left|\boldsymbol{w}^{T} \boldsymbol{x}+b\right|}{\|\boldsymbol{w}\|} \end{aligned}</script><h6 id="SVM原始公式的导出-核心是最大化间隔"><a href="#SVM原始公式的导出-核心是最大化间隔" class="headerlink" title="SVM原始公式的导出(核心是最大化间隔)"></a>SVM原始公式的导出(核心是最大化间隔)</h6><p>（1）任一数据点到平面距离（假设w可以分开数据集）:</p><script type="math/tex; mode=display">r_{i}=\frac{y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)}{\|\boldsymbol{w}\|}</script><p>（2）最接近平面的距离</p><script type="math/tex; mode=display">d=\min _{i} y_{i}\left(\frac{\boldsymbol{w}^{T}}{\|\boldsymbol{w}\|} \boldsymbol{x}_{i}+\frac{b}{\|\boldsymbol{w}\|}\right)</script><p>（3）所有可能距离中，最大的距离。是间隔的一半</p><script type="math/tex; mode=display">\left\{\begin{array}{l}{d^{\star}=\max _{w, b} \min _{i} y_{i}\left(\frac{\boldsymbol{w}^{T}}{\|\boldsymbol{w}\|} \boldsymbol{x}_{i}+\frac{b}{\|\boldsymbol{w}\|}\right)} \\ {\gamma=2 d^{\star}=\max _{\boldsymbol{w}, b} \min _{i} 2 \times \boldsymbol{y}_{i}\left(\frac{\boldsymbol{w}^{T}}{\|\boldsymbol{w}\|} \boldsymbol{x}_{i}+\frac{b}{\|\boldsymbol{w}\|}\right)}\end{array}\right.</script><script type="math/tex; mode=display">\begin{array}{l}{\max _{w, b} 2 d} \\ {\text { s.t. } y_{i}\left(\frac{w^{T}}{\|w\|} x_{i}+\frac{b}{\|\boldsymbol{w}\|}\right) \geq d \text { for } i=1, \ldots, m} \\ {d>0}\end{array}</script><p>引入新的定义：函数间隔$\hat{d}=|\boldsymbol{w}| d$</p><script type="math/tex; mode=display">\begin{array}{l}{\max _{\boldsymbol{w}, b} \frac{2 \hat{d}}{\|\boldsymbol{w}\|}} \\ {\text { s.t. } \boldsymbol{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq \hat{d} \text { for } i=1, \ldots, N} \\ {\hat{d}>0}\end{array}</script><p>实际上，$\hat{d}$取值并不影响最优化问题的解！<br>所以我们可以始终取$\hat{d}$=1</p><script type="math/tex; mode=display">\begin{array}{ll}{\max _{w, b}} & {\frac{2}{\|w\|}} \\ {\text { s.t. }} & {y_{i}\left(w^{T} x_{i}+b\right) \geq 1, \quad i=1,2, \dots, m}\end{array}</script><script type="math/tex; mode=display">\begin{array}{ll}{\min _{w, b}} & {\frac{1}{2}\|w\|^{2}} \\ {\text { s.t. }} & {y_{i}\left(w^{T} x_{i}+b\right) \geq 1, \quad i=1,2, \ldots, m}\end{array}</script><h6 id="SVM的性质"><a href="#SVM的性质" class="headerlink" title="SVM的性质"></a>SVM的性质</h6><p>① 最大间隔超平面的存在唯一性<br>② 支持向量和间隔边界</p><p>存在性易得，现证唯一性：<br>假设有两个最优解$\left(\boldsymbol{w}_{1}^{\star}, b_{1}^{\star}\right),\left(\boldsymbol{w}_{2}^{\star}, b_{2}^{\star}\right)$<br>1）证$\boldsymbol{w}_{1}^{\star}=\boldsymbol{w}_{2}^{\star}$<br>根据假设，有$\left|\boldsymbol{w}_{1}^{\star}\right|=\left|\boldsymbol{w}_{2}^{\star}\right|=c \neq 0$,假设$w=\frac{w_{1}^{\star}+w_{2}^{\star}}{2}, b=\frac{b_{1}^{\star}+b_{2}^{\star}}{2}$<br>$c \leq| | w| |=\frac{\left|w_{i}^{\star}+w_{2}^{\star}\right|}{2} \leq \frac{ | w_{i}^{\star}|+| w_{2}^{\star} |}{2}=c \Rightarrow | w |\frac{\left|w_{1}^{\star}+w_{2}^{\star}\right|}{2}=\frac{\left|w_{1}^{\star}\right|+\left|w_{2}^{\star}\right|}{2}\Rightarrow \quad w_{1}^{\star}=k w_{2}^{\star}$<br>$\Rightarrow k=\left\{\begin{array}{l}{1} \\ {-1}\end{array} \Rightarrow w=\left\{\begin{array}{ll}{w_{i}^{\star}=w_{i}^{\star}} &amp; {\text { if } k=1} \\ {0} &amp; {\text { if } k=-1}\end{array} \Rightarrow w_{1}^{\star}=w_{2}^{\star}\right.\right.$<br>2）证$b_{1}^{\star}=b_{2}^{\star}$，设$x_{1}^{\prime}, x_{2}^{\prime} \in\left\{x_{i} | y_{i}=+1\right\} \quad x_{1}^{\prime \prime}, x_{2}^{\prime \prime} \in\left\{x_{i} | y_{i}=-1\right\}$，且有$\left\{\begin{array}{l}{\left(w^{T} x_{1}^{\prime}+b_{1}^{\star}\right)=1} \\ {\left(w^{T} x_{2}^{\prime}+b_{2}^{\star}\right)=1} \\ {\left(w^{T} x_{1}^{\prime \prime}+b_{1}^{\star}\right)=-1} \\ {\left(w^{T} x_{2}^{\prime \prime}+b_{2}^{\star}\right)=-1}\end{array}\right.$</p><p>$\left\{\begin{array}{l}{b_{1}^{\star}=-\frac{1}{2} w^{T}\left(\boldsymbol{x}_{1}^{\prime}+\boldsymbol{x}_{1}^{\prime \prime}\right)} \\ {b_{2}^{\star}=-\frac{1}{2} \boldsymbol{w}^{T}\left(\boldsymbol{x}_{2}^{\prime}+\boldsymbol{x}_{2}^{\prime \prime}\right)}\end{array} \Rightarrow b_{1}^{\star}-b_{2}^{\star}=-\frac{1}{2}\left[\boldsymbol{w}^{T}\left(\boldsymbol{x}_{1}^{\prime}-\boldsymbol{x}_{2}^{\prime}\right)+\boldsymbol{w}^{T}\left(\boldsymbol{x}_{1}^{\prime \prime}-\boldsymbol{x}_{2}^{\prime \prime}\right)\right]\right.$</p><p>$\left\{\begin{array}{1}{\left(w^{T} x_{2}^{\prime}+b_{1}^{\star}\right)\geq 1=w^{T} x_{1}^{\prime}+b_{1}^{\star}} \\ {\left(w^{T} x_{1}^{\prime}+b_{2}^{\star}\right)\geq 1=w^{T} x_{2}^{\prime}+b_{2}^{\star}} \\ {\left(w^{T} x_{2}^{\prime \prime}+b_{1}^{\star}\right)\geq 1=w^{T} x_{1}^{\prime \prime}+b_{1}^{\star}} \\ {\left(w^{T} x_{1}^{\prime \prime}+b_{2}^{\star}\right)\geq 1=w^{T} x_{2}^{\prime \prime}+b_{2}^{\star}}\end{array}\right.\Rightarrow\left\{\begin{array}{l}{\boldsymbol{w}^{T}\left(\boldsymbol{x}_{1}^{\prime}-\boldsymbol{x}_{2}^{\prime}\right)=0} \\ {\boldsymbol{w}^{T}\left(\boldsymbol{x}_{1}^{\prime \prime}-\boldsymbol{x}_{2}^{\prime \prime}\right)=0}\end{array} \Rightarrow b_{1}^{\star}=b_{2}^{\star}\right.$</p><h5 id="SVM问题的对偶形式推导"><a href="#SVM问题的对偶形式推导" class="headerlink" title="SVM问题的对偶形式推导"></a>SVM问题的对偶形式推导</h5><h6 id="1-拉格朗日函数的介绍"><a href="#1-拉格朗日函数的介绍" class="headerlink" title="1. 拉格朗日函数的介绍"></a>1. 拉格朗日函数的介绍</h6><p>优化问题的一般形式</p><script type="math/tex; mode=display">\begin{array}{ll}{\min _{x}} & {f_{0}(x)} \\ {\text { s.t. }} & {f_{i}(x) \leq 0, \quad i=1, \ldots, m} \\ {} & {h_{i}(x)=0, \quad i=1, \ldots, p}\end{array}</script><script type="math/tex; mode=display">\begin{array}{cl}{\min _{x}} & {f_{0}(x)} \\ {\text { s.t. }} & {f_{i}(x) \geq 0, \quad i=1, \ldots, m} \\ {} & {h_{i}(x)=0, \quad i=1, \ldots, p}\end{array}</script><script type="math/tex; mode=display">\begin{array}{ll}{\max _{x}} & {f_{0}(x)} \\ {\text { s.t. }} & {f_{i}(x) \leq 0, \quad i=1, \ldots, m} \\ {} & {h_{i}(x)=0, \quad i=1, \ldots, p}\end{array}</script><p>概念：拉格朗日函数是拉格朗日乘子法的核心部分，所以当我们看到拉格朗日函数时，不假思素地将其对应到拉格朗日乘子法上<br>核心：拉格朗日乘子法，其思想就是把有约束优化问题转变为等价的无约束优化问题（形式上）</p><p>拉格朗日函数与原始问题的关系：</p><script type="math/tex; mode=display">\begin{array}{ll}{\min _{x}} & {f_{0}(x)} \\ {\text { s.t. }} & {f_{i}(x) \leq 0, \quad i=1, \ldots, m} \\ {} & {h_{i}(x)=0, \quad i=1, \ldots, p}\end{array}</script><script type="math/tex; mode=display">\begin{array}{c}{\mathcal{L}(x, \lambda, \nu)=f_{0}(x)+\sum_{i=1}^{m} \lambda_{i} f_{i}(x)+\sum_{i=1}^{p} \nu_{i} h_{i}(x)} \\ {\text { s.t. } \quad \lambda_{i} \geq 0, \quad i=1, \ldots, m}\end{array}</script><script type="math/tex; mode=display">\begin{array}{rl}{\min _{x} \max _{\lambda, v}} & {\mathcal{L}(x, \lambda, \nu)} \\ {\text { s.t. }} & {\lambda_{i} \geq 0, \quad i=1, \ldots, m}\end{array}</script><p>证明：<br>$\begin{array}{ll}{\min _{x}} &amp; {f_{0}(x)} \\ {\text { s.t. }} &amp; {f_{i}(x) \leq 0, \quad i=1, \ldots, m} \\ {} &amp; {h_{i}(x)=0, \quad i=1, \ldots, p}\end{array}\Leftrightarrow\begin{array}{rl}{\min _{x} \max _{\lambda, v}} &amp; {\mathcal{L}(x, \lambda, \nu)} \\ {\text { s.t. }} &amp; {\lambda_{i} \geq 0, \quad i=1, \ldots, m}\end{array}$<br>记：$\theta_{p}(x)=\max _{\lambda, v} \mathcal{L}(x, \lambda, \nu)$<br>s.t. $\quad \lambda_{i} \geq 0, \quad i=1, \ldots, m$<br>则：$\theta_{P}(x)=\left\{\begin{array}{ll}{f_{0}(x)} &amp; {\text { for } x \text { that satisfied the origin constraint }} \\ {+\infty} &amp; {\text { otherwise }}\end{array}\right.$<br>验证上述性质：<br>①若存在$\boldsymbol{x}$使得某个$f_{i}(x)&gt;0$则我们可令$0 \leq \lambda_{i} \rightarrow+\infty$，进而有$\theta_{p}(x)=+\infty$<br>②若存在$\boldsymbol{x}$使得某个$h_{i}(x) \neq 0$则我们可令$v_{i} h_{i}(x) \rightarrow+\infty$，进而有$\theta_{p}(x)=+\infty$<br>③若$x \in\left\{x | \forall i, v_{i}, \lambda_{i} \geq 0, \lambda_{i} f_{i}(x) \leq 0, v_{i} h_{i}(x)=0\right\}$，则有$\max _{\lambda, v} \mathcal{L}(x, \lambda, \nu)=f_{0}(x)$</p><p>拉格朗日乘子法在SVM问题上的应用<br>原始形式：<br>$\begin{array}{ll}{\min _{\boldsymbol{w}, b}} &amp; {\frac{1}{2}|\boldsymbol{w}|^{2}} \\ {\text { s.t. }} &amp; {\boldsymbol{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq 1, \quad i=1,2, \ldots, m}\end{array}\leftrightarrow\begin{array}{ll}{\min _{\boldsymbol{w}, b}} &amp; {\frac{1}{2}|\boldsymbol{w}|^{2}} \\ {\text { s.t. }} &amp; {1-\boldsymbol{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \leq 0, \quad i=1,2, \ldots, m}\end{array}$</p><p>拉格朗日函数：<br>$\mathcal{L}(\boldsymbol{w}, b, \alpha)=\frac{1}{2}|\boldsymbol{w}|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-\boldsymbol{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right)$<br>s.t. $\quad \alpha_{i} \geq 0 \quad$ for $i=1, \ldots, m$</p><p>等价问题：<br>$\begin{array}{cl}{\min _{\boldsymbol{w}, b} \max _{\alpha}} &amp; {\mathcal{L}(\boldsymbol{w}, b, \alpha)} \\ {\text { s.t. }} &amp; {\alpha_{i} \geq 0 \text { for } i=1, \ldots, m}\end{array}$</p><h6 id="2-“对偶”概念的介绍"><a href="#2-“对偶”概念的介绍" class="headerlink" title="2. “对偶”概念的介绍"></a>2. “对偶”概念的介绍</h6><p>进一步求解SVM的拉格朗日对偶问题<br>$\begin{array}{cl}{\max _{\alpha} \min _{\alpha}} &amp; {\frac{1}{2}|\boldsymbol{w}|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-\boldsymbol{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right)} \\ {\text { s.t. }} &amp; {\alpha_{i} \geq 0 \quad \text { for } i=1, \ldots, m}\end{array}$</p><p>①必要条件<br>$\frac{\partial \mathcal{L}}{\partial w}=0 \Rightarrow w=\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}$<br>$\frac{\partial \mathcal{L}}{\partial b}=0 \Rightarrow \sum_{i=1}^{m} \alpha_{i} y_{i}=0$<br>②回代得式<br>$\max _{a} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}$<br>s.t. $\sum_{i=1}^{m} \alpha_{i} y_{i}=0$</p><p>$\begin{aligned} \frac{\partial \mathcal{L}}{\partial \boldsymbol {w}}=0 &amp;=\frac{\partial}{\partial \boldsymbol{w}}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}-\sum_{i=1}^{m} \alpha_{i} y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right) \\ &amp;=\frac{\partial}{\partial \boldsymbol{w}}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}-\sum_{i=1}^{m} \alpha_{i} {y}_{i} \boldsymbol{w}^{T} \boldsymbol{x}_{i}-\sum_{i=1}^{m} \alpha_{i} {y}_{i} \boldsymbol{x}_{i} b\right) \\&amp;=\frac{\partial}{\partial \boldsymbol{w}}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}-\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{w}^{T} x_{i}\right)\\&amp;=\frac{\partial}{\partial \boldsymbol{w}}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}-\boldsymbol{w}^{T} \sum_{i=1}^{m} \alpha_{i} {y}_{i} \boldsymbol{x}_{i}\right)\\&amp;=\boldsymbol{w}-\sum_{i=1}^{m} a_{i} y_{i} \boldsymbol{x}_{i}\end{aligned}$</p><p>$\begin{aligned} \frac{\partial \mathcal{L}}{\partial b}=0&amp;=\frac{\partial}{\partial b}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}-\sum_{i=1}^{m} \alpha_{i} y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right)\\&amp;=\frac{\partial}{\partial b}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}-\sum_{k=1}^{m} \alpha_{i} y_{i} \boldsymbol{w}^{T} \boldsymbol{x}_{i}-\sum_{i=1}^{m} \alpha_{i} y_{i} b\right)\\&amp;=\frac{\partial}{\partial b}\left(-\sum_{i=1}^{m} \alpha_{i} y_{i} b\right)\\&amp;=\frac{\partial}{\partial b}\left(-b \sum_{i=1}^{m} \alpha_{i}{y}_{i}\right)\\&amp;=-\sum_{i=1}^{m} \alpha_{i} y_{i}\end{aligned}$</p><p>$\begin{aligned} \boldsymbol{w} &amp;=\sum_{i=1}^{m} \alpha_{i} \boldsymbol{y}_{i} \boldsymbol{x}_{i} \\ 0 &amp;=\sum_{i=1}^{m} \alpha_{i} \boldsymbol{y}_{i} \end{aligned}$</p><p>$\begin{aligned} \min _{w, b} \mathcal{L}(\boldsymbol{w}, b, \alpha) &amp;=\frac{1}{2}|\boldsymbol{w}|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right) \\ &amp;=\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}-\boldsymbol{w}^{T}\left(\sum_{i=1}^{m} \alpha_{i} {y}_{i} \boldsymbol{x}_{i}\right)-b\left(\sum_{i=1}^{m} \alpha_{i} {y}_{i}\right)+\left(\sum_{i=1}^{m} \alpha_{i}\right)\\&amp;=\frac{1}{2} \boldsymbol{w}^{T}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\right)-\boldsymbol{w}^{T}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\right)-b \cdot 0+\left(\sum_{i=1}^{m} \alpha_{i}\right) \\&amp;=\left(\sum_{i=1}^{m} \alpha_{i}\right)+\left(\frac{1}{2} \boldsymbol{w}^{T}-\boldsymbol{w}^{T}\right)\left(\sum_{i=1}^{m} \alpha_{i} {y}_{i} \boldsymbol{x}_{i}\right)\\&amp;=\left(\sum_{i=1}^{m} \alpha_{i}\right)-\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\right)^{T}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\right)\\&amp;=\left(\sum_{i=1}^{m} \alpha_{i}\right)-\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{T}\right)\left(\sum_{j=1}^{m} \alpha_{j} y_{j} \boldsymbol{x}_{j}\right)\\&amp;=\left(\sum_{i=1}^{m} \alpha_{i}\right)-\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{T}\left(\sum_{j=1}^{m} \alpha_{j} y_{j} \boldsymbol{x}_{j}\right)\right)\\&amp;=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{T} \boldsymbol{x}_{j}\\&amp;=\mathcal{L}(\alpha)\end{aligned}$</p><h6 id="3-KKT条件的介绍"><a href="#3-KKT条件的介绍" class="headerlink" title="3. KKT条件的介绍"></a>3. KKT条件的介绍</h6><p>用途：KKT条件就是一组方程，用于部分最优化问题的求解。<br>优化问题：$\left\{\begin{aligned} \text { optimize } &amp; f(x) \\ \text { s.t. } &amp; g_{i}(x) \leq 0 \\ &amp; h_{j}(x)=0 \end{aligned}\right.$ $\Rightarrow$ KKT条件 $\left\{\begin{aligned} \nabla_{x} \mathcal{L} &amp;=0 &amp;\text{(stationary equation)}\\ g_{j}(\boldsymbol{x}) &amp;=0, \quad j=1, \ldots, m &amp;\text{(primal feasibility)}\\ h_{k}(x) &amp; \leq 0 &amp;\text{(primal feasibility)}\\ \lambda_{k} &amp; \geq 0 &amp;\text{(dual feasibility)}\\ \lambda_{k} h_{k}(\boldsymbol{x}) &amp;=0, \quad k=1, \ldots, p &amp;\text{(complementary slackness)}\end{aligned}\right.$</p><p>使用方法：<br>①假设待优化的的函数为$f : \mathbb{R}^{n} \rightarrow \mathbb{R}$，约束为$y_{i} : \mathbb{R}^{n} \rightarrow \mathbb{R}, h_{i} : \mathbb{R}^{n} \rightarrow \mathbb{R}$，并且优化问题满足regularity condition，那么，若$\mathcal{x}^{\star}$是局部最优值，其必然满足KKT条件必要性）<br>②假设待优化的的函数$f(x)$和不等式约束$g_{i}(x)$是凸函数，等式约束$h_{i}(x)$是仿射函数，且不等式约束等号能够成立（严格可行）。那么满足KKT条件的x点就是全局最优（充要条件）。</p><p>理解KKT条件：<br>考虑不等式约束：<br>①解在边界上<br>②解在非边界上<br>考虑等式+不等式约束：<br>①等式约束<br>②不等式约束（解在边界上）<br>③不等式约束（解非边界上）</p><h5 id="SVM问题的求解算法（SMO）"><a href="#SVM问题的求解算法（SMO）" class="headerlink" title="SVM问题的求解算法（SMO）"></a>SVM问题的求解算法（SMO）</h5><h6 id="1-SMO算法的思路讲解"><a href="#1-SMO算法的思路讲解" class="headerlink" title="1. SMO算法的思路讲解"></a>1. SMO算法的思路讲解</h6><p>重点：<br>①每次迭代过程仅优化两个参数，有闭式解<br>②启发式寻找每次优化的两个参数，有效减少迭代次数<br>思路：<br>①设置$\alpha$列表，并设其初值为0（每个数据点对应一个$\alpha_{i}$）<br>②选取两个待优化变量（为了方便，记为$\alpha_{1}, \alpha_{2} $）<br>③解释地求解两个变量的最优解$\alpha_{1}^{\star}, \alpha_{2}^{\star}$，并更新至$\alpha$的列表<br>④检查更新后的$\alpha$列表是否在某个精度范围内满足KKT条件，若不满足，返回②</p><h6 id="2-SMO算法的简单实现"><a href="#2-SMO算法的简单实现" class="headerlink" title="2. SMO算法的简单实现"></a>2. SMO算法的简单实现</h6><h5 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h5><h5 id="软间隔与支持向量回归公式推导"><a href="#软间隔与支持向量回归公式推导" class="headerlink" title="软间隔与支持向量回归公式推导"></a>软间隔与支持向量回归公式推导</h5><h6 id="Soft-Margin-SVM公式推导"><a href="#Soft-Margin-SVM公式推导" class="headerlink" title="Soft-Margin SVM公式推导"></a>Soft-Margin SVM公式推导</h6><ol><li>Soft-Margin SVM的原始形式导出<br>Hard-Margin<script type="math/tex; mode=display">\begin{array}{ll}{\min _{w, b}} & {\frac{1}{2}\|w\|^{2}} \\ {\text { s.t. }} & {y_{i}\left(w^{T} x_{i}+b\right) \geq 1, \quad i=1,2, \ldots, m}\end{array}</script>Naive-Soft-Margin<script type="math/tex; mode=display">\begin{array}{l}{\min _{w, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{0 / 1}\left(y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)-1\right)} \\ {\ell_{0 / 1}(z)=\left\{\begin{array}{l}{1, \text { if } z<0} \\ {0, \text { otherwise }}\end{array}\right.}\end{array}</script>Soft-Margin<script type="math/tex; mode=display">\begin{array}{cl}{\min _{w, b, \xi_{i}}} & {\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i}} \\ {\text { s.t. }} & {y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1-\xi_{i}} \\ {} & {\xi_{i} \geqslant 0, i=1,2, \ldots, m}\end{array}</script>Soft-Margin(equivalent)<script type="math/tex; mode=display">\min _{w, b} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m} \underbrace{\max \left(0,1-y_{i}\left(w^{\mathrm{T}} x_{i}+b\right)\right)}_{\varepsilon_{i}}</script>proof<script type="math/tex; mode=display">\xi_{i}=\max \left(0,1-y_{i}\left(w^{\mathrm{T}} x_{i}+b\right)\right)\left\{\begin{array}{l}{ \geq 0} \\ { \geq 1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_ {i}+b\right)}\end{array}\right.</script></li><li>Soft-Margin SVM的对偶形式导出<br>求偏导数<script type="math/tex; mode=display">\begin{aligned} \frac{\partial \mathcal{L}}{\partial w} &=0 \Rightarrow \boldsymbol{w}=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \\ \frac{\partial \mathcal{L}}{\partial \boldsymbol{b}} &=0 \Rightarrow 0=\sum_{i=1}^{m} \alpha_{i} y_{i} \\ \frac{\partial \mathcal{L}}{\partial \boldsymbol{\xi}_{i}} &=0 \Rightarrow C=\alpha_{i}+\mu_{i} \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} \frac{\partial \mathcal{L}}{\partial w}=0 &=\frac{\partial}{\partial w}\left(\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i}+\sum_{i=1}^{m} \alpha_{i}\left(1-\xi_{i}-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}\right) \\ &=\frac{\partial}{\partial \boldsymbol{w}}\left(\frac{1}{2} w^{T} w-\sum_{i=1}^{m} \alpha_{i} \boldsymbol{y}_{i} \boldsymbol{w}^{T} \boldsymbol{x}_{i}\right) \\ &= \frac{\partial}{\partial w}\left(\frac{1}{2} w^{T} w-\sum_{i=1}^{m} \alpha_{i} y_{i} w^{T} x_{i}\right) \\ &= \frac{\partial}{\partial w}\left(\frac{1}{2} w^{T} w-w^{T} \sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}\right)\\ &= w-\sum_{i=1}^{m} n_{i} y_{i} x_{i}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} \frac{\partial \mathcal{L}}{\partial b}=0 &=\frac{\partial}{\partial b}\left(\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i}+\sum_{i=1}^{m} \alpha_{i}\left(1-\xi_{i}-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}\right) \\ &=\frac{\partial}{\partial b}\left(\frac{1}{2} w^{T} w-\sum_{i=1}^{m} \alpha_{i} \boldsymbol{y}_{i} \boldsymbol{w}^{T} \boldsymbol{x}_{i}\right) \\ &= \frac{\partial}{\partial b}\left(-b \sum_{i=1}^{m} \alpha_{i} y_{i}\right)\\ &= -\sum_{i=1}^{m} \alpha_{i} y_{i}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} \frac{\partial \mathcal{L}}{\partial \xi_{i}}=0 &=\frac{\partial}{\partial \xi_{i}}\left(\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i}+\sum_{i=1}^{m} \alpha_{i}\left(1-\xi_{i}-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}\right) \\ &=\frac{\partial}{\partial \xi_{i}}\left(C \sum_{i=1}^{m} \xi_{i}+\sum_{i=1}^{m} \alpha_{i}\left(-\xi_{i}\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}\right) \\ &=\frac{\partial}{\partial \xi_{i}}\left(C \xi_{i}-\alpha_{i} \xi_{i}-\mu_{i} \xi_{i}\right) \\ &=C-\alpha_{i}-\mu_{i} \end{aligned}</script>代入获得对偶形式：<script type="math/tex; mode=display">\begin{array}{l}{\max _{\alpha} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}} \\ {\text { s.t. } \sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\quad 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \ldots, m}\end{array}</script><script type="math/tex; mode=display">\begin{aligned} \min _{w, b, \xi} \mathcal{L}(\boldsymbol{w}, b, \xi, \alpha) &=\left(\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i}+\sum_{i=1}^{m} \alpha_{i}\left(1-\xi_{i}-y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}\right) \\ &=\frac{1}{2} w^{T} w-w^{T}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}\right)+\left(\sum_{i=1}^{m} \alpha_{i}\right)-b\left(\sum_{i=1}^{m} \alpha_{i} y_{i}\right)+C \sum_{i=1}^{m} \xi_{i}-\sum_{i=1}^{m} \alpha_{i} \xi_{i}-\sum_{i=1}^{m} \mu_{i} \xi_{i}\\ &= \frac{1}{2} w^{T}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}\right)-w^{T}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}\right)+\left(\sum_{i=1}^{m} \alpha_{i}\right)-b \cdot 0+\left(C-\alpha_{i}-\mu_{i}\right) \sum_{i=1}^{m} \xi_{i} \\ &= \left(\sum_{i=1}^{m} \alpha_{i}\right)+\left(\frac{1}{2} w^{T}-w^{T}\right)\left(\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}\right) \\ &= \left(\sum_{i=1}^{m} \alpha_{i}\right)-\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}\right)^{T}\left(\sum_{j=1}^{m} \alpha_{i} y_{j} x_{j}\right) \\ &= \left(\sum_{i=1}^{m} \alpha_{i}\right)-\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}^{T}\right)\left(\sum_{j=1}^{m} \alpha_{j} y_{j} x_{j}\right)\\ &= \left(\sum_{i=1}^{m} \alpha_{i}\right)-\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}^{T}\left(\sum_{j=1}^{m} \alpha_{j} y_{j} x_{j}\right)\right) \\ &= \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}\\&= \mathcal{L}(\alpha)\end{aligned}</script>约束：<script type="math/tex; mode=display">\left\{\begin{array}{l}{\alpha_{i} \geq 0} \\ {\mu_{i} \geq 0} \\ {\sum_{i=1}^{m} \alpha_{i} y_{i}=0}\end{array} \quad \Rightarrow \quad\left\{\begin{array}{l}{\alpha_{i} \geq 0} \\ {C-\alpha_{i} \geq 0} \\ {\sum_{i=1}^{m} \alpha_{i} y_{i}=0}\end{array} \quad \Rightarrow\left\{\begin{array}{l}{0 \leq \alpha_{i} \leq C} \\ {\sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\sum_{i=1}^{m} \alpha_{i} y_{i}=0}\end{array} \quad \Rightarrow\left\{\sum_{i=1}^{m} \alpha_{i} y_{i}=0\right.\right.\right.\right.</script>稀疏性分析：<br>KKT条件<script type="math/tex; mode=display">\left\{\begin{array}{l}{\alpha_{i} \geqslant 0, \quad \mu_{i} \geqslant 0} \\ {y_{i} f\left(x_{i}\right)-1+\xi_{i} \geqslant 0} \\ {\alpha_{i}\left(y_{i} f\left(x_{i}\right)-1+\xi_{i}\right)=0} \\ {\xi_{i} \geqslant 0, \mu_{i} \xi_{i}=0}\end{array}\right.</script></li></ol><h6 id="Support-Vector-Regression（SVR）公式推导"><a href="#Support-Vector-Regression（SVR）公式推导" class="headerlink" title="Support Vector Regression（SVR）公式推导"></a>Support Vector Regression（SVR）公式推导</h6><p>1.SVR原始形式的导出<br>SVR（一般形式）：</p><script type="math/tex; mode=display">\begin{array}{l}{\min _{w, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{c}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}\right)} \\ {\ell_{\epsilon}(z)=\left\{\begin{array}{ll}{0,} & {\text { if }|z| \leqslant \epsilon} \\ {|z|-\epsilon, \text { otherwise }}\end{array}\right.}\end{array}</script><p>SVR（原始形式）：</p><script type="math/tex; mode=display">\begin{array}{cl}{\min _{w, b, \xi_{i}, \hat{\xi}_{i}}} & {\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\hat{\xi}_{i}\right)} \\ {\text { s.t. }} & {f\left(x_{i}\right)-y_{i} \leqslant \epsilon+\xi_{i}} \\ {} & {y_{i}-f\left(x_{i}\right) \leqslant \epsilon+\hat{\xi}_{i}} \\ {} & {\xi_{i} \geq 0, \hat{\xi}_ {i} \geq 0, i=1,2, \ldots, m}\end{array}</script><p>2.SVR对偶形式的导出<br>求偏导数:</p><script type="math/tex; mode=display">\begin{array}{l}{\frac{\partial \mathcal{L}}{\partial w}=0 \Rightarrow w=\sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right) x_{i}} \\ {\frac{\partial \mathcal{L}}{\partial b}=0 \Rightarrow 0=\sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right)} \\ {\frac{\partial \mathcal{L}}{\partial \xi_{i}}=0 \Rightarrow C=\alpha_{i}+\mu_{i}} \\ {\frac{\partial \mathcal{L}}{\partial \hat{\xi}_{i}}=0 \Rightarrow C=\dot{\alpha}_{i}+\hat{\mu}_{i}}\end{array}</script><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \mathcal{L}}{\partial w}=0&=\frac{\partial}{\partial w}\left(\frac{1}{2}\|u\|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\overline{\xi}_{i}\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}-\sum_{i=1}^{m} \mu_{i} \xi_{i}-\sum_{i=1}^{m} \mu_{i} \dot{\xi}_{i}+\sum_{i=1}^{m} \alpha_{i}\left(f\left(x_{i}\right)-y_{i}-\epsilon-\xi_{i}\right)+\sum_{i=1}^{m} \hat{\alpha}_{i}\left(y_{i}-f\left(x_{i}\right)-\epsilon-\overline{\xi}_{i}\right) \right)\\&= \frac{\partial}{\partial w}\left(\frac{1}{2} | w \|^{2}+\sum_{i=1}^{m} \alpha_{i} f\left(x_{i}\right)-\sum_{i=1}^{m} \delta_{i} f\left(x_{i}\right)\right)\\ &=\frac{\partial}{\partial w}\left(\frac{1}{2}\|w\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)-\sum_{i=1}^{m} \hat{a}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right)\\&=\boldsymbol{w}+\sum_{i=1}^{m}\left(\alpha_{i}-\hat{\alpha}_{i}\right) \boldsymbol{x}_{i} \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \mathcal{L}}{\partial b}=0&=\frac{\partial}{\partial b}\left(\frac{1}{2} \|\left.w\right|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\hat{\xi}_{i}\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}-\sum_{i=1}^{m} \hat{\mu}_{i} \hat{\xi}_{i}+\sum_{i=1}^{m} \alpha_{i}\left(f\left(x_{i}\right)-y_{i}-\epsilon-\xi_{i}\right)+\sum_{i=1}^{m} a_{i}\left(y_{i}-f\left(x_{i}\right)-c-\tilde{\xi}_{i}\right)\right)\\&=\frac{\partial}{\partial b}\left(\sum_{i=1}^{m} a_{i} f\left(x_{i}\right)-\sum_{i=1}^{m} a_{i} f\left(x_{i}\right)\right)\\&=\frac{\partial}{\partial b}\left(\sum_{i=1}^{m} \alpha_{i}\left(w^{T} x_{i}+b\right)-\sum_{i=1}^{m} \delta_{i}\left(w^{T} x_{i}+b\right)\right)\\&=\sum_{i=1}^{m}\left(a_{i}-\hat{a}_{i}\right)\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \mathcal{L}}{\partial \xi_{i}}=0&=\frac{\partial}{\partial \xi_{i}}\left(\frac{1}{2} \|\left.w\right|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\hat{\xi}_{i}\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}-\sum_{i=1}^{m} \hat{\mu}_{i} \hat{\xi}_{i}+\sum_{i=1}^{m} \alpha_{i}\left(f\left(x_{i}\right)-y_{i}-\epsilon-\xi_{i}\right)+\sum_{i=1}^{m} a_{i}\left(y_{i}-f\left(x_{i}\right)-c-\tilde{\xi}_{i}\right)\right)\\&=\frac{\partial}{\partial \xi_{i}}\left(C \sum_{i=1}^{m}\left(\xi_{i}\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}+\sum_{i=1}^{m} a_{i}\left(-\xi_{i}\right)\right)\\&=\frac{\partial}{\partial \xi_{i}}\left(C \xi_{i}-\mu_{i} \xi_{i}-a_{i} \xi_{i}\right)\\& =C-\mu_{i}-\alpha_{i}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \mathcal{L}}{\partial \hat{\xi}_{i}}=0&=\frac{\partial}{\partial \hat{\xi}_{i}}\left(\frac{1}{2} \|\left.w\right|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\hat{\xi}_{i}\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}-\sum_{i=1}^{m} \hat{\mu}_{i} \hat{\xi}_{i}+\sum_{i=1}^{m} \alpha_{i}\left(f\left(x_{i}\right)-y_{i}-\epsilon-\xi_{i}\right)+\sum_{i=1}^{m} a_{i}\left(y_{i}-f\left(x_{i}\right)-c-\tilde{\xi}_{i}\right)\right)\\&=\frac{\partial}{\partial \hat{\xi}_{i}}\left(C \sum_{i=1}^{m}\left(\hat{\xi}_{i}\right)-\sum_{i=1}^{m} \hat{\mu_{i}} \hat{\xi}_{i}+\sum_{i=1}^{m} \hat{a_{i}}\left(-\xi_{i}\right)\right)\\&=\frac{\partial}{\partial \xi_{i}}\left(C \xi_{i}-\hat{\mu_{i}} \xi_{i}-\hat{\alpha}_{i} \xi_{i}\right)\\& =C-\hat{\mu_{i}}-\hat{\alpha_{i}}\end{aligned}</script><p>代入获得对偶形式：</p><script type="math/tex; mode=display">\begin{aligned} \max _{\boldsymbol{\alpha}, \hat{\alpha}} & \sum_{i=1}^{m} y_{i}\left(\hat{\alpha}_{i}-\alpha_{i}\right)-\epsilon\left(\hat{\alpha}_{i}+\alpha_{i}\right) \\ &-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right)\left(\hat{\alpha}_{j}-\alpha_{j}\right) \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j} \\ \text { s.t. } & \sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right)=0 \\ & 0 \leqslant \alpha_{i}, \hat{\alpha}_{i} \leqslant C \end{aligned}</script><p>为什么SVM要引入核函数？<br>现实任务中训练样本可能是线性不可分的，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。由于特征空间维数可能很高，甚至可能是无穷维，直接计算通常比较困难，因此引入核函数用于简化计算。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;第6章-支持向量模型&quot;&gt;&lt;a href=&quot;#第6章-支持向量模型&quot; class=&quot;headerlink&quot; title=&quot;第6章 支持向量模型&quot;&gt;&lt;/a&gt;第6章 支持向量模型&lt;/h3&gt;&lt;h4 id=&quot;支持向量机公式推导&quot;&gt;&lt;a href=&quot;#支持向量机公式推导&quot; c
      
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://minhzou.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》第4章 决策树模型 公式推导</title>
    <link href="http://minhzou.top/2019/07/03/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC4%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%20%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://minhzou.top/2019/07/03/《机器学习》第4章 决策树模型 公式推导/</id>
    <published>2019-07-03T11:41:10.895Z</published>
    <updated>2019-07-16T08:29:12.559Z</updated>
    
    <content type="html"><![CDATA[<h3 id="第4章-决策树模型"><a href="#第4章-决策树模型" class="headerlink" title="第4章 决策树模型"></a>第4章 决策树模型</h3><h4 id="ID3算法公式推导"><a href="#ID3算法公式推导" class="headerlink" title="ID3算法公式推导"></a>ID3算法公式推导</h4><h5 id="1-信息熵与信息增益"><a href="#1-信息熵与信息增益" class="headerlink" title="1. 信息熵与信息增益"></a>1. 信息熵与信息增益</h5><p>信息熵：熵是度量样本集合纯度最常用的一种指标，代表一个系统中蕴含多少信息量，信息量越大表明一个系统不确定性就越大，就存在越多的可能性，即信息熵越大。<br>假定当前样本集合D中第类样本所占的比$p_{k}(k=1,2, \ldots,|y|)$，则D的信息熵为：</p><script type="math/tex; mode=display">\operatorname{Ent}(D)=-\sum_{k=1}^{ | \mathcal{Y |}} p_{k} \log _{2} p_{k}</script><p>信息熵满足下列不等式：</p><script type="math/tex; mode=display">0 \leq \operatorname{Ent}(D) \leq \log _ {2}|\mathcal{Y}|</script><p>y表示样本D中的类别数</p><p>已知集合D的信息熵的定义为：</p><script type="math/tex; mode=display">\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|} p_{k} \log _{2} p_{k}</script><p>其中，$|\mathcal{Y}|$表示样本类别总数，$p_{k}$表示第k类样本所占的比例，且$0 \leq p_{k} \leq 1, \sum_{k=1}^{n} p_{k}=1$</p><p>若令$|\mathcal{Y}|=n, p_{k}=x_{k}$，那么信息熵Ent（D）就可以看作一个n元实值函数，即</p><script type="math/tex; mode=display">\operatorname{Ent}(D)=f\left(x_{1}, \ldots, x_{n}\right)=-\sum_{k=1}^{n} x_{k} \log _{2} x_{k}</script><p>其中：$0 \leq x_{k} \leq 1, \sum_{k=1}^{n} x_{k}=1$<br>引入拉格朗日乘子$\lambda$：<br>$L\left(x_{1}, \ldots, x_{n}, \lambda\right)=-\sum_{k=1}^{n} x_{k} \log _{2} x_{k}+\lambda\left(\sum_{k=1}^{n} x_{k}-1\right)$</p><p>对L分别关于$x, \lambda$求一阶偏导，并令偏导等于0：</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial L\left(x_{1}, \ldots, x_{n}, \lambda\right)}{\partial x_{1}} &=\frac{\partial}{\partial x_{1}}\left[-\sum_{k=1}^{n} x_{k} \log _{2} x_{k}+\lambda\left(\sum_{k=1}^{n} x_{k}-1\right)\right]=0 \\ &=-\log _{2} x_{1}-x_{1} \cdot \frac{1}{x_{1} \ln 2}+\lambda=0 \\ &=-\log _{2} x_{1}-\frac{1}{\ln 2}+\lambda=0 \\ & \Rightarrow \lambda=\log _{2} x_{1}+\frac{1}{\ln 2} \end{aligned}</script><p>同理推得：</p><script type="math/tex; mode=display">\lambda=\log _{2} x_{1}+\frac{1}{\ln 2}=\log _{2} x_{2}+\frac{1}{\ln 2}=\ldots=\log _{2} x_{n}+\frac{1}{\ln 2}</script><p>对于任意的x，满足约束条件：</p><script type="math/tex; mode=display">\sum_{k=1}^{n} x_{k}=1</script><p>因此：</p><script type="math/tex; mode=display">x_{1}=x_{2}=\ldots=x_{n}=\frac{1}{n}</script><p>最大值点还是最小值点需要做个简单的验证：<br>$x_{1}=x_{2}=\ldots=x_{n}=\frac{1}{n}$时：</p><script type="math/tex; mode=display">f\left(\frac{1}{n}, \ldots, \frac{1}{n}\right)=-\sum_{k=1}^{n} \frac{1}{n} \log _{2} \frac{1}{n}=-n \cdot \frac{1}{n} \log _{2} \frac{1}{n}=\log _{2} n</script><p>$x_{1}=1, x_{2}=x_{3}=\ldots=x_{n}=0$时：</p><script type="math/tex; mode=display">f(1,0, \ldots, 0)=-1 \cdot \log _{2} 1-0 \cdot \log _{2} 0 \ldots-0 \cdot \log _{2} 0=0</script><p>显然$\log _{2} n \geq 0$，所以$x_{1}=x_{2}=\ldots=x_{n}=\frac{1}{n}$为最大值点，最大值为$\log _{2} n$。</p><p>下面考虑求$f\left(x_{1}, \ldots, x_{2}\right)$的最小值，仅考虑$0 \leq x_{k} \leq 1, f\left(x_{1}, \ldots, x_{n}\right)$可以看作是n个互不相关一元函数的加和，即：</p><script type="math/tex; mode=display">\begin{array}{c}{f\left(x_{1}, \ldots, x_{n}\right)=\sum_{k=1}^{n} g\left(x_{k}\right)} \\ {g\left(x_{k}\right)=-x_{k} \log _{2} x_{k}, 0 \leq x_{k} \leq 1}\end{array}</script><p>求$g\left(x_{1}\right)$的最小值，首先对$g\left(x_{1}\right)$关于$x_{1}$，求一阶和二阶导数：</p><script type="math/tex; mode=display">\begin{array}{l}{g^{\prime}\left(x_{1}\right)=\frac{d\left(-x_{1} \log _{2} x_{1}\right)}{d x_{1}}=-\log _{2} x_{1}-x_{1} \cdot \frac{1}{x_{1} \ln 2}=-\log _{2} x_{1}-\frac{1}{\ln 2}} \\ {g^{\prime \prime}\left(x_{1}\right)=\frac{d\left(-\log _{2} x_{1}-\frac{1}{\ln 2}\right)}{d x_{1}}=-\frac{1}{x_{1} \ln 2}}\end{array}</script><p>在定义域$0 \leq x_{k} \leq 1$上，始终有$g^{\prime \prime}\left(x_{1}\right)=-\frac{1}{x_{1} \ln 2}&lt;0$，即$g\left(x_{1}\right)$为开口向下的凹函数，最小值在边界$x_{1}=0$或$x_{1}=1$处取得：</p><script type="math/tex; mode=display">\begin{array}{l}{g(0)=-0 \log _{2} 0=0} \\ {g(1)=-1 \log _{2} 1=0}\end{array}</script><p>$g\left(x_{1}\right)$的最小值即为0，同理可得$g\left(x_{2}\right), \ldots, g\left(x_{n}\right)$的最小值也为0，那么$f\left(x_{1}, \ldots, x_{n}\right)$的最小值此时也为0<br>如果令某个$x_{k}=1$，那么根据约束条件$\sum_{k=1}^{n} x_{k}=1$可知：</p><script type="math/tex; mode=display">x_{1}=x_{2}=\ldots=x_{k-1}=x_{k+1}=\ldots=x_{n}=0</script><p>将其代入$f\left(x_{1}, \ldots, x_{n}\right)$，得</p><script type="math/tex; mode=display">f(0,0, \ldots, 0,1,0, \ldots, 0)=-0 \log _{2} 0-0 \log _{2} 0 \ldots-0 \log _{2} 0-1 \log _{2} 1-0 \log _{2} 0 \ldots-0 \log _{2} 0=0</script><p>所以$x_{k}=1, x_{1}=x_{2}=\ldots=x_{k-1}=x_{k+1}=\ldots=x_{n}=0$，一定是$f\left(x_{1}, \ldots, x_{n}\right)$在满足约束条件下的最小值点，其最小值为0.<br>所以：</p><script type="math/tex; mode=display">0 \leq \operatorname{Ent}(D) \leq \log _{2} n</script><p>证明：$0 \leq \operatorname{Ent}(D) \leq \log_{2}|y|$<br>其中：$p_{1}+\ldots+p_{|y|}=1$<br>$-p_{|y|} \log _{2} p_{|y|} \geq 0$<br>两个概率变量x,y满足$x+y=p, 0&lt;p \leq 1$，p为一定值，当x越靠近$\frac{p}{2}$时，下面的z越大</p><script type="math/tex; mode=display">z=-\left(x \log _{2} x+(p-x) \log_{2}(p-x)\right)</script><p>对x求导：</p><script type="math/tex; mode=display">\begin{aligned} z^{\prime} &=-\left(\log_{2} x+x \frac{1}{x}-\log_{2}(p-x)-(p-x) \frac{1}{p-x}\right) \\ &=-\log_{2} \frac{x}{p-x} \end{aligned}</script><p>当 $x=\frac{p}{2}$时，$z^{\prime}=0$；<br>当$x&lt;\frac{p}{2}$时，$z^{\prime}&gt;0$；z单调递增<br>当$x&gt;\frac{p}{2}$时，$z^{\prime}&lt;0$；z单调递减</p><p>证明命题： 假设$p=\frac{1}{n}$，定义</p><script type="math/tex; mode=display">x_{1}=\min \left(p_{i}\right)<\max \left(p_{i}\right)=x_{2}</script><p>则必有$x_{1}&lt;p&lt;x_{2}$，可推出</p><script type="math/tex; mode=display">-\left(p \log _{2} p+\left(x_{1}+x_{2}-p\right) \log _{2}\left(x_{1}+x_{2}-p\right)\right)>-\left(x_{1} \log _{2} x_{1}+x_{2} \log _{2} x_{2}\right)</script><p>所以当$p=\frac{1}{|y|}$时，信息熵取得最大值$\log_{2}|y|$</p><p>样本D中只有一类样本，此时信息熵最小，其值为：</p><script type="math/tex; mode=display">\operatorname{Ent}(D)=-1 \log_{2} 1-0 \log_{2} 0-\ldots-0 \log_{2} 0=0</script><p>当只有一类样本时，表征系统就是确定的，因此，系统的信息量最小。</p><p>信息增益：<br>假定离散属性a有V个可能的取值$\left\{a^{1}, a^{2}, \ldots a^{v}\right\}$，如果使用特征a来对数据集D进行划分，则会产生V个分支结点，其中第个结点包含了数据集D中所有在特征a上取值为$a^{V}$的样本总数，记为$D^{\nu}$，再考虑到不同的分支结点所包含的样本数量不同，给分支节点赋予不同的权重，这样对样本数越多的分支节点的影响就会越大，因此，就能够计算出特征对样本集D进行划分所获得的“信息增益”：</p><script type="math/tex; mode=display">\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)</script><h5 id="2-ID3算法流程"><a href="#2-ID3算法流程" class="headerlink" title="2. ID3算法流程"></a>2. ID3算法流程</h5><p>ID3算法流程<br>1）初始化信息增益的阈值$\varepsilon$；<br>2）判断样本是否为同一类输出$D^{v}$，如果是，则返回单节点树T。标记类别为$D^{v}$；<br>3）判断属性是否为空，如果是，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。<br>4）计算所有属性A中的各个属性（一共个）对输出D的信息增益，选择信息增益最大的属性a。<br>5）如果属性a的信息增益小于阈值$\varepsilon$，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。<br>6）否则，按属性a的不同属性值$\left\{a^{1}, a^{2}, \ldots, a^{v}\right\}$.，将对应的样本输出D分成不同的类别$D^{v}$，每个类别产生一个子节点。对应属性值为$a^{v}$。返回增加了节点的树T。<br>7）对于所有的子节点，令$D=D^{v}, A=A-a$递归调用2-6步，得到子树并返回。</p><p>ID3算法虽然提出了新思路，但是还是有很多值得改进的地方。<br>a）ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。<br>b）ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。如果校正这个问题呢？<br>c）ID3算法对于缺失值的情况没有做考虑<br>d）没有考虑过拟合的问题</p><h4 id="C4-5算法公式推导"><a href="#C4-5算法公式推导" class="headerlink" title="C4.5算法公式推导"></a>C4.5算法公式推导</h4><h5 id="1-增益率"><a href="#1-增益率" class="headerlink" title="1. 增益率"></a>1. 增益率</h5><p>增益率：信息增益偏向于选择取值较多的属性，容易过拟合基于信息增益的缺点，C4.5算法不直接使用信息增益，而是使用一种叫增益率的方法来选择最优属性进行划分，对于样本集D中的离散属性a，增益率为：</p><script type="math/tex; mode=display">\operatorname{Gain}_{-} \operatorname{ratio}(D, a)=\frac{\operatorname{Gain}(D, a)}{I V(a)}</script><p>IV值：</p><script type="math/tex; mode=display">I V(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log_{2} \frac{\left|D^{v}\right|}{|D|}</script><p>IV（a）是属性a的固有值。</p><p>增益率可能产生一个问题就是，对可取数值数目较少的属性有所偏好。因此C4.5算法并不是直接选择使用增益率最大的候选划分属性，而是使用了一个启发式算法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益率最高的。</p><h5 id="2-C4-5算法的不足："><a href="#2-C4-5算法的不足：" class="headerlink" title="2. C4.5算法的不足："></a>2. C4.5算法的不足：</h5><p>C4.5虽然改进或者改善了ID3算法的几个主要的问题，仍然有优化的空间。<br>1）由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。<br>2）C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。<br>3）C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。<br>4）C4.5由于使用了熵模型，里面有大量的耗时的对数运算，如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。</p><h4 id="CART算法公式推导"><a href="#CART算法公式推导" class="headerlink" title="CART算法公式推导"></a>CART算法公式推导</h4><h5 id="1-基尼值与基尼指数"><a href="#1-基尼值与基尼指数" class="headerlink" title="1. 基尼值与基尼指数"></a>1. 基尼值与基尼指数</h5><p>基尼值：用于度量数据集的纯度，Gimi（D）反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，因此，Gimi（D）越小，则数据集的纯度越高.<br>假定当前样本集合D中第k类样本所占的比例为$p_{k}(k=1,2, \dots,|y|)$，则D的基尼值为：</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{Gini}(D) &=\sum_{k=1}^{|y|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\ &=p_{1} \cdot\left(p_{2}+p_{3}+\ldots+p_{k}\right)+\ldots+p_{k} \cdot\left(p_{1}+p_{2}+p_{3}+\ldots+p_{k-1}\right) \\ &=\left(p_{1}+p_{2}+\ldots+p_{k}\right)-\left(p_{1}^{2}+p_{2}^{2}+\ldots+p_{k}^{2}\right) \\ &=1-\sum_{k=1}^{|y|} p_{k}^{2} \end{aligned}</script><p>基尼指数：表示在样本集合中一个随机选中的样本被分错的概率。基尼指数越大，样本的不确定性也就越大。</p><script type="math/tex; mode=display">\operatorname{Gini}_{-} \operatorname{index}(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)</script><p>$D^{v}$表示第v个类别的样本集。</p><h5 id="2-CART算法流程"><a href="#2-CART算法流程" class="headerlink" title="2.CART算法流程"></a>2.CART算法流程</h5><p>CART和ID3的思路区别不大。</p><h4 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h4><h5 id="预剪枝（pre-pruning）"><a href="#预剪枝（pre-pruning）" class="headerlink" title="预剪枝（pre-pruning）"></a>预剪枝（pre-pruning）</h5><p>预剪枝（pre-pruning）：预剪枝就是在构造决策树的过程中，先对每个结点在划分前进行估计，如果当前结点的划分不能带来决策树模型泛化性增加。则不对当前结点进行划分并且将当前结点标记为叶结点。</p><h5 id="后剪枝（post-purning）"><a href="#后剪枝（post-purning）" class="headerlink" title="后剪枝（post-purning）"></a>后剪枝（post-purning）</h5><p>后剪枝就是先把整颗决策树构造完毕，然后自底向上的对非叶结点进行考察，若将该结点对应的子树换为叶结点能够带来泛化性能的提升，则把该子树替换为叶结点。</p><p>对比预剪枝和后剪枝，能够发现，后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛华性能往往也要优于预剪枝决策树。但后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，因此其训练时间开销要比未剪枝决策树和预剪枝决策树都大得多。</p><h4 id="连续值与缺失值的处理"><a href="#连续值与缺失值的处理" class="headerlink" title="连续值与缺失值的处理"></a>连续值与缺失值的处理</h4><h5 id="连续值的处理："><a href="#连续值的处理：" class="headerlink" title="连续值的处理："></a>连续值的处理：</h5><p>样本集D中的连续属性a，假设属性a有n个不同的取值，对其进行大小排序，记为$\left\{a^{1}, a^{2}, \ldots, a^{n}\right\}$，根据属性可得到n-1个划分点t，划分点t的集合为：</p><script type="math/tex; mode=display">T_{a}=\left\{\frac{a^{i}+a^{i+1}}{2} | 1 \leqslant i \leqslant n-1\right\}</script><p>对于取值集合$T_{a}$中的每个t值会将属性a离散为一个属性值只有两个值，分别是$\{a&gt;t\}$和$\{a \leq t\}$的属性，计算新属性的信息增益，找到信息增益最大的，值即为该属性的最优划分点。</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{Gain}(D, a) &=\max _{t \in T_{o}} \operatorname{Gain}(D, a, t) \\ &=\max _{t \in I_{a}} \operatorname{Ent}(D)-\sum_{\lambda \in[-,+\}} \frac{\left|D_{t}^{\lambda}\right|}{|D|} \operatorname{Ent}\left(D_{t}^{\lambda}\right) \end{aligned}</script><h5 id="缺失值的处理："><a href="#缺失值的处理：" class="headerlink" title="缺失值的处理："></a>缺失值的处理：</h5><p>在决策树中处理含有缺失值的样木的时候，需要解决两个问题：<br>1、如何在属性值缺失的情况下进行划分属性的选择？<br>2、给定划分属性，若样本在该属性上的值是缺失的，那么该如何对这个样本进行划分？</p><p>对于第一个问题：我们可以根据$\widetilde{D}$（即在该属性上没有缺失的样本集）来计算属性a的信息增益或者其它指标。我们只要再给根据计算出来的值一个权重，就可以表示训练集D中属性a的优劣。</p><p>假定属性a有V个可取值$\left\{a^{1}, a^{2} \ldots, a^{v}\right\}$令$\tilde{D}^{v}$表示D中在属性a上取值为$a^{v}$的样本予集，$\tilde{D}_ {k}$表示$\tilde{D}$中属于第k类$(k=1,2, \dots,|y|)$）样本子集，则显然有$\tilde{D}=\cup_{k=1}^{v_{1}} \tilde{D}_{k}, \tilde{D}=\cup_{v=1}^{V} \tilde{D}^{v}$，假定我们为每个样本 赋予一个权重$w_{x}$<br>（在决策树的初始阶段，根节点中个样本的权重初始化为1），并定义：</p><script type="math/tex; mode=display">\rho=\frac{\sum_{x \in \tilde{D}} w_{x}}{\sum_{x \in D} w_{x}}</script><p>表示无缺失值样本作占的比例</p><script type="math/tex; mode=display">\tilde{p}_{k}=\frac{\sum_{x \in \tilde{D}_{k}} w_{x}}{\sum_{x \in \tilde{D}} w_{x}} \quad(1 \leqslant k \leqslant|y|)</script><p>表示无缺失值样本中第k类所占的比例</p><script type="math/tex; mode=display">\tilde{r}_{v}=\frac{\sum_{x \in \tilde{D}^{v}} w_{x}}{\sum_{x \in \tilde{D}} w_{x}}(1 \leqslant v \leqslant V)</script><p>表示无缺失值样本中在属性a上取值$a^{v}$的样本所占的比例</p><p>信息增益推广为：</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{Gain}(D, a) &=\rho \times \operatorname{Gain}(\tilde{D}, a) \\ &=\rho \times\left(\operatorname{Ent}(\tilde{D})-\sum_{v=1}^{V} \tilde{r}_{v} \operatorname{Ent}\left(\tilde{D}^{v}\right)\right) \end{aligned}</script><script type="math/tex; mode=display">\operatorname{Ent}(\tilde{D})=-\sum_{k=1}^{|y|} \tilde{p}_{k} \log _{2} \tilde{p}_{k}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;第4章-决策树模型&quot;&gt;&lt;a href=&quot;#第4章-决策树模型&quot; class=&quot;headerlink&quot; title=&quot;第4章 决策树模型&quot;&gt;&lt;/a&gt;第4章 决策树模型&lt;/h3&gt;&lt;h4 id=&quot;ID3算法公式推导&quot;&gt;&lt;a href=&quot;#ID3算法公式推导&quot; class
      
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://minhzou.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>距离，范数，向量空间，欧几里得空间（欧式空间），希尔伯特空间，巴拿赫空间</title>
    <link href="http://minhzou.top/2019/06/25/%E8%B7%9D%E7%A6%BB%EF%BC%8C%E8%8C%83%E6%95%B0%EF%BC%8C%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%EF%BC%8C%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%A9%BA%E9%97%B4%EF%BC%88%E6%AC%A7%E5%BC%8F%E7%A9%BA%E9%97%B4%EF%BC%89%EF%BC%8C%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4%EF%BC%8C%E5%B7%B4%E6%8B%BF%E8%B5%AB%E7%A9%BA%E9%97%B4/"/>
    <id>http://minhzou.top/2019/06/25/距离，范数，向量空间，欧几里得空间（欧式空间），希尔伯特空间，巴拿赫空间/</id>
    <published>2019-06-25T01:54:16.471Z</published>
    <updated>2019-06-25T01:53:15.011Z</updated>
    
    <content type="html"><![CDATA[<h4 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h4><p>直线距离、向量距离、折线距离等等都是具体的距离，这里谈的距离是一个抽象的概念，需要满足非负、自反、三角不等式三个条件。<br>设X是任一非空集，对X中任意两点x，y，有一实数d（x，y）与之对应且满足：<br>1.d（x，y）≥ 0，且d（x，y）= 0 当且仅当 x=y；<br>2.d（x， y） = d（y，x）；<br>3.d（x，y）≤ d（x，z）+d（z，y）。<br>称d（x，y）为X中的一个距离。</p><h4 id="向量空间"><a href="#向量空间" class="headerlink" title="向量空间"></a>向量空间</h4><p><strong>距离 + 线性结构⟶线性空间 = 向量空间</strong><br>线性结构如向量的加法、数乘，使其满足加法的交换律、结合律、零元、负元；数乘的交换律、单位一；数乘与加法的结合律（两个）共八点要求，从而形成一个线性空间，这个线性空间就是向量空间</p><h4 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h4><p>范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。可以简单把范数当作距离来理解。<br>范数分为：向量范数和矩阵范数。向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。<br>并满足一定的条件，即①非负性；②齐次性；③三角不等式。它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。</p><ol><li>||x|| ≥0; </li><li>||ax||=|a|||x||; </li><li>||x+y||≤||x||+||y||。</li></ol><p><strong>范数的集合⟶ 赋范空间<br>赋范空间 +线性结构⟶线性赋范空间</strong></p><p><strong>距离的集合⟶ 度量空间<br>度量空间 +线性结构⟶线性度量空间</strong></p><h4 id="欧式空间"><a href="#欧式空间" class="headerlink" title="欧式空间"></a>欧式空间</h4><p>有限维线性内积空间称作欧几里得空间<br>已经构成的线性赋范空间上继续扩展，添加内积运算，使空间中有角的概念，形成内积空间：<br><strong>线性赋范空间+内积运算⟶ 内积空间</strong><br>有限维的内积空间也就是欧氏空间<br><strong>内积空间+有限维⟶欧几里德空间</strong></p><h4 id="希尔伯特空间"><a href="#希尔伯特空间" class="headerlink" title="希尔伯特空间"></a>希尔伯特空间</h4><p>线性完备内积空间称作希尔伯特空间<br><strong>线性赋范空间+内积运算⟶ 内积空间</strong><br><strong>内积空间+完备性⟶ 希尔伯特空间</strong><br>其中完备性的意思就是空间中的极限运算不能跑出该空间</p><h4 id="巴拿赫空间"><a href="#巴拿赫空间" class="headerlink" title="巴拿赫空间"></a>巴拿赫空间</h4><p>线性完备赋范空间称作巴拿赫空间<br><strong>范数的集合⟶ 赋范空间</strong><br><strong>赋范空间 +线性结构⟶线性赋范空间</strong><br><strong>线性赋范空间+完备性⟶巴拿赫空间</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;距离&quot;&gt;&lt;a href=&quot;#距离&quot; class=&quot;headerlink&quot; title=&quot;距离&quot;&gt;&lt;/a&gt;距离&lt;/h4&gt;&lt;p&gt;直线距离、向量距离、折线距离等等都是具体的距离，这里谈的距离是一个抽象的概念，需要满足非负、自反、三角不等式三个条件。&lt;br&gt;设X是任一非空
      
    
    </summary>
    
      <category term="总结" scheme="http://minhzou.top/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="基础" scheme="http://minhzou.top/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>范数分类</title>
    <link href="http://minhzou.top/2019/06/25/%E8%8C%83%E6%95%B0%E5%88%86%E7%B1%BB/"/>
    <id>http://minhzou.top/2019/06/25/范数分类/</id>
    <published>2019-06-25T01:54:16.471Z</published>
    <updated>2019-06-25T01:53:38.517Z</updated>
    
    <content type="html"><![CDATA[<h4 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h4><p>即对一个线性空间X，定义泛函|| · ||：X→R满足</p><ol><li>$|x| \geq 0$ and $|x|=0 \leftrightarrow x=0$ for $x \in X$（正定）</li><li>$|c x|=|c||x|$ for $c \in \mathbb{R}, x \in X$（齐次）</li><li>$|a+b| \leq|a|+|b|$ for any $a, b \in X$（三角不等式）</li></ol><h4 id="L-P范数"><a href="#L-P范数" class="headerlink" title="L-P范数"></a>L-P范数</h4><p>L-P范数不是一个范数，而是一组范数<br>$L_{p}=|\mathbf{x}|_{p}=\sqrt[p]{\sum_{i=1}^{n} x_{i}^{p}}, \mathbf{x}=\left(x_{1}, x_{2}, \cdots, x_{n}\right)$</p><h4 id="L0范数"><a href="#L0范数" class="headerlink" title="L0范数"></a>L0范数</h4><p>当P=0时，也就是L0范数，L0范数并不是一个真正的范数，它主要被用来度量向量中非零元素的个数，可以定义为：<br>$|x|_{0}=\sum_{i=1}^{k}\left|x_{i}\right|^{0}$<br>L0范数表示向量中非零元素的个数。如果我们使用L0来规则化参数向量w，就是希望w的元素大部分都为零。L0范数的这个属性，使其非常适用于机器学习中的稀疏编码。在特征选择中，通过最小化L0范数来寻找最少最优的稀疏特征项。但是，L0范数的最小化问题是NP难问题。而L1范数是L0范数的最优凸近似，它比L0范数要更容易求解。因此，优化过程将会被转换为更高维的范数（例如L1范数）问题。</p><h4 id="L1范数"><a href="#L1范数" class="headerlink" title="L1范数"></a>L1范数</h4><p>L1范数是向量中各个元素绝对值之和，也被称作“Lasso regularization”（稀疏规则算子）<br>$|x|_{1}=\sum_{i=1}^{k}\left|x_{i}\right|$</p><p>在机器学习特征选择中，稀疏规则化能够实现特征的自动选择。一般来说，输入向量X的大部分元素（也就是特征）都是和最终的输出Y没有关系或者不提供任何信息的，在最小化目标函数的时候考虑这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确Y的预测。稀疏规则化算子的引入就是为了完成特征自动选择，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。</p><p>L0范数与L1范数都可以实现稀疏，而L1范数比L0具有更好的优化求解特性而被广泛使用。 L0范数本身是特征选择的最直接的方案，但因为之前说到的理由，其不可分，且很难优化，因此实际应用中我们使用L1来得到L0的最优凸近似。</p><p>总结一下上两段的结论就是：L1范数和L0范数可以实现稀疏，L1因为拥有比L0更好的优化求解特性而被广泛应用。这样我们大概知道了可以实现稀疏，但是为什么我们希望稀疏？让参数稀疏有什么好处呢？这里有两个理由：</p><ul><li><ol><li>特征选择(Feature Selection)：<br>大家希望稀疏规则化的一个关键原因在于它能实现特征的自动选择。一般来说，X的大部分元素（也就是特征）都是和最终的输出没有关系或者不提供任何信息的，在最小化目标函数的时候考虑这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0</li></ol></li><li><ol><li>可解释性(Interpretability)：<br>另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设这是个回归模型：<br>$y=\sum_{i=1}^{1000} w_{i} * x_{i}+b$<br>当然了，为了让y限定在的范围，一般还得加个Logistic函数。 通过学习，如果最后学习到的就只有很少的非零元素，例如只有5个非零的，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个都非0，医生面对这1000种因素只能一脸懵逼不知如何是好。</li></ol></li></ul><h4 id="L2范数"><a href="#L2范数" class="headerlink" title="L2范数"></a>L2范数</h4><p>$|x|_{2}=\sqrt{\sum_{i=1}^{k}\left|x_{i}\right|^{2}}$<br>L2范数是最常用的范数了，我们用的最多的度量距离欧氏距离就是一种L2范数。在回归里面，有人把加了L2范数项的回归c称为“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。它被广泛的应用在解决机器学习里面的过拟问题合。</p><p>为什么L2范数可以防止过拟合？回答这个问题之前，我们得先看看L2范数实际上是什么。</p><p>L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项最小，可以使得的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这是有很大的区别的。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？因为当限制了参数很小，实际上就限制了多项式某些分量的影响很小，这样就相当于减少参数个数。</p><p>总结下：通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。</p><p>L2范数的好处是什么呢？</p><p>1）学习理论的角度： 从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。</p><p>2）优化计算的角度： 从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。</p><p>来源：<a href="https://baijiahao.baidu.com/s?id=1607333156323286278&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1607333156323286278&amp;wfr=spider&amp;for=pc</a><br><a href="https://blog.csdn.net/zouxy09/article/details/24971995" target="_blank" rel="noopener">https://blog.csdn.net/zouxy09/article/details/24971995</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;范数&quot;&gt;&lt;a href=&quot;#范数&quot; class=&quot;headerlink&quot; title=&quot;范数&quot;&gt;&lt;/a&gt;范数&lt;/h4&gt;&lt;p&gt;即对一个线性空间X，定义泛函|| · ||：X→R满足&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$|x| \geq 0$ and $|x|=0 \left
      
    
    </summary>
    
      <category term="总结" scheme="http://minhzou.top/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="范数" scheme="http://minhzou.top/tags/%E8%8C%83%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》第3章 线性回归模型 公式推导</title>
    <link href="http://minhzou.top/2019/06/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC3%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://minhzou.top/2019/06/24/《机器学习》第3章 线性回归模型 公式推导/</id>
    <published>2019-06-24T10:08:03.801Z</published>
    <updated>2019-07-16T02:04:01.463Z</updated>
    
    <content type="html"><![CDATA[<h3 id="第3章-线性回归模型"><a href="#第3章-线性回归模型" class="headerlink" title="第3章 线性回归模型"></a>第3章 线性回归模型</h3><h4 id="一元线性回归公式推导"><a href="#一元线性回归公式推导" class="headerlink" title="一元线性回归公式推导"></a>一元线性回归公式推导</h4><h5 id="1-求解偏置b"><a href="#1-求解偏置b" class="headerlink" title="1. 求解偏置b"></a>1. 求解偏置b</h5><p>推导思路：<br>①由最小二乘法导出损失函数$E(w, b)$</p><script type="math/tex; mode=display">\begin{aligned} E_{(w, b)} &=\sum_{i=1}^{m}\left(y_{i}-f\left(x_{i}\right)\right)^{2} \\ &=\sum_{i=1}^{m}\left(y_{i}-\left(w x_{i}+b\right)\right)^{2} \\ &=\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2} \end{aligned}</script><p>②证明损失函数$E(w, b)$是关于w和b的凸函数</p><ul><li>求$A=f_{x x}^{\prime \prime}(x, y)$ ：<script type="math/tex; mode=display">\begin{aligned} \frac{\partial E_{(w, b)}}{\partial w} &=\frac{\partial}{\partial w}\left[\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\right] \\ &=\sum_{i=1}^{m} \frac{\partial}{\partial w}\left(y_{i}-w x_{i}-b\right)^{2} \\ &=\sum_{i=1}^{m} 2 \cdot\left(y_{i}-w x_{i}-b\right) \cdot\left(-x_{i}\right) \\ &=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right) \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} \frac{\partial^{2} E_{(w, b)}}{\partial w^{2}} &=\frac{\partial}{\partial w}\left(\frac{\partial E_{(w, b)}}{\partial w}\right) \\ &=\frac{\partial}{\partial w}\left[2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)\right] \\ &=\frac{\partial}{\partial w}\left[2 w \sum_{i=1}^{m} x_{i}^{2}\right] \\ & = 2 \sum_{i=1}^{m} x_{i}^{2} \end{aligned}</script></li><li>求$B=f_{x y}^{\prime \prime}(x, y)$：<script type="math/tex; mode=display">\begin{aligned} \frac{\partial^{2} E_{(w, b)}}{\partial w \partial b} &=\frac{\partial}{\partial b}\left(\frac{\partial E_{(w, b)}}{\partial w}\right) \\ &=\frac{\partial}{\partial b}\left[2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)\right] \\ &=\frac{\partial}{\partial b}\left(-2 \sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right] \\ &=\frac{\partial}{\partial b}\left(-2 \sum_{i=1}^{m} y_{i} x_{i}+2 \sum_{i=1}^{m} b x_{i}\right) \\ &=\frac{\partial}{\partial b}\left(2 \sum_{i=1}^{m} b x_{i}\right)=2 \sum_{i=1}^{m} x_{i} \end{aligned}</script></li><li>求$C=f_{y y}^{\prime \prime}(x, y)$：<script type="math/tex; mode=display">\begin{aligned} \frac{\partial E_{(w, b)}}{\partial b} &=\frac{\partial}{\partial b}\left[\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\right] \\ &=\sum_{i=1}^{m} \frac{\partial}{\partial b}\left(y_{i}-w x_{i}-b\right)^{2} \\ &=\sum_{i=1}^{m} 2 \cdot\left(y_{i}-w x_{i}-b\right) \cdot(-1) \\ &=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right) \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} \frac{\partial^{2} E_{(w, b)}}{\partial b^{2}} &=\frac{\partial}{\partial b}\left(\frac{\partial E_{(w, b)}}{\partial b}\right) \\ &=\frac{\partial}{\partial b}\left[2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)\right] \\ &=\frac{\partial}{\partial b}(2 m b) \\ &=2m  \end{aligned}</script></li><li>证明<script type="math/tex; mode=display">A=2 \sum_{i=1}^{m} x_{i}^{2} \qquad B=2 \sum_{i=1}^{m} x_{i} \qquad C=2 m</script><script type="math/tex; mode=display">\begin{aligned} A C-B^{2} &=2 m \cdot 2 \sum_{i=1}^{m} x_{i}^{2}-\left(2 \sum_{i=1}^{m} x_{i}\right)^{2}=4 m \sum_{i=1}^{m} x_{i}^{2}-4\left(\sum_{i=1}^{m} x_{i}\right)^{2}=4 m \sum_{i=1}^{m} x_{i}^{2}-4 \cdot m \cdot \frac{1}{m} \cdot\left(\sum_{i=1}^{m} x_{i}\right)^{2} \\ &=4 m \sum_{i=1}^{m} x_{i}^{2}-4 m \cdot \overline{x} \cdot \sum_{i=1}^{m} x_{i}=4 m\left(\sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m} x_{i} \overline{x}\right)=4 m \sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}\right) \end{aligned}</script>又因为：$\sum_{i=1}^{m} x_{i} \overline{x}=\overline{x} \sum_{i=1}^{m} x_{i}=\overline{x} \cdot m \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} x_{i}=m \overline{x}^{2}=\sum_{i=1}^{m} \overline{x}^{2}$<script type="math/tex; mode=display">=4 m \sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}-x_{i} \overline{x}+x_{i} \overline{x}\right)=4 m \sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}-x_{i} \overline{x}+\overline{x}^{2}\right)=4 m \sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)^{2}</script>所以：$A C-B^{2}=4 m \sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)^{2} \geq 0$<br>也即损失函数$E(w, b)$是关于w和b的凸函数得证<br>③对损失函数$E(w, b)$关于b求一阶偏导数<script type="math/tex; mode=display">\frac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)</script>④令一阶偏导数等于0解出b<script type="math/tex; mode=display">\begin{aligned} \frac{\partial E_{(w, b)}}{\partial b} &=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)=0 \\ & m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)=0 \\ b &=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right) \end{aligned}</script><script type="math/tex; mode=display">b=\frac{1}{m} \sum_{i=1}^{m} y_{i}-w \cdot \frac{1}{m} \sum_{i=1}^{m} x_{i}=\overline{y}-w \overline{x}</script></li></ul><h5 id="2-求解权重w"><a href="#2-求解权重w" class="headerlink" title="2. 求解权重w"></a>2. 求解权重w</h5><p>推导思路：<br>①由最小二乘法导出损失函数$E(w, b)$<br>②证明损失函数$E(w, b)$是关于w和b的凸函数<br>③对损失函数$E(w, b)$关于w求一阶偏导数<br>④令一阶偏导数等于0解出w</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial E_{(w, b)}}{\partial w}= 2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)=0 \\ w  \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}=0 \\ w \sum_{i=1}^{m} x_{i}^{2} =\sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m} b x_{i} \end{aligned}</script><p>将$b=\overline{y}-w \overline{x}$代入$w \sum_{i=1}^{m} x_{i}^{2}=\sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m} b x_{i}$得</p><script type="math/tex; mode=display">w \sum_{i=1}^{m} x_{i}^{2}=\sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m}(\overline{y}-w \overline{x}) x_{i}</script><script type="math/tex; mode=display">w \sum_{i=1}^{m} x_{i}^{2}=\sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i}+w \overline{x} \sum_{i=1}^{m} x_{i}</script><script type="math/tex; mode=display">w \sum_{i=1}^{m} x_{i}^{2}-w \overline{x} \sum_{i=1}^{m} x_{i}=\sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i}</script><script type="math/tex; mode=display">w\left(\sum_{i=1}^{m} x_{i}^{2}-\overline{x} \sum_{i=1}^{m} x_{i}\right)=\sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i}</script><script type="math/tex; mode=display">w=\frac{\sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i}}{\sum_{i=1}^{m} x_{i}^{2}-\overline{x} \sum_{i=1}^{m} x_{i}}</script><p>根据</p><script type="math/tex; mode=display">\overline{y} \sum_{i=1}^{m} x_{i}=\frac{1}{m} \sum_{i=1}^{m} y_{i} \sum_{i=1}^{m} x_{i}=\overline{x} \sum_{i=1}^{m} y_{i}</script><script type="math/tex; mode=display">\overline{x} \sum_{i=1}^{m} x_{i}=\frac{1}{m} \sum_{i=1}^{m} x_{i} \sum_{i=1}^{m} x_{i}=\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}</script><p>得</p><script type="math/tex; mode=display">w=\frac{\sum_{i=1}^{m} y_{i} x_{i}-\overline{x} \sum_{i=1}^{m} y_{i}}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}</script><h5 id="3-将w向量化"><a href="#3-将w向量化" class="headerlink" title="3. 将w向量化"></a>3. 将w向量化</h5><script type="math/tex; mode=display">w=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}</script><p>将$\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}=\overline{x} \sum_{i=1}^{m} x_{i}=\sum_{i=1}^{m} x_{i} \overline{x}$代入分母可得</p><script type="math/tex; mode=display">\begin{aligned} w &=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m} x_{i} \overline{x}} \\ &=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \overline{x}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}\right)} \end{aligned}</script><p>由于</p><script type="math/tex; mode=display">\sum_{i=1}^{m} y_{i} \overline{x}=\overline{x} \sum_{i=1}^{m} y_{i}=\frac{1}{m} \sum_{i=1}^{m} x_{i} \sum_{i=1}^{m} y_{i}=\sum_{i=1}^{m} x_{i} \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} y_{i}=\sum_{i=1}^{m} x_{i} \overline{y}</script><script type="math/tex; mode=display">\sum_{i=1}^{m} y_{i} \overline{x}=\overline{x} \sum_{i=1}^{m} y_{i}=\overline{x} \cdot m \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} y_{i}=m \overline{x} \overline{y}=\sum_{i=1}^{m} \overline{x} \overline{y}</script><script type="math/tex; mode=display">\sum_{i=1}^{m} x_{i} \overline{x}=\overline{x} \sum_{i=1}^{m} x_{i}=\overline{x} \cdot m \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} x_{i}=m \overline{x}^{2}=\sum_{i=1}^{m} \overline{x}^{2}</script><p>所以</p><script type="math/tex; mode=display">\begin{aligned} w &=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \overline{x}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}\right)}=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \overline{x}-y_{i} \overline{x}+y_{i} \overline{x}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}-x_{i} \overline{x}+x_{i} \overline{x}\right)} \\ &=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \overline{x}-x_{i} \overline{y}+\overline{x} \overline{y}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}-x_{i} \overline{x}+\overline{x}^{2}\right)}=\frac{\sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)\left(y_{i}-\overline{y}\right)}{\sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)^{2}} \end{aligned}</script><p>令</p><script type="math/tex; mode=display">\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{m}\right)^{T} \quad \boldsymbol{y}=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{T}</script><script type="math/tex; mode=display">\boldsymbol{x}_{d}=\left(x_{1}-\overline{x}, x_{2}-\overline{x}, \ldots, x_{m}-\overline{x}\right)^{T} \quad \boldsymbol{y}_{d}=\left(y_{1}-\overline{y}, y_{2}-\overline{y}, \ldots, y_{m}-\overline{y}\right)^{T}</script><p>则</p><script type="math/tex; mode=display">\begin{aligned} w &=\frac{\sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)\left(y_{i}-\overline{y}\right)}{\sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)^{2}} \\ &=\frac{\boldsymbol{x}_{d}^{T} \boldsymbol{y}_{d}}{\boldsymbol{x}_{d}^{T} \boldsymbol{x}_{d}} \end{aligned}</script><h4 id="多元线性回归公式推导"><a href="#多元线性回归公式推导" class="headerlink" title="多元线性回归公式推导"></a>多元线性回归公式推导</h4><h5 id="求解权重-hat-w-的公示推导"><a href="#求解权重-hat-w-的公示推导" class="headerlink" title="求解权重$\hat{w}$的公示推导"></a>求解权重$\hat{w}$的公示推导</h5><p>推导思路：<br>将w和b组合成$\hat{\boldsymbol{w}}$：</p><script type="math/tex; mode=display">f\left(\boldsymbol{x}_{i}\right)=\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b</script><script type="math/tex; mode=display">f\left(\boldsymbol{x}_{i}\right)=\left(\begin{array}{llll}{w_{1}} & {w_{2}} & {\cdots} & {w_{d}}\end{array}\right)\left(\begin{array}{c}{x_{i 1}} \\ {x_{i 2}} \\ {\vdots} \\ {x_{i d}}\end{array}\right)+b</script><script type="math/tex; mode=display">f\left(\boldsymbol{x}_{i}\right)=w_{1} x_{i 1}+w_{2} x_{i 2}+\ldots+w_{d} x_{i d}+b</script><script type="math/tex; mode=display">f\left(\boldsymbol{x}_{i}\right)=w_{1} x_{i 1}+w_{2} x_{i 2}+\ldots+w_{d} x_{i d}+w_{d+1} \cdot 1</script><script type="math/tex; mode=display">f\left(\boldsymbol{x}_{i}\right)=\left(\begin{array}{llllll}{w_{1}} & {w_{2}} & {\cdots} & {w_{d}} & {w_{d+1}}\end{array}\right)\left(\begin{array}{c}{x_{i 1}} \\ {x_{i 2}} \\ {\vdots} \\ {x_{i d}} \\ {1}\end{array}\right)</script><script type="math/tex; mode=display">f\left(\hat{\boldsymbol{x}}_{i}\right)=\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{i}</script><p>①由最小二乘法导出损失函数$E_{\hat{\boldsymbol{w}}}$</p><script type="math/tex; mode=display">\begin{aligned} E_{\hat{\boldsymbol{w}}} &=\sum_{i=1}^{m}\left(y_{i}-f\left(\hat{\boldsymbol{x}}_{i}\right)\right)^{2} \\ &=\sum_{i=1}^{m}\left(y_{i}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{i}\right)^{2} \end{aligned}</script><script type="math/tex; mode=display">\mathbf{X}=\left(\begin{array}{ccccc}{x_{11}} & {x_{12}} & {\dots} & {x_{1 d}} & {1} \\ {x_{21}} & {x_{22}} & {\dots} & {x_{2 d}} & {1} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\vdots} \\ {x_{m 1}} & {x_{m 2}} & {\dots} & {x_{m d}} & {1}\end{array}\right)=\left(\begin{array}{cc}{\boldsymbol{x}_{1}^{\mathrm{T}}} & {1} \\ {\boldsymbol{x}_{2}^{\mathrm{T}}} & {1} \\ {\vdots} & {\vdots} \\ {\boldsymbol{x}_{m}^{\mathrm{T}}} & {1}\end{array}\right)=\left(\begin{array}{c}{\hat{\boldsymbol{x}}_{1}^{T}} \\ {\hat{\boldsymbol{x}}_{2}^{T}} \\ {\vdots} \\ {\hat{\boldsymbol{x}}_{m}^{T}}\end{array}\right)</script><script type="math/tex; mode=display">\boldsymbol{y}=\left(y_{1}, y_{2}, \dots, y_{m}\right)^{T}</script><script type="math/tex; mode=display">\begin{aligned} E_{\hat{\boldsymbol{w}}} &=\sum_{i=1}^{m}\left(y_{i}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{i}\right)^{2} \\ &=\left(y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1}\right)^{2}+\left(y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2}\right)^{2}+\ldots+\left(y_{m}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{m}\right)^{2} \end{aligned}</script><p>抽象成向量的内积：</p><script type="math/tex; mode=display">E_{\hat{\boldsymbol{w}}}=\left(y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1} \quad y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2} \quad \cdots \quad y_{d}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{d}\right)\left(\begin{array}{c}{y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1}} \\ {y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2}} \\ {\vdots} \\ {y_{d}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{d}}\end{array}\right)</script><p>又因为：</p><script type="math/tex; mode=display">\left(\begin{array}{c}{y_{1}-\hat{w}^{T} \hat{x}_{1}} \\ {y_{2}-\hat{w}^{T} \hat{x}_{2}} \\ {\vdots} \\ {y_{d}-\hat{w}^{T} \hat{x}_{d}}\end{array}\right)=\left(\begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{d}}\end{array}\right)-\left(\begin{array}{c}{\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1}} \\ {\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2}} \\ {\vdots} \\ {\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{d}}\end{array}\right)=\left(\begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{d}}\end{array}\right)-\left(\begin{array}{c}{\hat{\boldsymbol{x}}_{1}^{T} \hat{\boldsymbol{w}}} \\ {\hat{\boldsymbol{x}}_{2}^{T} \hat{\boldsymbol{w}}} \\ {\vdots} \\ {\hat{\boldsymbol{x}}_{d}^{T} \hat{\boldsymbol{w}}}\end{array}\right)</script><script type="math/tex; mode=display">\left(\begin{array}{c}{\hat{\boldsymbol{x}}_{1}^{T} \hat{\boldsymbol{w}}} \\ {\hat{\boldsymbol{x}}_{2}^{T} \hat{\boldsymbol{w}}} \\ {\vdots} \\ {\hat{\boldsymbol{x}}_{d}^{T} \hat{\boldsymbol{w}}}\end{array}\right)=\left(\begin{array}{c}{\hat{\boldsymbol{x}}_{1}^{T}} \\ {\hat{\boldsymbol{x}}_{2}^{T}} \\ {\vdots} \\ {\hat{\boldsymbol{x}}_{d}^{T}}\end{array}\right) \cdot \boldsymbol{\hat { w }}=\mathbf{X} \cdot \hat{\boldsymbol{w}}</script><p>所以</p><script type="math/tex; mode=display">\left(\begin{array}{c}{y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1}} \\ {y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2}} \\ {\vdots} \\ {y_{d}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{d}}\end{array}\right)=\left(\begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{d}}\end{array}\right)-\left(\begin{array}{c}{\hat{\boldsymbol{x}}_{1}^{T} \hat{\boldsymbol{w}}} \\ {\hat{\boldsymbol{x}}_{2}^{T} \hat{\boldsymbol{w}}} \\ {\vdots} \\ {\hat{\boldsymbol{x}}_{d}^{T} \hat{\boldsymbol{w}}}\end{array}\right)=\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}}</script><script type="math/tex; mode=display">E_{\hat{w}}=\left(\begin{array}{cccc}{y_{1}-\hat{w}^{T} \hat{x}_{1}} & {y_{2}-\hat{w}^{T} \hat{x}_{2}} & {\cdots} & {y_{d}-\hat{w}^{T} \hat{x}_{d}}\end{array}\right)\left(\begin{array}{c}{y_{1}-\hat{w}^{T} \hat{x}_{1}} \\ {y_{2}-\hat{w}^{T} \hat{x}_{2}} \\ {\vdots} \\ {y_{d}-\hat{w}^{T} \hat{x}_{d}}\end{array}\right)$$=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})</script><p>②证明损失函数$E_{\hat{\boldsymbol{w}}}$是关于$\hat{w}$的凸函数</p><ul><li>凸集定义：设集合$D \in R^{n}$，如果对任意的$x, y \in D$与E意的$a \in[0,1]$，有$a \boldsymbol{x}+(1-a) \boldsymbol{y} \in D$，则称集$D$凸集。</li><li><p>凸集的几何意义是：若两个点属于此集合，则这两点连线上的任意一点均属于此集合。</p></li><li><p>梯度定义：设n元函数$f(\boldsymbol{x})$对自变量$\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{T}$的各分量$x_{i}$的偏导数$\frac{\partial f(\boldsymbol{x})}{\partial x_{i}}(i=1,2, \dots, n)$都存在，则称函数$f(\boldsymbol{x})$在$\mathcal{x}$处一阶可导，并称向量</p><script type="math/tex; mode=display">\nabla f(\boldsymbol{x})=\left(\begin{array}{c}{\frac{\partial f(\boldsymbol{x})}{\partial x_{1}}} \\ {\frac{\partial f(\boldsymbol{x})}{\partial x_{2}}} \\ {\vdots} \\ {\frac{\partial f(\boldsymbol{x})}{\partial x_{n}}}\end{array}\right)</script><p>为函数$f(\boldsymbol{x})$在$\boldsymbol{x}$处的一阶导数或梯度，记为$\nabla f(\boldsymbol{x})$（列向量）</p></li><li><p>Hessian（海塞）矩阵定义：设n元函数$f(\boldsymbol{x})$对自变量$\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{T}$的各分量$\mathcal{x}_{i}$的二阶偏导数$\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{i} \partial x_{j}} \quad(i=1,2, \ldots, n ; j=1,2, \dots, n)$都存在，则称函数$f(\boldsymbol{x})$在点$\mathcal{x}$处二阶可导，并称矩阵</p><script type="math/tex; mode=display">\nabla^{2} f(\boldsymbol{x})=\left[\begin{array}{cccc}{\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{1}^{2}}} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{1} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{2} \partial x_{1}}} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{2}^{2}}} & {\cdots} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{2} \partial x_{n}}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{n} \partial x_{1}}} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{n} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{n}^{2}}}\end{array}\right]</script><p>为$f(\boldsymbol{x})$在$\mathcal{x}$处的二阶导数或Hessian矩阵，记为$\nabla^{2} f(\boldsymbol{x})$，若$f(\boldsymbol{x})$对$\mathcal{x}$各变元的所有二阶偏导数都连续，则$\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{i} \partial x_{j}}=\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{j} \partial x_{i}}$此时$\nabla^{2} f(\boldsymbol{x})$为对称矩阵。</p></li><li><p>多元实值函数凹凸性判定定理：<br>设$D \subset R^{n}$是非空开凸集，$f : D \subset R^{n} \rightarrow R$，且$f(\boldsymbol{x})$在D上二阶连续可微，如果$f(\boldsymbol{x})$的Hessian矩阵$\nabla^{2} f(\boldsymbol{x})$在D上是正定的，则$f(\boldsymbol{x})$是D上的严格凸函数。</p></li><li>凸充分性定理：<br>若$f : R^{n} \rightarrow R$是凸函数，且$f(\boldsymbol{x})$一阶连续可微，则$\boldsymbol{x}^{ \ast }$是全局解的充分必要条件是$\nabla f\left(\boldsymbol{x}^{ \ast }\right)=\mathbf{0}$，其中$\nabla f(\boldsymbol{x})$为$f(\boldsymbol{x})$关于$\mathcal{x}$的一阶导数（也称梯度）<br>参考文献：王燕军，梁治安，最优化基础理论与方法.复旦大学出版社，2011</li></ul><script type="math/tex; mode=display">\begin{aligned} \frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}} &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})\right] \\ &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[\left(\boldsymbol{y}^{T}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T}\right)(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})\right] \\ &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[\boldsymbol{y}^{T} \boldsymbol{y}-\boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}\right] \\ &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[-\boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}\right] \end{aligned}</script><p>【标量-向量】的矩阵微分公式为：<br>（分母布局）默认采用：</p><script type="math/tex; mode=display">\frac{\partial y}{\partial x}=\left(\begin{array}{c}{\frac{\partial y}{\partial x_{1}}} \\ {\frac{\partial y}{\partial x_{2}}} \\ {\vdots} \\ {\frac{\partial y}{\partial x_{n}}}\end{array}\right)</script><p>（分子布局）：</p><script type="math/tex; mode=display">\frac{\partial y}{\partial \boldsymbol{x}}=\left(\begin{array}{ccc}{\frac{\partial y}{\partial x_{1}}} & {\frac{\partial y}{\partial x_{2}}} & {\cdots} & {\frac{\partial y}{\partial x_{n}}}\end{array}\right)</script><p>其中，$\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{T}$为n维列向量$y$为$\mathcal{x}$的n元标量函数</p><p>由【标量-向量】的矩阵微分公式可推得：</p><script type="math/tex; mode=display">\frac{\partial \boldsymbol{x}^{T} \boldsymbol{a}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{a}^{T} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\begin{array}{c}{\frac{\partial\left(a_{1} x_{1}+a_{2} x_{2}+\ldots+a_{n} x_{n}\right)}{\partial x_{1}}} \\ {\frac{\partial\left(a_{1} x_{1}+a_{2} x_{2}+\ldots+a_{n} x_{n}\right)}{\partial x_{2}}} \\ {\vdots} \\ {\frac{\partial\left(a_{1} x_{1}+a_{2} x_{2}+\ldots+a_{n} x_{n}\right)}{\partial x_{n}}}\end{array}\right)=\left(\begin{array}{c}{a_{1}} \\ {a_{2}} \\ {\vdots} \\ {a_{n}}\end{array}\right)=a</script><p>同理可推得：$\frac{\partial \boldsymbol{x}^{T} \mathbf{B} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\mathbf{B}+\mathbf{B}^{T}\right) \boldsymbol{x}$</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}} &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[-\boldsymbol{y}^{T} \mathbf{X} \boldsymbol{w}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}\right] \\ &=-\frac{\partial \boldsymbol{y}^{T} \mathbf{X} \hat{w}}{\partial \hat{\boldsymbol{w}}}-\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}}{\partial \hat{\boldsymbol{w}}}+\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}} \end{aligned}</script><p>由矩阵微分公式$\frac{\partial \boldsymbol{x}^{T} \boldsymbol{a}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{a}^{T} \boldsymbol{x}}{\partial \boldsymbol{x}}=\boldsymbol{a}, \frac{\partial \boldsymbol{x}^{T} \mathbf{B} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\mathbf{B}+\mathbf{B}^{T}\right) \boldsymbol{x}$可得：</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial E_{\hat{w}}}{\partial \hat{w}} &=-\mathbf{X}^{T} y-\mathbf{X}^{T} \boldsymbol{y}+\left(\mathbf{X}^{T} \mathbf{X}+\mathbf{X}^{T} \mathbf{X}\right) \hat{w} \\ &=2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-\boldsymbol{y}) \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} \frac{\partial^{2} E_{\hat{w}}}{\partial \hat{w} \partial \hat{w}^{T}} &=\frac{\partial}{\partial \hat{w}}\left(\frac{\partial E_{\hat{w}}}{\partial \hat{w}}\right) \\ &=\frac{\partial}{\partial \hat{w}}\left[2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-y)\right] \\ &=\frac{\partial}{\partial \hat{w}}\left(2 \mathbf{X}^{T} \mathbf{X} \hat{w}-2 \mathbf{X}^{T} \boldsymbol{y}\right) \\ &=2 \mathbf{X}^{T} \mathbf{X} \end{aligned}</script><p>此即为Hessian矩阵<br>③对损失函数$E_{\hat{\boldsymbol{w}}}$关于$\hat{w}$求一阶导数</p><script type="math/tex; mode=display">\frac{\partial E_{\hat{w}}}{\partial \hat{w}}=2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-\boldsymbol{y})</script><p>④令一阶导数等于0解出$\hat{\boldsymbol{w}}^{ \ast }$</p><script type="math/tex; mode=display">\frac{\partial E_{\hat{w}}}{\partial \hat{w}}=2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-\boldsymbol{y})=0</script><script type="math/tex; mode=display">2 \mathbf{X}^{T} \mathbf{X} \hat{w}-2 \mathbf{X}^{T} \boldsymbol{y}=0</script><script type="math/tex; mode=display">2 \mathbf{X}^{T} \mathbf{X} \hat{w}=2 \mathbf{X}^{T} \boldsymbol{y}</script><script type="math/tex; mode=display">\hat{w}=\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \boldsymbol{y}</script><h4 id="对数几率回归公式"><a href="#对数几率回归公式" class="headerlink" title="对数几率回归公式"></a>对数几率回归公式</h4><h5 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h5><ul><li>指数族分布<br>指数族（Exponential family）分布是一类分布的总称，该类分布的分布律（或者概率密度函数）的一般形式如下：<script type="math/tex; mode=display">p(y ; \eta)=b(y) \exp \left(\eta^{T} T(y)-a(\eta)\right)</script>其中，$\eta$称为该分布的自然参数；$T(y)$为充分统计量，视具体的分布而定，通常是等于随机变量y本身；$a(\eta)$为配分函数；$b(y)$为关于随机变量y的函数，常见的伯努利分布和正态分布均属于指数族分布。</li></ul><p>证明伯努利分布属于指数族分布：<br>已知伯努利分布的分布律为：</p><script type="math/tex; mode=display">p(y)=\phi^{y}(1-\phi)^{1-y}</script><p>其中$y \in\{0,1\}, \phi$中为y=1的概率，即$p(y=1)=\phi$，对上式恒等变形可得：</p><script type="math/tex; mode=display">\begin{aligned} p(y) &=\phi^{y}(1-\phi)^{1-y} \\ &=\exp \left(\ln \left(\phi^{y}(1-\phi)^{1-y}\right)\right) \\ &=\exp \left(\ln \phi^{y}+\ln (1-\phi)^{1-y}\right) \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} p(y) &=\exp (y \ln \phi+(1-y) \ln (1-\phi)) \\ &=\exp (y \ln \phi+\ln (1-\phi)-y \ln (1-\phi)) \\ &=\exp (y(\ln \phi-\ln (1-\phi))+\ln (1-\phi)) \\ &=\exp \left(y \ln \left(\frac{\phi}{1-\phi}\right)+\ln (1-\phi)\right) \end{aligned}</script><p>对比指数族分布的一般形式$p(y ; \eta)=b(y) \exp \left(\eta^{T} T(y)-a(\eta)\right)$可知：<br>伯努利分布的指数族分布对应参数为：</p><script type="math/tex; mode=display">\begin{aligned} b(y) &=1 \\ \eta &=\ln \left(\frac{\phi}{1-\phi}\right) \\ T(y) &=y \\ a(\eta) &=-\ln (1-\phi)=\ln \left(1+e^{\eta}\right) \end{aligned}</script><ul><li>广义线性模型的三条假设</li></ul><ol><li>在给$\mathcal{x}$ 的条件下，假设随机变量$y$服从某个指数族分布</li><li>在给定$\mathcal{x}$的条件下，我们的目标是得到一个模型$h(\boldsymbol{x})$能预估出$T(y)$的期望值；</li><li>假设该指数族分布中的自然参数$\eta$和$\mathcal{x}$呈线性关系，即$\eta=\boldsymbol{w}^{T} \boldsymbol{x}$<br>参考文献：Andrew Ng.cs229-notes1</li></ol><h5 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h5><p>对数几率回归是在对一个二分类问题进行建模，并且假设被建模的随机变量y取值为0或1，因此我们可以很自然得假设y服从伯努利分布。此时，如果我们想要构建一个线性模型来预测在给定$\mathcal{x}$的条件下y的取值的话，可以考虑使用广义线性模型来进行建模。</p><h6 id="对数几率回归的广义线性模型推导"><a href="#对数几率回归的广义线性模型推导" class="headerlink" title="对数几率回归的广义线性模型推导"></a>对数几率回归的广义线性模型推导</h6><p>已知y是服从伯努利分布，而伯努利分布属于指数族分布，所以满足广义线性模型的第一条假设，接着根据广义线性模型的第二条假设我们可以推得模型$h(\boldsymbol{x})$的表达式为：</p><script type="math/tex; mode=display">h(\boldsymbol{x})=E[T(y | \boldsymbol{x})]</script><p>由于伯努利分布的$T(y | \boldsymbol{x})=y | \boldsymbol{x}$所以：</p><script type="math/tex; mode=display">h(\boldsymbol{x})=E[y | \boldsymbol{x}]</script><p>又因为$E[y | \boldsymbol{x}]=1 \times p(y=1 | \boldsymbol{x})+0 \times p(y=0 | \boldsymbol{x})=p(y=1 | \boldsymbol{x})=\phi$，所以</p><script type="math/tex; mode=display">h(\boldsymbol{x})=\phi</script><p>在前面证明伯努利分布属于指数族分布时我们知道：</p><script type="math/tex; mode=display">\begin{array}{l}{\begin{array}{l}{\eta=\ln \left(\frac{\phi}{1-\phi}\right)} \\ {e^{\eta}=\frac{\phi}{1-\phi}} \\ {e^{-\eta}=\frac{1-\phi}{\phi}} \\ {e^{-\eta}=\frac{1}{\phi}-1} \\ {1+e^{-\eta}=\frac{1}{\phi}} \\ {\frac{1}{1+e^{-\eta}}=\phi}\end{array}}\end{array}</script><p>将$\phi=\frac{1}{1+e^{-\eta}}$代入$h(\boldsymbol{x})$的表达式可得：</p><script type="math/tex; mode=display">h(\boldsymbol{x})=\phi=\frac{1}{1+e^{-\eta}}</script><p>根据广义模型的第三条假设$\eta=\boldsymbol{w}^{T} \boldsymbol{x}, h(\boldsymbol{x})$最终可化为：</p><script type="math/tex; mode=display">h(\boldsymbol{x})=\phi=\frac{1}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}}=p(y=1 | \boldsymbol{x})</script><p>此即为对数几率回归模型。</p><h6 id="3、-对数几率回归的参数估计"><a href="#3、-对数几率回归的参数估计" class="headerlink" title="3、 对数几率回归的参数估计"></a>3、 对数几率回归的参数估计</h6><h6 id="极大似然估计法"><a href="#极大似然估计法" class="headerlink" title="极大似然估计法"></a>极大似然估计法</h6><p>极大似然估计法：设总体的概率密度函数（或分布律）为$f\left(y, w_{1}, w_{2}, \dots, w_{k}\right), y_{1}, y_{2}, \dots, y_{m}$为从该总体中抽出的样本。因为$y_{1}, y_{2}, \dots, y_{m}$相互独立且同分布，于是，它们的联合概率密度函数（或联合概率）为：</p><script type="math/tex; mode=display">L\left(y_{1}, y_{2}, \ldots, y_{m} ; w_{1}, w_{2}, \dots, w_{k}\right)=\prod_{i=1}^{m} f\left(y_{i}, w_{1}, w_{2}, \dots, w_{k}\right)</script><p>其中，$w_{1}, w_{2}, \dots, w_{k}$被看作固定但是未知的参数。当我们已经观测到一组样本观测值$y_{1}, y_{2}, \dots, y_{m}$时，要去估计未知参数，一种直观的想法就是，哪一组参数值使得现在的样本观测值出现的概率最大，哪一组参数可能就是真正的参数，我们就用它作为参数的估计值，这就是所谓的极大似然估计。<br>极大似然估计的具体方法：通常记$L\left(y_{1}, y_{2}, \ldots, y_{m} ; w_{1}, w_{2}, \dots, w_{k}\right)=L(\boldsymbol{w})$，称其为似然函数。于是求$\boldsymbol{w}$的极大似然估计就归结为求$L(\boldsymbol{w})$的最大值点。由于对数函数是单调递增函数，所以</p><script type="math/tex; mode=display">\ln L(\boldsymbol{w})=\ln \left(\prod_{i=1}^{m} f\left(y_{i}, w_{1}, w_{2}, \ldots, w_{k}\right)\right)=\sum_{i=1}^{m} \ln f\left(y_{i}, w_{1}, w_{2}, \ldots, w_{k}\right)</script><p>与$L(\boldsymbol{w})$有相同的最大值点，，而在许多情况下，求$\ln L(\boldsymbol{w})$的最大值点比较简单，于是，我们就将求$L(\boldsymbol{w})$的最大值点转化为了求$\ln L(\boldsymbol{w})$的最大值点，通常称$\ln L(\boldsymbol{w})$为对数似然函数。<br>对数几率回归的极大似然估计：<br>已知随机变量y取1和0的概率分别为</p><script type="math/tex; mode=display">\begin{array}{l}{p(y=1 | \boldsymbol{x})=\frac{e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}} \\ {p(y=0 | \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}}\end{array}</script><p>令$\boldsymbol{\beta}=(\boldsymbol{w} ; b), \hat{\boldsymbol{x}}=(\boldsymbol{x} ; 1)$，则$\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$可简写为$\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}$，于是上式可化简为：</p><script type="math/tex; mode=display">\begin{aligned} p(y=1 | \boldsymbol{x}) &=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}} \\ p(y=0 | \boldsymbol{x}) &=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}} \end{aligned}</script><p>记</p><script type="math/tex; mode=display">\begin{array}{l}{p(y=1 | \boldsymbol{x})=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}=p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})} \\ {p(y=0 | \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}=p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})}\end{array}</script><p>于是，使用一个小技巧即可得到随机变量y的分布律表达式</p><script type="math/tex; mode=display">p(y | \boldsymbol{x} ; \boldsymbol{w}, b)=y \cdot p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})+(1-y) \cdot p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})</script><p>或者</p><script type="math/tex; mode=display">p(y | \boldsymbol{x} ; \boldsymbol{w}, b)=\left[p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{y}\left[p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{1-y}</script><p>根据对数似然函数的定义可知</p><script type="math/tex; mode=display">\ln L(\boldsymbol{w})=\sum_{i=1}^{m} \ln f\left(y_{i}, w_{1}, w_{2}, \dots, w_{k}\right)</script><p>由于此时的y为离散型，所以将对数似然函数中的概率密度函数换成分布律即可</p><script type="math/tex; mode=display">\ell(\boldsymbol{w}, b) :=\ln L(\boldsymbol{w}, b)=\sum_{i=1}^{m} \ln p\left(y_{i} | \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)</script><p>将$p(y | \boldsymbol{x} ; \boldsymbol{w}, b)=y \cdot p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})+(1-y) \cdot p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})$代入对数似然函数可得</p><script type="math/tex; mode=display">\ell(\boldsymbol{\beta})=\sum_{i=1}^{m} \ln \left(y_{i} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)+\left(1-y_{i}\right) p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)</script><p>由于$p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}, \quad p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}$所以上式可化为</p><script type="math/tex; mode=display">\begin{aligned} \ell(\boldsymbol{\beta}) &=\sum_{i=1}^{m} \ln \left(\frac{y_{i} e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}+\frac{1-y_{i}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}\right) \\ &=\sum_{i=1}^{m} \ln \left(\frac{y_{i} e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-y_{i}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}\right) \\ &=\sum_{i=1}^{m}\left(\ln \left(y_{i} e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-y_{i}\right)-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right) \end{aligned}</script><p>由于 $y_{i} \in\{0,1\}$，所以<br>当$y_{i}=0$时：</p><script type="math/tex; mode=display">\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(\ln \left(0 \cdot e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-0\right)-\ln \left(1+e^{\boldsymbol{\beta}^{T} \boldsymbol{x}_{i}}\right)\right)=\sum_{i=1}^{m}\left(\ln 1-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{x}_{i}}\right)\right)=\sum_{i=1}^{m}\left(-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)</script><p>当$y_{i}=1$时：</p><script type="math/tex; mode=display">\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(\ln \left(1 \cdot e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-1\right)-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)=\sum_{i=1}^{m}\left(\ln e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)=\sum_{i=1}^{m}\left(\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)</script><p>综合可得</p><script type="math/tex; mode=display">\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(y_{i} \boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)</script><p>若$p(y | \boldsymbol{x} ; \boldsymbol{w}, b)=\left[p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{y}\left[p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{1-y}$，将其代入对数似然函数可得</p><script type="math/tex; mode=display">\begin{aligned} \ell(\boldsymbol{\beta}) &=\sum_{i=1}^{m} \ln \left(\left[p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right]^{y_{i}}\left[p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right]^{1-y_{i}}\right) \\ &=\sum_{i=1}^{m}\left[\ln \left(\left[p_{1} ; \boldsymbol{\beta}\right)\right]^{y_{i}}\right)+\ln \left(\left[p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right]^{1-y_{i}}\right) ] \\ &=\sum_{i=1}^{m}\left[y_{i} \ln \left(p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)+\left(1-y_{i}\right) \ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right] \\ &=\sum_{i=1}^{m}\left\{y_{i}\left[\ln \left(p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)-\ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right]+\ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right\} \end{aligned}</script><script type="math/tex; mode=display">\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left[y_{i} \ln \left(\frac{p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)}{p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)}\right)+\ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right]</script><p>由于$p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}, \quad p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}$，上式可以化为：</p><script type="math/tex; mode=display">\begin{aligned} \ell(\boldsymbol{\beta}) &=\sum_{i=1}^{m}\left[y_{i} \ln \left(e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)+\ln \left(\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}\right)\right] \\ &=\sum_{i=1}^{m}\left(y_{i} \boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right) \end{aligned}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;第3章-线性回归模型&quot;&gt;&lt;a href=&quot;#第3章-线性回归模型&quot; class=&quot;headerlink&quot; title=&quot;第3章 线性回归模型&quot;&gt;&lt;/a&gt;第3章 线性回归模型&lt;/h3&gt;&lt;h4 id=&quot;一元线性回归公式推导&quot;&gt;&lt;a href=&quot;#一元线性回归公式推导&quot;
      
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://minhzou.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理（Digital Image Processing, DIP）</title>
    <link href="http://minhzou.top/2019/06/17/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%EF%BC%88Digital%20Image%20Processing%EF%BC%89/"/>
    <id>http://minhzou.top/2019/06/17/数字图像处理（Digital Image Processing）/</id>
    <published>2019-06-17T09:52:43.251Z</published>
    <updated>2019-06-17T10:20:31.027Z</updated>
    
    <content type="html"><![CDATA[<h4 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h4><h5 id="图像由来"><a href="#图像由来" class="headerlink" title="图像由来"></a>图像由来</h5><p>在计算机2D图像用f(x,y),3D图像用f(x,y,z)表示<br>图像（Image）：也叫位图，保存方式为点阵存储，也称为点阵图像或绘制图像<br>图形（Graphic）：也叫矢量图，用数学方法描述存储，也称为面向对象的图像或绘图图像<br>图像的数字化丢失信息原因：采样和编码丢失数据</p><h5 id="图像的种类"><a href="#图像的种类" class="headerlink" title="图像的种类"></a>图像的种类</h5><p>伪彩图像：图像中每个像素点用RGB索引表示</p><h5 id="图像的处理流程"><a href="#图像的处理流程" class="headerlink" title="图像的处理流程"></a>图像的处理流程</h5><p>数字图像处理的基本流程：图像预处理 - 图像分割 - 图像识别 - 图像建模<br>模式识别：通过计算机用数学的方法来对不同模式进行自动处理和判读</p><h4 id="图像的基本算法"><a href="#图像的基本算法" class="headerlink" title="图像的基本算法"></a>图像的基本算法</h4><h5 id="图像的灰度直方图和二值化-Histogram-and-Threshold-Binary-Operation"><a href="#图像的灰度直方图和二值化-Histogram-and-Threshold-Binary-Operation" class="headerlink" title="图像的灰度直方图和二值化 Histogram and Threshold;  Binary Operation"></a>图像的灰度直方图和二值化 Histogram and Threshold;  Binary Operation</h5><p>Histogram: 横轴表示灰阶，纵轴表示每个灰阶的像素数量<br>灰度直方图的具有双峰性<br>灰度直方图的双峰可以对应于图像中的前景和背景<br>取最优二值化值的算法有Entropy Method, Otsu algorithm, Isodata algorithm<br>最优二值化处于两个峰值之间<br>二值化后的图像只有两个灰阶</p><h5 id="图像卷积及其滤波-Convolution-Correlation"><a href="#图像卷积及其滤波-Convolution-Correlation" class="headerlink" title="图像卷积及其滤波 Convolution; Correlation"></a>图像卷积及其滤波 Convolution; Correlation</h5><p>图像卷积运算的过程：将卷积核覆盖上图像做运算前，要先将卷积核旋转180度，卷积核中心依次覆盖在图像上的每一个像素点上进行相乘并累加作为卷积核中心的值，卷积运算每一个步骤得到的值要存储新开的内存中，不能直接在原图上进行修改<br>提取图像边缘：卷积核本质是利用梯度变化<br>图像除噪：中值滤波，均值滤波，高斯滤波</p><h5 id="图像的基本操作和特征-Basic-Image-Operations-Region-Property"><a href="#图像的基本操作和特征-Basic-Image-Operations-Region-Property" class="headerlink" title="图像的基本操作和特征 Basic Image Operations; Region Property"></a>图像的基本操作和特征 Basic Image Operations; Region Property</h5><p>图像间的基本操作：点操作，线性点操作， 非线性点操作，应用：对比度增强，二值化，边缘线，切割，暗亮调节，图像间噪声去除（星空图），多次曝光，背景去除，运动检测，图像提取（全1），几何操作（旋转，平移），插值<br>图像的邻域操作：四邻域， 八邻域，可用于图像分割<br>图像特征：周长，面积，中心，半径，直径，中心距（Centroid Moments）,朝向（orientation）,极值点，曲率，中值，平均值，extreme points</p><h5 id="VTK-and-ITK"><a href="#VTK-and-ITK" class="headerlink" title="VTK and ITK"></a>VTK and ITK</h5><p>图像处理工具库</p><h4 id="数学形态学"><a href="#数学形态学" class="headerlink" title="数学形态学"></a>数学形态学</h4><h5 id="二值形态学-Binary-Morphology"><a href="#二值形态学-Binary-Morphology" class="headerlink" title="二值形态学 Binary Morphology"></a>二值形态学 Binary Morphology</h5><p>集合思想考虑<br>结构化要素Structuring element(SE)：类似于kernel，原点可不在中心</p><ul><li>膨胀（Dilation）：<br>$D(F, K)=F \oplus K=\bigcup_{b \in K}(\{a+b | a \in F\})$<br>求并集，填充小孔小洞</li><li>腐蚀（Erosion）：<br>$E(F, K)=F \ominus K=\bigcap_{b \in K}(\{a-b | a \in F\})$<br>求交集，删去细枝末节，不满足交换律，不可逆</li><li>开运算（Opening）：<br>$O(F, K)=F \circ K=(F \ominus K) \oplus K$<br>先腐蚀，后膨胀，去除孤立，狭小的区域，结构元素大小的不同将导致滤波效果的不同，图像进行重复的开操作会不会产生新的不同的结果</li><li>闭运算（Closing）：<br>$C(F, K)=F \bullet K=(F \oplus K) \ominus K$<br>先膨胀，后腐蚀， 填平小湖（即小孔），弥合小裂缝，结构元素大小的不同将导致滤波效果的不同</li><li>Hit-and-Miss：<br>$\mathrm{F} \otimes \mathrm{K}= \left(\mathrm{F} \ominus \mathrm{K}_{1}\right) \cap\left(\mathrm{F}^{\mathrm{c}} \ominus \mathrm{K}_{2}\right)^{\mathrm{c}}, \mathrm{K}_{1} \cap \mathrm{K}_{2}=\varnothing, \mathrm{K}_{1} \in \mathrm{K}, \mathrm{K}_{2} \in \mathrm{K}$<br>用于求取特殊的Pattern，可以得到轮廓线、孤立的点</li><li>Pattern Spectrum： 柱状图，在进行过程中需要变换SE，可以区分图像中不同形状的图案，不同尺寸的图案，不同形态的图像Pattern Spectrum不同</li><li>Distance Transform：重复腐蚀，相同SE，distance transform得到的结果中，所有邻域内的最大值点就是skeleton<br>应用：Shape Inspection 不同SE进行开运算</li></ul><h5 id="灰度形态学-Grayscale-Morphology"><a href="#灰度形态学-Grayscale-Morphology" class="headerlink" title="灰度形态学 Grayscale Morphology"></a>灰度形态学 Grayscale Morphology</h5><p>灰度曲线考虑</p><ul><li>灰度膨胀（Grayscale  Dilation）：<br>$D_{G}(F, K)=F \oplus_{g} K=\max_{[a, b] \in K}\{F(m+a, n+b)+K(a, b)\}$<br>例：白区变白变大，黑区变小，会提高图像的整体亮度</li><li>灰度腐蚀（Grayscale Erosion）：<br>$E_{G}(F, K)=F \ominus_{g} K=\min_{[ a, b ] \in K}\{F(m-a, n-b)-K(a, b)\}$<br>例：白区变黑变小，黑区变大</li><li>灰度开运算（Grayscale Opening）：<br>$O_{G}(F, K)=F \circ_{g} K=\left(F \ominus_{g} K\right) \oplus_{g} K$<br>例：抹去小的白区，大的白区变化不大，可能会使图像变模糊</li><li>灰度闭运算（Grayscale Closing）：<br>$C_{G}(F, K)=F \bullet_{g} K=\left(F \oplus_{g} K\right) \ominus_{g} K$<br>例：小黑区变白，大的黑区不怎么变化，可能会使图像变模糊</li></ul><h5 id="分水岭算法-Watershed"><a href="#分水岭算法-Watershed" class="headerlink" title="分水岭算法 Watershed"></a>分水岭算法 Watershed</h5><p>Condition Erosion：M（Maker）种子， V（Mask） 模板，需要不断重复灰度膨胀操作，可以获取图像中的特定区域，可以用来进行灰度重建<br>Grayscale  Reconstruction: </p><ul><li>Grayscale  Opening by Reconstruction（OBR）</li><li>Grayscale  Closing by Reconstruction （CBR）<br>分水岭算法（Watershed）：模仿地理结构中的山川，沟壑，可用于图像分割<br>测地距离（Gedesic Distance）：</li></ul><h4 id="彩色图像和三维图像"><a href="#彩色图像和三维图像" class="headerlink" title="彩色图像和三维图像"></a>彩色图像和三维图像</h4><h5 id="彩色图像"><a href="#彩色图像" class="headerlink" title="彩色图像"></a>彩色图像</h5><p>RGB<br>CMYK<br>HIS 色相（Hue），饱和度（Saturation）和亮度 （Intensity）</p><h5 id="三维图像"><a href="#三维图像" class="headerlink" title="三维图像"></a>三维图像</h5><p>光学断层成像</p><h4 id="图像分割与形状轮廓模型"><a href="#图像分割与形状轮廓模型" class="headerlink" title="图像分割与形状轮廓模型"></a>图像分割与形状轮廓模型</h4><p>重复性（Reproducibility）从大到小排序：全自动分割，手工勾画大体轮廓后自动分割，自动分割后手工修正，纯手工分割</p><h5 id="形变模型"><a href="#形变模型" class="headerlink" title="形变模型"></a>形变模型</h5><p>DDC（Discrete Dynamic Contour）<br>力：$\vec{f}_{i}^{tot}(t)=w^{i m} \vec{f}_{i}^{i m}+w^{i n} \vec{f}_{i}^{i n}+w^{d}\vec{f}_{i}^{d}$<br>$\vec f_{i}^{i m}(t)$：$\vec{f}_{i}^{i m}=-\vec{\nabla} E\left(x_{i}, y_{i}\right)$图像力，使轮廓朝着能量降低的方向运动<br>$\vec f_{i}^{i n}(t)$：内部约束力，通过减小局部曲率，使边缘变得光滑<br>$\vec f_{i}^{d}(t)$：阻止顶点振动，使轮廓形变稳定</p><h5 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h5><p>蛇形算法（snake）：<br>水平集算法（Level Set Algorithm）：通过增加一个维度后的零水平集来表示轮廓，容易地对多个目标进行分割，容易地表示复杂的结构，初始轮廓的敏感性相对较小，在实际运用中，可以采用快速行进（Fast marching）与水平集的组合，以提高运算速度同时保证最后结果的精确</p><h4 id="图像配准"><a href="#图像配准" class="headerlink" title="图像配准"></a>图像配准</h4><p>图像的配准（registration）：图像的融合（fusion），图像的匹配（matching）<br>图像的叠加（superimposition）</p><h5 id="多模态医学图像配准"><a href="#多模态医学图像配准" class="headerlink" title="多模态医学图像配准"></a>多模态医学图像配准</h5><h5 id="图像重采样"><a href="#图像重采样" class="headerlink" title="图像重采样"></a>图像重采样</h5><p>上采样（Super-Sampling）：像素点数目增多<br>下采样（Sub-Sampling）：像素点数目减少，像素点对应的实际空间范围增大</p><h5 id="刚体配准"><a href="#刚体配准" class="headerlink" title="刚体配准"></a>刚体配准</h5><p>线性配准（linear registration）：刚体配准是线性配准中的一部分</p><ul><li>平移(Translation)， 缩放（Scaling）， 旋转（Rotation）</li><li>Rigid3D， Rigid2D</li><li>B-Splines</li><li>仿射变换（affine transformation）</li></ul><p>Intra-modal</p><ul><li>均方误差（Mean Squared Difference）(MSD, minimise), 三维灰度直方图</li><li>正规化的互相关（Normalized cross correlation）（maximise）</li><li>Entropy of difference（minimise）</li><li>Mutal information （maximise）</li><li>正规化的互信息（Normalized mutual information）(maximise)</li><li>Entropy correlation Coefficient(maximise)</li><li>AIR (Automatic image registration) cost function (minimise)</li></ul><h5 id="非刚体配准"><a href="#非刚体配准" class="headerlink" title="非刚体配准"></a>非刚体配准</h5><p>可以用薄板样条（Thin plate spline）描述非线性形变（Non-linear deformation）<br>三次b样条（Cubic B-spline）可以用来描述非线性形变<br>非刚体配准的相似度测度（Non-rigid Registration Similarity Metrics）</p><ul><li>均方误差（Mean squared error）理想值为0</li><li>正规化的互相关（Normalized cross correlation）NCC理想值为1</li><li>正规化的互信息（Normalized mutual information）NMI理想值为2</li><li>理想的联合直方图（Joint histogram）完全集中在对角线上</li></ul><h5 id="互信息测度-mutual-information-measures"><a href="#互信息测度-mutual-information-measures" class="headerlink" title="互信息测度(mutual information measures)"></a>互信息测度(mutual information measures)</h5><p>互信息（Mutual information）<br>联合熵（Joint entropy）<br>联合熵的大小是对联合直方图（Joint histogram）的分散程度的描述</p><h5 id="多解像度配准和评价"><a href="#多解像度配准和评价" class="headerlink" title="多解像度配准和评价"></a>多解像度配准和评价</h5><p>评价配准效果<br>多解像度（multi-resolution）图像配准</p><ul><li>加快配准速度</li><li>增强配准鲁棒性（Robustness）</li><li>避免陷入局部最优值<br>来源：mooc《数字图像处理》 顾力栩</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;绪论&quot;&gt;&lt;a href=&quot;#绪论&quot; class=&quot;headerlink&quot; title=&quot;绪论&quot;&gt;&lt;/a&gt;绪论&lt;/h4&gt;&lt;h5 id=&quot;图像由来&quot;&gt;&lt;a href=&quot;#图像由来&quot; class=&quot;headerlink&quot; title=&quot;图像由来&quot;&gt;&lt;/a&gt;图像由来&lt;/h
      
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="图像处理" scheme="http://minhzou.top/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫基础知识整理</title>
    <link href="http://minhzou.top/2019/06/02/Python%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/"/>
    <id>http://minhzou.top/2019/06/02/Python爬虫基础知识整理/</id>
    <published>2019-06-02T03:36:19.536Z</published>
    <updated>2019-06-02T04:03:07.242Z</updated>
    
    <content type="html"><![CDATA[<p>整理下Python涉及的基本知识，巩固和方便查阅。</p><h5 id="Python爬虫的流程"><a href="#Python爬虫的流程" class="headerlink" title="Python爬虫的流程"></a>Python爬虫的流程</h5><ol><li>获取网页<br>request、urllib和selenium（模拟登陆），多进程多线程抓取、登陆抓取、突破IP封禁和服务器抓取</li><li>解析网页<br>html文档，json格式文本可用re正则表达式、BeautifulSoup和lxml解析，二进制数据（如图片视频）保存</li><li>存储数据<br>保存特定格式的文件如：txt, xls, csv等，存入MySQL数据库和存入MongoDB数据库</li></ol><h5 id="HTML-CSS"><a href="#HTML-CSS" class="headerlink" title="HTML+CSS"></a>HTML+CSS</h5><h6 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h6><p>HTML就是说明一个网页中包含哪些元素，HTML标签指由成对尖括号包围的关键词，解析网页常常需要根据这些标签进行定位。<br>举几个例子：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span>&gt;</span>定义锚</span><br><span class="line"><span class="tag">&lt;<span class="name">dd</span>&gt;</span>定义定义列表中项目的描述。</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span>定义文档中的节</span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span> to <span class="tag">&lt;<span class="name">h6</span>&gt;</span>定义 HTML 标题</span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span>定义列表的项目</span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>定义段落</span><br><span class="line"><span class="tag">&lt;<span class="name">span</span>&gt;</span>定义文档中的节</span><br><span class="line"><span class="tag">&lt;<span class="name">ul</span>&gt;</span>定义无序列表。</span><br></pre></td></tr></table></figure></p><h6 id="CSS"><a href="#CSS" class="headerlink" title="CSS"></a>CSS</h6><p>CSS指层叠样式表 (Cascading Style Sheets)，就是用来让网页中的元素显示的更加美观。<br>CSS 规则由两个主要的部分构成：选择器，以及一条或多条声明，每条声明由一个属性和一个值组成，例如：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">p</span><br><span class="line">&#123;</span><br><span class="line">color:red;</span><br><span class="line">text-align:center;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>id 和 class 选择器：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">#center</span></span><br><span class="line"><span class="undefined">&#123;</span></span><br><span class="line"><span class="undefined">text-align:center;</span></span><br><span class="line"><span class="undefined">color:red;</span></span><br><span class="line"><span class="undefined">&#125; </span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">id</span>=<span class="string">"center"</span>&gt;</span>Hello World!<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">p.center</span></span><br><span class="line"><span class="undefined">&#123;</span></span><br><span class="line"><span class="undefined">text-align:center;</span></span><br><span class="line"><span class="undefined">&#125;</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"center"</span>&gt;</span>Hello World!<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br></pre></td></tr></table></figure><p>id选择器和class选择器的区别就是class可以在多个元素中使用。<br>id 选择器以 “#” 来定义<br>class选择器以一个点”.”号显示</p><h5 id="获取网页"><a href="#获取网页" class="headerlink" title="获取网页"></a>获取网页</h5><h6 id="request库"><a href="#request库" class="headerlink" title="request库"></a>request库</h6><ol><li>请求方式<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">requests.get(<span class="string">'http://www.minhzou.top/'</span>) <span class="comment"># 请求指定的页面信息，并返回实体主体</span></span><br><span class="line">requests.post(<span class="string">'url'</span>)  <span class="comment"># 从请求服务器接受所指定的文档作为对所标识的URI的新的从属实体</span></span><br><span class="line">requests.put(<span class="string">'url'</span>) <span class="comment"># 从客户端向服务器传送的数据取代指定的文档的内容</span></span><br><span class="line">requests.head(<span class="string">'url'</span>) <span class="comment"># 只请求页面的首部</span></span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'http://www.minhzou.top/'</span>)</span><br><span class="line">response = response.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加headers</span></span><br><span class="line">headers = &#123;</span><br><span class="line"><span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">'http://www.minhzou.top/'</span>,headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用params带参数请求</span></span><br><span class="line">data = &#123;</span><br><span class="line"><span class="string">'key1'</span>: <span class="string">'value1'</span>,</span><br><span class="line"><span class="string">'key2'</span>: <span class="string">'value2'</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"http://www.minhzou.top/"</span>, params=data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析json</span></span><br><span class="line">response = requests.get(<span class="string">'http://www.minhzou.top/'</span>)</span><br><span class="line">print(response.json())</span><br><span class="line">print(json.loads(response.text))</span><br><span class="line"></span><br><span class="line"><span class="comment"># .content获取二进制文件</span></span><br><span class="line"><span class="keyword">print</span>（response.content）</span><br><span class="line"></span><br><span class="line"><span class="comment"># post请求</span></span><br><span class="line">data = &#123;</span><br><span class="line"><span class="string">'key1'</span>: <span class="string">'value1'</span>,</span><br><span class="line"><span class="string">'key2'</span>: <span class="string">'value2'</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(<span class="string">"http://www.minhzou.top/"</span>, data=data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 响应</span></span><br><span class="line">response = requests.get(<span class="string">'http://www.minhzou.top/'</span>)</span><br><span class="line">response.status_code</span><br><span class="line">response.headers</span><br><span class="line">response.cookies</span><br><span class="line">response.url</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代理设置</span></span><br><span class="line">proxies = &#123;</span><br><span class="line"> <span class="string">"http"</span>: <span class="string">"http://127.0.0.1:1234"</span>,</span><br><span class="line"> <span class="string">"https"</span>: <span class="string">"https://127.0.0.1:1234"</span>,</span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">"http://www.minhzou.top/"</span>, proxies=proxies)</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 超时设置</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ReadTimeout</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">response = requests.get(<span class="string">"http://www.minhzou.top/"</span>, timeout = <span class="number">0.5</span>)</span><br><span class="line">print(response.status_code)</span><br><span class="line"><span class="keyword">except</span> ReadTimeout:</span><br><span class="line">print(<span class="string">'Timeout'</span>)</span><br></pre></td></tr></table></figure></li></ol><h6 id="urllib库"><a href="#urllib库" class="headerlink" title="urllib库"></a>urllib库</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">urllib.request <span class="comment"># 请求模块</span></span><br><span class="line">urllib.error <span class="comment"># 异常处理模块</span></span><br><span class="line">urllib.parse <span class="comment"># url解析模块</span></span><br><span class="line">urllib.robotparser <span class="comment"># robots.txt解析模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get请求</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>)) <span class="comment"># response.read() 获取到网页的内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># urlopen常用参数</span></span><br><span class="line">urllib.requeset.urlopen(url,data,timeout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用data参数 post请求</span></span><br><span class="line">data = bytes(urllib.parse.urlencode(&#123;<span class="string">'word'</span>: <span class="string">'hello'</span>&#125;), encoding=<span class="string">'utf8'</span>)</span><br><span class="line">print(data)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">'http://www.minhzou.top/'</span>, data=data)</span><br><span class="line">print(response.read())</span><br><span class="line"></span><br><span class="line"><span class="comment"># request添加头部信息</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://www.lagou.com/'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'www.lagou.com'</span></span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;</span><br><span class="line">    <span class="string">'first'</span>: <span class="string">'true'</span>,</span><br><span class="line"><span class="string">'pn'</span>: <span class="string">'1'</span>,</span><br><span class="line"><span class="string">'kd'</span>: <span class="string">'python'</span></span><br><span class="line">&#125;</span><br><span class="line">data = bytes(parse.urlencode(dict), encoding=<span class="string">'utf8'</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">'POST'</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代理，ProxyHandler</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">proxy_handler = urllib.request.ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:1234'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://127.0.0.1:1234'</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = urllib.request.build_opener(proxy_handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.minhzou.top/'</span>)</span><br><span class="line">print(response.read())</span><br><span class="line"></span><br><span class="line"><span class="comment"># cookie,HTTPCookiProcessor</span></span><br><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">"="</span>+item.value)</span><br></pre></td></tr></table></figure><h6 id="selenium库"><a href="#selenium库" class="headerlink" title="selenium库"></a>selenium库</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">chrome_driver = <span class="string">'E:\chromedriver_win32\chromedriver.exe'</span>  <span class="comment"># 需要下载适合浏览器版本的driver</span></span><br><span class="line">browser = webdriver.Chrome(executable_path=chrome_driver)</span><br><span class="line">input = browser.find_element_by_id(<span class="string">"q"</span>)</span><br><span class="line">browser.get(url)</span><br><span class="line">browser.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8种查找元素的方法 / element改成elements 多元素查找</span></span><br><span class="line">find_element_by_name</span><br><span class="line">find_element_by_id</span><br><span class="line">find_element_by_xpath</span><br><span class="line">find_element_by_link_text</span><br><span class="line">find_element_by_partial_link_text</span><br><span class="line">find_element_by_tag_name</span><br><span class="line">find_element_by_class_name</span><br><span class="line">find_element_by_css_selector</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">input = browser.find_element(By.ID,<span class="string">"q"</span>)</span><br><span class="line">print(input)</span><br><span class="line">browser.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 元素交互操作</span></span><br><span class="line"><span class="comment"># 交互动作</span></span><br><span class="line"><span class="comment"># 获取元素属性</span></span><br><span class="line">input.get_attribute(<span class="string">'class'</span>)</span><br><span class="line"><span class="comment"># 获取文本值, ID，位置，标签名</span></span><br><span class="line">text</span><br><span class="line">id</span><br><span class="line">location</span><br><span class="line">tag_name</span><br><span class="line">size</span><br><span class="line"><span class="comment"># 浏览器的前进和后退</span></span><br><span class="line">browser.back()</span><br><span class="line">browser.forward()</span><br><span class="line"><span class="comment"># 选项卡管理</span></span><br><span class="line">browser.execute_script(<span class="string">'window.open()'</span>) <span class="comment"># 新开选项卡</span></span><br><span class="line">browser.switch_to_window(browser.window_handles[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h5 id="解析网页"><a href="#解析网页" class="headerlink" title="解析网页"></a>解析网页</h5><h6 id="re正则表达式"><a href="#re正则表达式" class="headerlink" title="re正则表达式"></a>re正则表达式</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1. 原子</span><br><span class="line">普通字符：abc</span><br><span class="line">非打印字符：\n, \t</span><br><span class="line">通用字符： \w, \W, \d, \D, \s, \S</span><br><span class="line">原子表：[abc],[^abc]</span><br><span class="line">2. 元字符</span><br><span class="line">任意匹配元字符：.    </span><br><span class="line">边界限制元字符： ^ ,$</span><br><span class="line">限定符：* , ?, +, &#123;n&#125;, &#123;n,&#125;, &#123;n,m&#125;</span><br><span class="line">模式选择符：|</span><br><span class="line">模式单元符：() </span><br><span class="line">3. 模式修正</span><br><span class="line">re.I, re.M, re.L, re.U, re.S</span><br><span class="line">4. 贪婪模式与懒惰模式</span><br><span class="line">贪婪模式.*  加问号变成懒惰模式 .* ?</span><br></pre></td></tr></table></figure><h6 id="XPath选择器"><a href="#XPath选择器" class="headerlink" title="XPath选择器"></a>XPath选择器</h6><ol><li><p>基本语法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nodename <span class="comment"># 选取此节点的所有子节点</span></span><br><span class="line">/ <span class="comment"># 从根节点选取</span></span><br><span class="line">// <span class="comment"># 从匹配选择的当前节点选择文档中的节点，而不考虑他们的位置</span></span><br><span class="line">. <span class="comment"># 选取当前节点</span></span><br><span class="line">.. <span class="comment"># 选取当前节点的父节点</span></span><br><span class="line"><span class="meta">@ # 选取属性</span></span><br></pre></td></tr></table></figure></li><li><p>核心思想：写XPath就是写地址</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//标签<span class="number">1</span>[@属性<span class="number">1</span>=“属性值<span class="number">1</span>”]/标签<span class="number">2</span>[@属性=“属性<span class="number">2</span>”]/.../test()</span><br><span class="line">//标签<span class="number">1</span>[@属性<span class="number">1</span>=“属性值<span class="number">1</span>”]/标签<span class="number">2</span>[@属性=“属性<span class="number">2</span>”]/.../@属性n</span><br><span class="line">/artical/div[<span class="number">1</span>]</span><br><span class="line">/artical/div[position()&lt;<span class="number">3</span>]  前两个元素</span><br><span class="line">//div[price&gt;<span class="number">3.5</span>]</span><br><span class="line">//div[@class="center"]/ul/li/text()</span><br></pre></td></tr></table></figure></li></ol><h6 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h6><ol><li><p>基本语法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> *        <span class="comment"># 选取所有节点</span></span><br><span class="line"><span class="comment">#title    # 选取 id 为 title 的元素</span></span><br><span class="line">.col-md   <span class="comment"># 选取所有 class 包含 col-md 的元素</span></span><br><span class="line">li a      <span class="comment"># 选取所有 li 下的 a 元素</span></span><br><span class="line">ul + p    <span class="comment"># 选取 ul 后面的第一个 p 元素</span></span><br><span class="line">div<span class="comment">#title &gt; ul   # 选取 id 为 title 的 div 的第一个 ul 子元素</span></span><br><span class="line">ul ~ p    <span class="comment"># 选取 与 url 相邻的所有 p 元素</span></span><br><span class="line">span<span class="comment">#title ::text  # 选取 id 为 title 的 span 元素的文本值</span></span><br><span class="line">a.link::attr(href) <span class="comment"># 选取 class 为 link 的 a 元素的 href 属性值</span></span><br><span class="line">div:<span class="keyword">not</span>(<span class="comment">#content-container) # 选取所有id为非content-container 的div</span></span><br><span class="line">a:nth-child(<span class="number">2</span>) <span class="comment"># 选取下面第二个标签，如果是a的话则选取，不是则不取</span></span><br><span class="line">a:nth-child(<span class="number">2</span>n) <span class="comment"># 选取第偶数个a元素</span></span><br><span class="line">a:nth-child(<span class="number">2</span>n+<span class="number">1</span>) <span class="comment"># 选取第奇数个a元素</span></span><br><span class="line">a[title] <span class="comment"># 选取所有拥有title属性的a元素</span></span><br><span class="line">a[href=”https://www.lagou.com/”] <span class="comment"># 选取所有href属性为https://www.lagou.com/的a元素</span></span><br><span class="line">a[href*=”www.lagou.com”] <span class="comment"># 选取所有href属性值中包含www.lagou.com的a元素</span></span><br><span class="line">a[href^=”http”] <span class="comment"># 选取所有href属性值中以http开头的a元素</span></span><br></pre></td></tr></table></figure></li><li><p>例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选取文字</span></span><br><span class="line">item.css(<span class="string">'::text'</span>).extract()[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># div a 选取所有div下所有a元素</span></span><br><span class="line">response.css(<span class="string">'div a'</span>)</span><br></pre></td></tr></table></figure></li></ol><h6 id="BeautifulSoup"><a href="#BeautifulSoup" class="headerlink" title="BeautifulSoup"></a>BeautifulSoup</h6><p>提取数据的方式：find选择器，css选择器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find选择器</span></span><br><span class="line">soup.find_all(attrs=&#123;<span class="string">'name'</span>: <span class="string">'elements'</span>&#125;</span><br><span class="line">title = house.find(<span class="string">'div'</span>, class_=<span class="string">'house-title'</span>).a.text.strip()</span><br><span class="line">rooms = house.find(<span class="string">'div'</span>, class_=<span class="string">'details-item'</span>).span.text</span><br><span class="line">soup.find_all(re.compile(), html)</span><br><span class="line"><span class="comment"># css选择器</span></span><br><span class="line">soup.select(<span class="string">"header h3"</span>）</span><br><span class="line">ranking = school.select(<span class="string">'td:nth-of-type(1)'</span>)[<span class="number">0</span>].text</span><br></pre></td></tr></table></figure><h6 id="lxml"><a href="#lxml" class="headerlink" title="lxml"></a>lxml</h6><p>提取数据的方式：XPath选择器，css选择器</p><h5 id="数据保存"><a href="#数据保存" class="headerlink" title="数据保存"></a>数据保存</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写入xls</span></span><br><span class="line">workbook = xlwt.Workbook(encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">worksheet = workbook.add_sheet(<span class="string">'house_price'</span>, cell_overwrite_ok=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> k, row <span class="keyword">in</span> enumerate(information):</span><br><span class="line"><span class="keyword">for</span> j, col <span class="keyword">in</span> enumerate(row):</span><br><span class="line">worksheet.write(k, j, col)</span><br><span class="line">workbook.save(<span class="string">'house_price.xls'</span>)</span><br><span class="line"><span class="comment"># 写入csv</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'ranking.csv'</span>, <span class="string">'a+'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">writer = csv.writer(f)</span><br><span class="line">writer.writerow(data)</span><br><span class="line"><span class="comment"># 写入txt</span></span><br><span class="line">filepath = <span class="string">"E:/data.txt"</span> </span><br><span class="line"><span class="keyword">with</span> open(filepath, <span class="string">"a+"</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(data)</span><br></pre></td></tr></table></figure><h5 id="Scrapy框架"><a href="#Scrapy框架" class="headerlink" title="Scrapy框架"></a>Scrapy框架</h5><p>Scrapy框架：分布式，异步处理请求，自动去重<br>包含部分：<br>Scrapy Engine:控制数据流，触发事件<br>Scheduler: 调度器<br>Downloader:获取页面数据<br>Spiders:获取Item和跟进的url<br>Item Pipeline:处理被Spide提取的Item</p><p>Downloader middlewares: 更换代理IP，cookies, User-Agent，自动重试等<br>Spider middleware：输出、处理异常等</p><p>只简单的在Scrapy框架上写过爬虫，因为要求不高所以很多功能自然也没用上，但是先了解了整个Scrapy，方便如果以后用得到拾起来。</p><h5 id="App爬虫"><a href="#App爬虫" class="headerlink" title="App爬虫"></a>App爬虫</h5><p>基本原理就是抓包分析<br>python还可以使用uiautomato操作Android手机模拟人为操作</p><h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><ol><li>异步更新技术AJAX(Asynchronous Javascript And XML，异步JavaScript和XML)</li><li>动态网页抓取的两种技术：通过浏览器审查元素解析真实网页地址和使用selenium模拟浏览器</li><li>Python中使用多线程的两种方法：① 函数式：调用_thread模块中的start_new_thread()函数产生新线程。 ② 调用Threading库创建线程，从threading.Thread继承。</li></ol><h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p>整个python爬虫学习过程大概花了十多天，只爬过7-8个网站，但也遇到了很多情况，有了一定基础，正如有前辈说过的：如果你爬过80%的主流网站，下次遇到需求你就都能很快的响应。想起《曾国藩家书》的箴言：为学譬如熬肉，先须用猛火煮，然后用慢火温。到此，Python爬虫学习告一段落。 ps: 希望传上之后，这篇博客排版还能看吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;整理下Python涉及的基本知识，巩固和方便查阅。&lt;/p&gt;
&lt;h5 id=&quot;Python爬虫的流程&quot;&gt;&lt;a href=&quot;#Python爬虫的流程&quot; class=&quot;headerlink&quot; title=&quot;Python爬虫的流程&quot;&gt;&lt;/a&gt;Python爬虫的流程&lt;/h5&gt;&lt;ol&gt;
      
    
    </summary>
    
      <category term="总结" scheme="http://minhzou.top/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="python" scheme="http://minhzou.top/tags/python/"/>
    
      <category term="爬虫" scheme="http://minhzou.top/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>卡特兰大数（Catalan大数）</title>
    <link href="http://minhzou.top/2019/05/20/%E5%8D%A1%E7%89%B9%E5%85%B0%E5%A4%A7%E6%95%B0%EF%BC%88Catalan%E5%A4%A7%E6%95%B0%EF%BC%89/"/>
    <id>http://minhzou.top/2019/05/20/卡特兰大数（Catalan大数）/</id>
    <published>2019-05-20T13:16:51.749Z</published>
    <updated>2019-05-20T13:19:20.298Z</updated>
    
    <content type="html"><![CDATA[<p>卡特兰数（Catalan数）因为有很多应用，很重要也很常见，之前做某机试题就是以Catalan大数为背景，当然用java可以方便求解，但用C/C++公式求解对于大数很容易超过long long 型，所以要运用大整数技巧。</p><h5 id="Catalan数的典型应用："><a href="#Catalan数的典型应用：" class="headerlink" title="Catalan数的典型应用："></a>Catalan数的典型应用：</h5><ol><li><p>一个有n个结点的二叉树总共有多少种形态？n个非叶节点的满二叉树的形态数?</p></li><li><p>在一个圆上，有2*K个不同的结点，我们以这些点为端点，连K条线段，使得每个结点都恰好用一次。在满足这些线段将圆分成最少部分的前提下，请计算有多少种连线的方法（类似：将多边行划分为三角形问题。将一个凸多边形区域分成三角形区域(划分线不交叉)的方法数?）</p></li><li><p>出栈次序问题。一个栈(无穷大)的进栈序列为1,2,3,..n,有多少个不同的出栈序列?</p></li><li><p>n*n的方格地图中，从一个角到另外一个角，不跨越对角线的路径数？</p></li><li><p>括号化问题。一个合法的表达式由()包围，()可以嵌套和连接，如(())()也是合法 表达式，它们可以组成的合法表达式的个数为?<br>…… 等等很多应用</p></li></ol><h5 id="Catalan数的公式"><a href="#Catalan数的公式" class="headerlink" title="Catalan数的公式"></a>Catalan数的公式</h5><p>递推关系：h(n)= h(0)<em>h(n-1)+h(1)</em>h(n-2) + … + h(n-1)h(0) (n&gt;=2)<br>递推关系的解：h(n)=C(2n,n)/(n+1) (n=0,1,2,…)<br>另类递推式：h(n)=h(n-1)<em>(4</em>n-2)/(n+1)<br>递推关系的另类解：h(n)=c(2n,n)-c(2n,n-1)(n=0,1,2,…)</p><h5 id="java版Catalan大数"><a href="#java版Catalan大数" class="headerlink" title="java版Catalan大数"></a>java版Catalan大数</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.BufferedInputStream;</span><br><span class="line"><span class="keyword">import</span> java.math.BigInteger;</span><br><span class="line"><span class="keyword">import</span> java.util.*;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> maxn = <span class="number">5000</span>;</span><br><span class="line"><span class="keyword">static</span> BigInteger[] C = <span class="keyword">new</span> BigInteger[maxn];</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">Scanner input = <span class="keyword">new</span> Scanner(<span class="keyword">new</span> BufferedInputStream(System.in));</span><br><span class="line"><span class="keyword">int</span> n, k;</span><br><span class="line"><span class="keyword">while</span>(input.hasNextInt()) &#123;</span><br><span class="line">n = input.nextInt();</span><br><span class="line">C[<span class="number">0</span>] = BigInteger.valueOf(<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span>(k = <span class="number">1</span>; k &lt;= n; k++) &#123;</span><br><span class="line">C[k] = (C[k-<span class="number">1</span>].multiply(BigInteger.valueOf(<span class="number">4</span>*k - <span class="number">2</span>))).divide(BigInteger.valueOf(k+<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(C[n]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="C语言版Catalan大数"><a href="#C语言版Catalan大数" class="headerlink" title="C语言版Catalan大数"></a>C语言版Catalan大数</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//大卡特兰数</span></span><br><span class="line"><span class="comment">// 递推公式 h(n) = ((4*n -2) / (n+1)) *h(n-1)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ll long long</span></span><br><span class="line"></span><br><span class="line">ll a[<span class="number">100000000</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n) != EOF)&#123;</span><br><span class="line">a[<span class="number">0</span>] = <span class="number">1</span>; <span class="comment">//h(0) = 1;</span></span><br><span class="line"><span class="keyword">int</span> d = <span class="number">1</span>; <span class="comment">// 位数</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line"><span class="comment">//处理乘法</span></span><br><span class="line">ll num = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; d; j++)&#123; </span><br><span class="line">num = a[j]*(<span class="number">4</span>*i <span class="number">-2</span>) + num;</span><br><span class="line">a[j] = num % <span class="number">10</span>;</span><br><span class="line">num = num / <span class="number">10</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">while</span>(num &gt; <span class="number">0</span>)&#123; <span class="comment">//最后计算完,处理进位</span></span><br><span class="line">a[d] = num % <span class="number">10</span>;</span><br><span class="line">num = num / <span class="number">10</span>;</span><br><span class="line">d++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//处理除法</span></span><br><span class="line">num = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = d - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--)&#123; </span><br><span class="line">num = a[j] + num*<span class="number">10</span>; <span class="comment">//</span></span><br><span class="line"><span class="keyword">if</span>( num &lt; (i + <span class="number">1</span>)) a[j] = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">a[j] = num / (i + <span class="number">1</span>);</span><br><span class="line">num = num % (i + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">while</span>(a[d - <span class="number">1</span>] == <span class="number">0</span> &amp;&amp; d  &gt;= <span class="number">2</span>)&#123; <span class="comment">//最后计算完，去除前面的0</span></span><br><span class="line">d--;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = d - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)&#123; <span class="comment">//倒序输出</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%lld"</span>, a[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;卡特兰数（Catalan数）因为有很多应用，很重要也很常见，之前做某机试题就是以Catalan大数为背景，当然用java可以方便求解，但用C/C++公式求解对于大数很容易超过long long 型，所以要运用大整数技巧。&lt;/p&gt;
&lt;h5 id=&quot;Catalan数的典型应用：
      
    
    </summary>
    
      <category term="总结" scheme="http://minhzou.top/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="C/C++" scheme="http://minhzou.top/tags/C-C/"/>
    
      <category term="算法" scheme="http://minhzou.top/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫实战——爬取特定微信公众号文章</title>
    <link href="http://minhzou.top/2019/05/18/Python%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E7%88%AC%E5%8F%96%E7%89%B9%E5%AE%9A%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0/"/>
    <id>http://minhzou.top/2019/05/18/Python爬虫实战——爬取特定微信公众号文章/</id>
    <published>2019-05-18T12:19:19.814Z</published>
    <updated>2019-05-18T15:42:12.793Z</updated>
    
    <content type="html"><![CDATA[<h5 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h5><p>学机器学习，学着学着就学偏了，就学着写爬虫了，目的主要是一熟悉下python，二解决自己某需求，这篇文章主要是解决需求写的代码中的主要部分——针对某一微信公众号的爬虫，当然除了学会了简单的爬虫，也运用了之前学习的很多知识，学有所用真的是一件非常开心的事情。</p><h5 id="本篇目标"><a href="#本篇目标" class="headerlink" title="本篇目标"></a>本篇目标</h5><blockquote><ol><li>根据微信公众号在Sogou微信搜索，提取微信公众号链接 </li><li>进入微信公众号界面，获取最近十篇微信文章链接</li><li>根据微信文章链接，解析网页，爬取所需文章内容</li></ol></blockquote><p>因为微信的反爬虫措施很多，每篇文章以及微信公众号的链接都是临时链接（从链接中包含timestamp字段可知）所以要另想办法，其中一种方法就是从Sogou微信搜索界面开始进行。</p><h5 id="1-确定URL"><a href="#1-确定URL" class="headerlink" title="1. 确定URL"></a>1. 确定URL</h5><p>直接在Sogou微信界面输入要搜索的微信公众号的具体名称，展示的第一条应该就是需要寻找的微信公众号，选取此时的url，例如：（已经输入搜索关键词）<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://weixin.sogou.com/weixin?type=1&amp;s_from=input&amp;query=%E5%B8%96%E6%9C%A8%E8%AE%B0&amp;ie=utf8&amp;_sug_=n&amp;_sug_type_=</span><br></pre></td></tr></table></figure></p><h5 id="2-提取微信公众号的链接"><a href="#2-提取微信公众号的链接" class="headerlink" title="2. 提取微信公众号的链接"></a>2. 提取微信公众号的链接</h5><p>搜狗微信搜索出来的文章链接均为微信的临时链接，通过客户端查看的文章链接均为永久链接，所以只能从搜狗微信搜索界面进入。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网页有反爬虫机制，这里需要换上浏览器的headers</span></span><br><span class="line">headers = &#123; </span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">begin_url = <span class="string">'https://weixin.sogou.com/weixin?type=1&amp;s_from=input&amp;query=%E5%B8%96%E6%9C%A8%E8%AE%B0&amp;ie=utf8&amp;_sug_=n&amp;_sug_type_='</span></span><br><span class="line">chrome_driver = <span class="string">'E:\chromedriver_win32\chromedriver.exe'</span><span class="comment"># 需要下载浏览器合适版本的driver</span></span><br><span class="line">browser = webdriver.Chrome(executable_path=chrome_driver)</span><br><span class="line">browser.get(begin_url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过selenium用chrome模拟点击链接</span></span><br><span class="line">element = browser.find_element_by_link_text(<span class="string">"帖木记"</span>)</span><br><span class="line">element.click()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前窗口的url，即微信公众号的链接</span></span><br><span class="line">browser.switch_to.window(browser.window_handles[<span class="number">1</span>])</span><br><span class="line">now_url = browser.current_url</span><br></pre></td></tr></table></figure></p><h5 id="3-获取微信文章的链接"><a href="#3-获取微信文章的链接" class="headerlink" title="3. 获取微信文章的链接"></a>3. 获取微信文章的链接</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取文章id, 比如最近一天发的文章id为WXAPPMSG1000000013</span></span><br><span class="line"><span class="comment"># 这里网页动态加载需要用selenium获取链接信息</span></span><br><span class="line"><span class="comment"># 需要获取十篇只需要加个for循环对id_inc递减即可</span></span><br><span class="line">id_inc = <span class="number">0</span></span><br><span class="line">start_id = <span class="number">1000000013</span> + id_inc</span><br><span class="line">true_id = <span class="string">"WXAPPMSG"</span> + str(start_id)</span><br><span class="line">element = browser.find_element_by_id(true_id)</span><br><span class="line">content = element.get_attribute(<span class="string">'innerHTML'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 无法直接获取链接，通过字符串切片获取文章的临时链接</span></span><br><span class="line"><span class="comment"># 这种切片方式只获取了每天的第一条图文消息的链接</span></span><br><span class="line">str1 = <span class="string">'hrefs="'</span></span><br><span class="line">right_url = content.partition(str1)[<span class="number">2</span>]</span><br><span class="line">str2 = <span class="string">'"&gt;&lt;/span&gt;'</span></span><br><span class="line">right_url = right_url.partition(str2)[<span class="number">0</span>]</span><br><span class="line">right_url = right_url.replace(<span class="string">'amp;'</span>, <span class="string">''</span>) <span class="comment"># 去除多余字符</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前后合并形成一个完整的url</span></span><br><span class="line">left_url = <span class="string">'https://mp.weixin.qq.com'</span></span><br><span class="line">url = left_url + right_url</span><br><span class="line">print(url)</span><br></pre></td></tr></table></figure><h5 id="4-爬取微信文章的内容"><a href="#4-爬取微信文章的内容" class="headerlink" title="4. 爬取微信文章的内容"></a>4. 爬取微信文章的内容</h5><p>临时链接直接在浏览器中浏览不显示阅读数以及点赞数，所以只能获取文章的内容，数据需要想其他办法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用requests获取网页</span></span><br><span class="line">req = requests.get(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用BeautifulSoup解析网页</span></span><br><span class="line">soup = BeautifulSoup(req.text, <span class="string">"html.parser"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取文章标题</span></span><br><span class="line">title = soup.h2.string.strip()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取文章内容</span></span><br><span class="line">contents = soup.find_all(<span class="string">'p'</span>) <span class="comment"># 等价于soup('p')</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.txt'</span>, <span class="string">"w+"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(title)</span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">        fp.write(content.text + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><h5 id="5-小结"><a href="#5-小结" class="headerlink" title="5. 小结"></a>5. 小结</h5><p>显然上面的代码显然可以优化的，主要是通过爬取微信文章用到了很多python库及技巧，比如：requests库，bs4库，selenium库和re正则表达式，字符串的处理，也包括验证码的识别，因为pytesseract识别率太低，自己又懒得去训练，也没必要花费选择现成的打码平台，所以就选择绕过验证码识别。当然最开心的是自己的小需求目前可以正常运行。</p><h5 id="6-完整代码"><a href="#6-完整代码" class="headerlink" title="6. 完整代码"></a>6. 完整代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网页有反爬虫机制，这里需要换上浏览器的headers</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">begin_url = <span class="string">'https://weixin.sogou.com/weixin?type=1&amp;s_from=input&amp;query=%E5%B8%96%E6%9C%A8%E8%AE%B0&amp;ie=utf8&amp;_sug_=n&amp;_sug_type_='</span></span><br><span class="line">chrome_driver = <span class="string">'E:\chromedriver_win32\chromedriver.exe'</span> <span class="comment"># 需要下载浏览器合适版本的driver</span></span><br><span class="line">browser = webdriver.Chrome(executable_path=chrome_driver)</span><br><span class="line">browser.get(begin_url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用selenium用chrome模拟点击链接</span></span><br><span class="line">element = browser.find_element_by_link_text(<span class="string">"帖木记"</span>)</span><br><span class="line">element.click()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前窗口的url，即微信公众号的临时链接</span></span><br><span class="line">browser.switch_to.window(browser.window_handles[<span class="number">1</span>])</span><br><span class="line">now_url = browser.current_url</span><br><span class="line"></span><br><span class="line">sleep(<span class="number">1</span>) <span class="comment"># 停一秒</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取文章id, 比如最近一天发的文章id为WXAPPMSG1000000013</span></span><br><span class="line"><span class="comment"># 这里网页动态加载需要用selenium获取链接信息</span></span><br><span class="line">id_inc = <span class="number">0</span></span><br><span class="line">start_id = <span class="number">1000000013</span> + id_inc</span><br><span class="line">true_id = <span class="string">"WXAPPMSG"</span> + str(start_id)</span><br><span class="line">element = browser.find_element_by_id(true_id)</span><br><span class="line">content = element.get_attribute(<span class="string">'innerHTML'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 无法直接获取链接，通过字符串切片获取文章的临时链接</span></span><br><span class="line"><span class="comment"># 这种切片方式只获取了每天的第一条图文消息的链接</span></span><br><span class="line">str1 = <span class="string">'hrefs="'</span></span><br><span class="line">right_url = content.partition(str1)[<span class="number">2</span>]</span><br><span class="line">str2 = <span class="string">'"&gt;&lt;/span&gt;'</span></span><br><span class="line">right_url = right_url.partition(str2)[<span class="number">0</span>]</span><br><span class="line">right_url = right_url.replace(<span class="string">'amp;'</span>, <span class="string">''</span>) <span class="comment"># 去除多余字符</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前后合并形成一个完整的url</span></span><br><span class="line">left_url = <span class="string">'https://mp.weixin.qq.com'</span></span><br><span class="line">url = left_url + right_url</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用requests获取网页</span></span><br><span class="line">req = requests.get(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用BeautifulSoup解析网页</span></span><br><span class="line">soup = BeautifulSoup(req.text, <span class="string">"html.parser"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文章标题获取</span></span><br><span class="line">title = soup.h2.string.strip()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文章内容获取</span></span><br><span class="line">contents = soup.find_all(<span class="string">'p'</span>) <span class="comment"># 等价于soup('p')</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.txt'</span>, <span class="string">"w+"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(title)</span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">        fp.write(content.text + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h5&gt;&lt;p&gt;学机器学习，学着学着就学偏了，就学着写爬虫了，目的主要是一熟悉下python，二解决自己某需求，这篇文章主要是解决需求写的代码中的主要部分—
      
    
    </summary>
    
      <category term="实战" scheme="http://minhzou.top/categories/%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="python" scheme="http://minhzou.top/tags/python/"/>
    
      <category term="爬虫实战" scheme="http://minhzou.top/tags/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
  <entry>
    <title>向量，矩阵求导</title>
    <link href="http://minhzou.top/2019/05/16/%E5%90%91%E9%87%8F%EF%BC%8C%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"/>
    <id>http://minhzou.top/2019/05/16/向量，矩阵求导/</id>
    <published>2019-05-16T12:33:48.100Z</published>
    <updated>2019-05-16T13:16:06.334Z</updated>
    
    <content type="html"><![CDATA[<p>向量求导有两种布局方式，分子布局和分母布局。分子布局和分母布局的操作结果可以通过转置来切换。<br>分母布局：标量对向量求导，得到的结果跟分母上的向量保持一致。即，如果标量是对列向量求导，得到的导数也是列向量。<br>分子布局：标量对向量求导，得到的结果是分母上的向量的转置。即，如果标量对列向量求导，得到的导数将是行向量。<br>以下以分母布局为例：</p><script type="math/tex; mode=display">A=\left[ \begin{array}{cccc}{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\ {a_{21}} & {a_{22}} & {\cdots} & {a_{2 n}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots}\\ {a_{m 1}} & {a_{m 2}} & {\cdots} & {a_{m n}}\end{array}\right]，x=\left[ \begin{array}{c}{x_{1}} \\ {x_{2}} \\ {\vdots} \\ {x_{n}}\end{array}\right]</script><script type="math/tex; mode=display">A x=\left[ \begin{array}{c}{a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}} \\ {a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}} \\ {\vdots} \\ {a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}}\end{array}\right]_{m×1}</script><script type="math/tex; mode=display">\frac{\partial A x}{\partial x}=\left[ \begin{array}{cccc}{a_{11}} & {a_{21}} & {\ldots} & {a_{m 1}} \\ {a_{12}} & {a_{22}} & {\ldots} & {a_{m 2}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {a_{1 n}} & {a_{2 n}} & {\ldots} & {a_{m n}}\end{array}\right]=A^{T}</script><p>以分母作为主序，x是列向量，Ax关于x求导数，那么对Ax的分量(每行)分别对x的分量求偏导数，然后整理排成一列，对每个分量都如此操作。</p><p>可以得到如下结论：</p><p><1></1></p><script type="math/tex; mode=display">\frac{\partial A x}{x}=A^{T}</script><p><2> </2></p><script type="math/tex; mode=display">\frac{\partial A x}{x^{T}}=A</script><p><3></3></p><script type="math/tex; mode=display">\frac{\partial x^{T} A x}{x}=\left(A^{T}+A\right) x</script><p><4></4></p><script type="math/tex; mode=display">\frac{\partial x^{T} y}{x}=y</script><p>元素对向量，矩阵求导：</p><ol><li>元素对行向量求导<br>设y是元素，$\mathbf{x}^{T}=\left[ \begin{array}{lll}{x_{1}} &amp; {\cdots} &amp; {x_{q}}\end{array}\right]$是q维行向量，则$\frac{\partial y}{\partial \mathbf{x}^{r}}=\left[\frac{\partial y}{\partial x_{1}} \ldots \frac{\partial y}{\partial x_{q}}\right]$</li><li>元素对列向量求导<br>设y是元素，$\mathbf{x}=\left[ \begin{array}{c}{x_{1}} \\ {\vdots} \\ {x_{p}}\end{array}\right]$是p维列向量，则$\frac{\partial y}{\partial \mathbf{x}}=\left[ \begin{array}{c}{\frac{\partial y}{\partial x_{1}}} \\ {\vdots} \\ {\frac{\dot{\partial} y}{\partial x_{p}}}\end{array}\right]$</li><li>元素对矩阵求导<br>设y是元素，$X=\left[ \begin{array}{ccc}{x_{11}} &amp; {\cdots} &amp; {x_{1 q}} \\ {\vdots} &amp;  &amp; {\vdots} \\ {x_{p 1}} &amp; {\cdots} &amp; {y_{p q}}\end{array}\right]$是p×q矩阵，则$\frac{\partial y}{\partial X}=\left[ \begin{array}{ccc}{\frac{\partial y}{\partial x_{11}}} &amp; {\cdots} &amp; {\frac{\partial y}{\partial x_{1 q}}} \\ {\vdots} \\ {\frac{\partial y}{\partial x_{p 1}}} &amp; {\cdots} &amp; {\frac{\partial y}{\partial x_{p q}}}\end{array}\right]$<br>```</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;向量求导有两种布局方式，分子布局和分母布局。分子布局和分母布局的操作结果可以通过转置来切换。&lt;br&gt;分母布局：标量对向量求导，得到的结果跟分母上的向量保持一致。即，如果标量是对列向量求导，得到的导数也是列向量。&lt;br&gt;分子布局：标量对向量求导，得到的结果是分母上的向量的转置
      
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://minhzou.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="基础" scheme="http://minhzou.top/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>C/C++梳理补缺</title>
    <link href="http://minhzou.top/2019/04/28/C%20and%20C++%E6%A2%B3%E7%90%86%E8%A1%A5%E7%BC%BA/"/>
    <id>http://minhzou.top/2019/04/28/C and C++梳理补缺/</id>
    <published>2019-04-28T09:33:55.566Z</published>
    <updated>2019-05-14T15:36:11.123Z</updated>
    
    <content type="html"><![CDATA[<h5 id="1-标准名字空间-amp-C-标准输入输出流"><a href="#1-标准名字空间-amp-C-标准输入输出流" class="headerlink" title="1. 标准名字空间 &amp; C++标准输入输出流"></a>1. 标准名字空间 &amp; C++标准输入输出流</h5><ul><li><p>标准名字空间</p><ul><li>cout是标准名字空间std的一个名字。必须加上<strong>名字空间限定</strong> std::<br>  std::cout</li><li>例如不同班级都可能有同名的学生，如“张伟”<br>计科::张伟    机械::张伟</li><li>也可以用：using std::cout</li><li>using namespace std</li></ul></li><li><p>C++标准输入输出流</p><ul><li>cout 是一个标准输出流变量（对象），代表控制台窗口</li><li>&lt;&lt;是一个运算符，假如o是一个输出流对象，x是一个数据， o&lt;&lt; x  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt; //文件输入输出 </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="function">ofstream <span class="title">oF</span><span class="params">(<span class="string">"1.txt"</span>)</span></span>;</span><br><span class="line">oF &lt;&lt; <span class="number">3.14</span> &lt;&lt;<span class="string">" "</span>&lt;&lt; <span class="string">"hello world\n"</span>;</span><br><span class="line">oF.close();</span><br><span class="line"><span class="function">ifstream <span class="title">iF</span><span class="params">(<span class="string">"1.txt"</span>)</span></span>;</span><br><span class="line"><span class="keyword">double</span> d;</span><br><span class="line"><span class="built_in">string</span> str;</span><br><span class="line">iF &gt;&gt; d &gt;&gt; str;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;d&lt;&lt;<span class="string">" "</span>&lt;&lt; str&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h5 id="2-引用变量和引用参数"><a href="#2-引用变量和引用参数" class="headerlink" title="2. 引用变量和引用参数"></a>2. 引用变量和引用参数</h5><ul><li><p>引用变量</p><ul><li>引用变量是其他变量的别名。如同一个人的外号或小名</li><li>既然是引用，定义引用变量时就必须指明其引用的是哪个变量<br>int a = 3;<br>int &amp;r = a;</li><li>引用变量“从一而终”， 一旦定义就不能再引用其他变量<br>int &amp;r=a;<br>int &amp;r = b; //错</li><li>引用变量和被引用的变量类型必须匹配<br>double d;<br>int &amp;r = d;</li><li>对引用变量的操作就是对它引用的变量的操作  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> a = <span class="number">3</span>, &amp;r = a;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="string">'\t'</span> &lt;&lt; r&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">r = <span class="number">5</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="string">'\t'</span> &lt;&lt; r &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>函数的值形参</p><ul><li><p>C函数的形参都是值参数，形参作为函数的局部变量有自己单独的内存块，当函数调用时，实参将值拷贝（赋值给）形参。对形参的修改不会影响实参</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="string">'\t'</span> &lt;&lt; y&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">int</span> t = x;</span><br><span class="line">x = y;</span><br><span class="line">y = t;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="string">'\t'</span> &lt;&lt; y&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> a = <span class="number">3</span>, b = <span class="number">4</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="string">'\t'</span> &lt;&lt; b&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">swap(a,b);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="string">'\t'</span> &lt;&lt; b &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>函数的值形参：传递指针</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> *x, <span class="keyword">int</span> *y)</span></span>&#123; <span class="comment">//指针 x, y </span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="string">'\t'</span> &lt;&lt; y&lt;&lt; <span class="built_in">endl</span>; <span class="comment">//输出的是a，b的地址 </span></span><br><span class="line"><span class="keyword">int</span> t = *x; <span class="comment">// 取x指针指向的值赋值给t </span></span><br><span class="line">*x = *y; <span class="comment">// 同理 </span></span><br><span class="line">*y = t; <span class="comment">// 同理 </span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> a = <span class="number">3</span>, b = <span class="number">4</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="string">'\t'</span> &lt;&lt; b&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">swap(&amp;a,&amp;b); <span class="comment">// &amp;取地址 a, b地址 传给x，y </span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="string">'\t'</span> &lt;&lt; b &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>函数的引用形参：引用实参</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> &amp;x, <span class="keyword">int</span> &amp;y)</span></span>&#123;  <span class="comment">// &amp;引用变量 </span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="string">'\t'</span> &lt;&lt; y&lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line"><span class="keyword">int</span> t = x; </span><br><span class="line">x = y; </span><br><span class="line">y = t; </span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> a = <span class="number">3</span>, b = <span class="number">4</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="string">'\t'</span> &lt;&lt; b&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">swap(a,b); </span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="string">'\t'</span> &lt;&lt; b &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h5 id="3-函数的默认形参、函数重载"><a href="#3-函数的默认形参、函数重载" class="headerlink" title="3. 函数的默认形参、函数重载"></a>3. 函数的默认形参、函数重载</h5><ul><li>函数的形参可以有默认值<br>void print(char ch, int n = 1)</li><li><p>默认形参必须在非默认形参右边，即一律靠右<br>add(x = 1, y, z = 3); // 错<br>add(y, x = 1, z = 3); // OK</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">(<span class="keyword">char</span> ch, <span class="keyword">int</span> n = <span class="number">1</span>)</span> </span>&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; ch;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">print(<span class="string">'*'</span>); <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">print(<span class="string">'*'</span>,<span class="number">3</span>); <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">print(<span class="string">'*'</span>,<span class="number">5</span>); <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y=<span class="number">2</span>,<span class="keyword">int</span> z=<span class="number">3</span>)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> x + y + z;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add(<span class="number">5</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add(<span class="number">5</span>,<span class="number">7</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add(<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>函数重载</p><ul><li>C++允许同一作用域里有同名的函数，只要他们的形参不同。如：<br>int add(int x, int y);<br>double add(double x, double y);</li><li>函数名和形参列表构造了<strong>函数的签名</strong></li><li><p>函数重载不能根据返回类型区分函数。如<br>int add(int x, int y);<br>double add(int x, int y);</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y = <span class="number">2</span>)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> x + y ;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">add</span><span class="params">(<span class="keyword">double</span> x, <span class="keyword">double</span> y = <span class="number">2.0</span>)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> x + y;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add(<span class="number">5</span>,<span class="number">3</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add(<span class="number">5.3</span>, <span class="number">7.8</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add((<span class="keyword">double</span>)<span class="number">5</span>, <span class="number">7.8</span>) &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//歧义性加double强制转换 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h5 id="4-函数模板"><a href="#4-函数模板" class="headerlink" title="4. 函数模板"></a>4. 函数模板</h5><ul><li><p>函数模板</p><ul><li>通用算法：函数模板。也称为泛型算法</li><li><p>用template关键字增加一个模板头，将数据类型变成<strong>类型模板参数</strong></p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">T <span class="title">add</span><span class="params">(T x, T y)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> x + y;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>给模板参数传递实际的模板参数</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span> &lt;&lt; add&lt;<span class="keyword">int</span>&gt;(<span class="number">5</span>, <span class="number">3</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add&lt;<span class="keyword">double</span>&gt;(<span class="number">5.3</span>, <span class="number">7.8</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>函数模板实例化</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">T <span class="title">add</span><span class="params">(T x, T y)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> x + y;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> 1</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add&lt;<span class="keyword">int</span>&gt;(<span class="number">5</span>, <span class="number">3</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add&lt;<span class="keyword">double</span>&gt;(<span class="number">5.3</span>, <span class="number">7.8</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add&lt;<span class="keyword">int</span>&gt;(<span class="number">4</span>, <span class="number">6</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add&lt;<span class="built_in">string</span>&gt;(<span class="string">"hello"</span>, <span class="string">"world"</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add(<span class="number">5</span>, <span class="number">3</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; add(<span class="number">5.3</span>, <span class="number">7.8</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>模板参数自动推断</p></li></ul><h5 id="5-用户自定义类型string和vector"><a href="#5-用户自定义类型string和vector" class="headerlink" title="5. 用户自定义类型string和vector"></a>5. 用户自定义类型string和vector</h5><ul><li><p>string</p><ul><li>是一个用户定义类型，表示的是符串。<br>string s = “hello”, s2(“world”);</li><li>用成员访问运算符.访问string类的成员<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span> &lt;&lt; s.size() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">string</span> s3 = s.substr(<span class="number">1</span>, <span class="number">3</span>);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>内在的数组（静态数组）</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">cout</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> arr[] = &#123; <span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span> &#125;; <span class="comment">//大小固定，以后不能添加更多int值</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; arr[i] &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li><li><p>vector</p><ul><li>向量，类似于数组，但可以动态增长。头文件  &lt;vector&gt; </li><li>是一个类模板，实例化产生一个类，如vector&lt;int&gt; 产生一个数据元素是int的vector&lt;int&gt; 类（向量）。</li><li>同样，可以通过vector&lt;int&gt; 类对象去访问其他成员，如成员函数。</li><li>同样可以用运算符进行一些运算。</li></ul></li></ul><h5 id="6-指针和动态内存分配"><a href="#6-指针和动态内存分配" class="headerlink" title="6. 指针和动态内存分配"></a>6. 指针和动态内存分配</h5><ul><li><p>指针</p><ul><li>指针就是地址，变量的指针就是变量的地址。可以用 <strong>取值地址符&amp;</strong> 获得一个变量的地址。如： &amp;var</li><li>指针变量就是存储指针（地址）的变量。如：<br>T *p; //p是存储 “T类型变量的地址”的变量</li><li><p>通过<strong>取内容运算符</strong> *可以得到一个指针变量指向的变量。<br>*p就是p指向的那个变量</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">指针就是地址，变量的指针就是变量的地址</span></span><br><span class="line"><span class="comment">指针变量就是存储指针（地址）的变量</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> a=<span class="number">3</span>;</span><br><span class="line"><span class="keyword">int</span> *p = &amp;a; <span class="comment">//取地址运算符&amp;用于获得a的地址：&amp;a</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p &lt;&lt; <span class="string">'\t'</span> &lt;&lt; &amp;a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">//取内容运算符*用于获得指针指向的变量(内存块)</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; *p &lt;&lt; <span class="string">'\t'</span> &lt;&lt; a &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//*p就是a</span></span><br><span class="line">*p = <span class="number">5</span>; <span class="comment">//即a = 5;</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; *p &lt;&lt; <span class="string">'\t'</span> &lt;&lt; a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> 0</span></span><br><span class="line"><span class="keyword">int</span> *q = p; <span class="comment">//q和p值相同，都是a的地址(指针)</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; *p &lt;&lt; <span class="string">'\t'</span> &lt;&lt; *q &lt;&lt; <span class="string">'\t'</span> &lt;&lt; a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">char</span> *s = &amp;a; <span class="comment">//int *</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">用指针访问数组元素</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> arr[] = &#123; <span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span> &#125;;</span><br><span class="line"><span class="keyword">int</span> *p = arr;<span class="comment">//数组名就是数组第一个元素的地址，即arr等于&amp;(arr[0])</span></span><br><span class="line"><span class="comment">// p[i]就是*(p+i)</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; *(p + <span class="number">2</span>) &lt;&lt; <span class="string">'\t'</span> &lt;&lt; p[<span class="number">2</span>] &lt;&lt; <span class="string">'\t'</span> &lt;&lt; arr[<span class="number">2</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> *q = p + <span class="number">4</span>; p &lt; q; p++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; *p &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>动态内存分配</p></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;malloc.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">char</span> *s =  (<span class="keyword">char</span> *)<span class="built_in">malloc</span>(<span class="number">10</span>*<span class="keyword">sizeof</span>(s)); <span class="comment">// char *s 指针， （char *）指针类型， sizeof s用内存大小</span></span><br><span class="line">s = <span class="string">"hello"</span>; </span><br><span class="line"><span class="built_in">puts</span>(s); </span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">malloc free realloc</span></span><br><span class="line"><span class="comment">动态内存分配：new用于申请内存块、delete用于释放内存块</span></span><br><span class="line"><span class="comment">T *p = new T;</span></span><br><span class="line"><span class="comment">delete p;</span></span><br><span class="line"><span class="comment">T *q = new T[5];</span></span><br><span class="line"><span class="comment">delete[] q;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> 1 </span></span><br><span class="line"><span class="comment">// 堆存储区</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> *p = <span class="keyword">new</span> <span class="keyword">int</span>; <span class="comment">//malloc</span></span><br><span class="line">*p = <span class="number">3</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p &lt;&lt; <span class="string">'\t'</span> &lt;&lt; *p &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">delete</span> p; <span class="comment">//如果没有这行，内存泄漏</span></span><br><span class="line">p = <span class="keyword">new</span> <span class="keyword">int</span>;</span><br><span class="line">*p = <span class="number">5</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p &lt;&lt; <span class="string">'\t'</span> &lt;&lt; *p &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">delete</span> p;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> n = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">int</span> *p = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">p[i] = <span class="number">2</span> * i + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> *q = p + n; p &lt; q; p++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; *p &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line"><span class="keyword">char</span> *s = (<span class="keyword">char</span> *)p;</span><br><span class="line"><span class="keyword">char</span> ch = <span class="string">'A'</span>;</span><br><span class="line"><span class="keyword">int</span> n2 = n * <span class="keyword">sizeof</span>(<span class="keyword">int</span>) / <span class="keyword">sizeof</span>(<span class="keyword">char</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n2; i++)</span><br><span class="line">s[i] = ch + i;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">char</span> *r = s+n2; s &lt; r; s++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; *s;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line"><span class="keyword">delete</span>[] p;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><h5 id="7-类和对象"><a href="#7-类和对象" class="headerlink" title="7. 类和对象"></a>7. 类和对象</h5><ul><li>面向对象编程<ul><li>传统的过程式编程：变量（对象）就是一些存储数据的内存块，而过程（函数）对这些数据进行处理。</li><li>面向对象编程：程序是由不同种类的许多对象相互协作完成的。对象之间通过发送/接收消息来协作完成各种任务。由这些对象构成的程序也称为“对象式系统”。</li><li>面向对象设计<ul><li>人驾驶车：设计对象“人”和“车”</li><li>从具有共同特征的许多抽象出某种概念，如“人”，“车”</li><li>某些概念之间可能存在某种关系<ul><li>组合（包含）关系</li><li>继承和派生关系</li></ul></li></ul></li><li>不同思考方式：面向对象编程 vs 过程式编程<ul><li>过程式编程：用内在类型（概念）如int、double表示数据，用面向这些机器类型的概念去解决复杂问题，不易于思考问题</li><li>面向对象编程：用现实世界中的概念（人、车、地图）来思考问题。更自然、更易于理解、易于查错、易于组装（组件式开发） </li></ul></li><li>C++的面向对象特性：用户定义类型<ul><li>程序员定义自己的“用户定义类型” 如类类型，来表示各种应用问题中的各种概念</li><li>C++ 标准库已经提供了很多实用的“用户定义类型”， 是C++标准库的程序员实现的<ul><li>cout是一个ostream类的对象（变量），cin是一个istream的对象（变量）。可以向它们发送消息：<br>cout &lt;&lt; “hello world”;</li><li>string是一个表示字符串的类。向一个string对象发送一个size()消息，查询该对象包含的字符数目<br>string str = “hello world”; cout &lt;&lt; str.size();</li></ul></li><li>一个用户定义类型包括：<ul><li>有哪些属性？</li><li>有哪些操作（运算）？</li><li>不同属性或操作的访问权限？哪些是（类）外部可以访问？哪些是仅仅内部才能访问的？</li></ul></li><li>面向对象设计要考虑多个用户定义类型的关系<ul><li>不同类型的对象之间是继承还是包含关系？</li></ul></li><li>程序：哪些具体对象如何进行交互协作</li></ul></li></ul></li><li>类和对象<ul><li>用struct或class关键字定义一个类。定义的类就是一个数据类型。<br>struct student{<br>  string name;<br>  double score;<br>} ;</li><li>类类型的变量通常称为对象。如：<br>student stu;  对象就是类的一个实例</li><li>成员访问运算符<ul><li>访问类对象的成员<br>stu.name = “LiPing”;<br>stu.score = 78.5;</li></ul></li><li>对象数组<ul><li>和内在一样，可以定义类类型的数组。存储一组类对象。</li></ul></li><li>类类型的指针变量<ul><li>T是一个类类型，则T *就是T指针类型。如int *是int指针类型。</li><li>T *变量可以指向一个类对象。<br>student stu;<br>student *p = &stu;<br>student students[3];<br>p = students + 2;</li></ul></li><li>间接访问运算符-&gt;、取内容运算符*<br>student stu;<br>student <em>p = &stu;<br>(\</em>p).name = “LiPing”; //*p就是p指向的变量stu<br>p-&gt;score = 78; // p指向的类对象的成员score</li><li>指向可以指向动态分配的对象</li><li>类的成员函数</li><li>类体外定义成员函数</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">输入一组学生成绩(姓名和分数)，输出：平均成绩、最高分和最低分。</span></span><br><span class="line"><span class="comment">当然，也要能输出所有学生信息</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student</span>&#123;</span></span><br><span class="line"><span class="built_in">string</span> name;</span><br><span class="line"><span class="keyword">double</span> score;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">void</span> student::print() &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; name &lt;&lt; <span class="string">" "</span> &lt;&lt; score &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> 0 </span></span><br><span class="line">student stu;</span><br><span class="line">stu.name = <span class="string">"Li Ping"</span>;</span><br><span class="line">stu.score = <span class="number">78.5</span>;</span><br><span class="line">stu.print();</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="built_in">vector</span>&lt;student&gt; students;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">student stu;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入姓名 分数:\n"</span>;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; stu.name &gt;&gt; stu.score;</span><br><span class="line"><span class="keyword">if</span> (stu.score &lt; <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">students.push_back(stu);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; students.size(); i++)</span><br><span class="line">students[i].print();</span><br><span class="line"><span class="keyword">double</span> min = <span class="number">100</span>, max=<span class="number">0</span>, average = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; students.size(); i++) &#123;</span><br><span class="line"><span class="keyword">if</span> (students[i].score &lt; min) min = students[i].score;</span><br><span class="line"><span class="keyword">if</span> (students[i].score &gt; max) max = students[i].score;</span><br><span class="line">average += students[i].score;</span><br><span class="line">&#125;</span><br><span class="line">average /= students.size();</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"平均分、最高分、最低分："</span>&lt;&lt; average &lt;&lt; <span class="string">" "</span> &lt;&lt; max &lt;&lt; <span class="string">" "</span> &lt;&lt; min &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="8-this指针，-访问控制，构造函数"><a href="#8-this指针，-访问控制，构造函数" class="headerlink" title="8. this指针， 访问控制，构造函数"></a>8. this指针， 访问控制，构造函数</h5><ul><li><p>this指针</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">this指针: 成员函数实际上隐含一个this指针。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student</span> &#123;</span></span><br><span class="line"><span class="built_in">string</span> name;</span><br><span class="line"><span class="keyword">double</span> score;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="keyword">this</span>-&gt;name &lt;&lt; <span class="string">" "</span> &lt;&lt; <span class="keyword">this</span>-&gt;score &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">student stu;</span><br><span class="line">stu.name = <span class="string">"Li Ping"</span>;</span><br><span class="line">stu.score = <span class="number">78.5</span>;</span><br><span class="line">stu.print(); <span class="comment">// print(&amp;stu);</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>访问控制</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">struct和class区别：</span></span><br><span class="line"><span class="comment">struct里的成员默认是public(公开的)</span></span><br><span class="line"><span class="comment">class里的成员默认是private(私有的)</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">student</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>: <span class="comment">//接口</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="keyword">this</span>-&gt;name &lt;&lt; <span class="string">" "</span> &lt;&lt; <span class="keyword">this</span>-&gt;score &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">get_name</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> name; &#125;</span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">get_score</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> score; &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_name</span><span class="params">(<span class="built_in">string</span> n)</span> </span>&#123; name = n; &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_score</span><span class="params">(<span class="keyword">double</span> s)</span> </span>&#123; score = s; &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="built_in">string</span> name;</span><br><span class="line"><span class="keyword">double</span> score;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">student stu;</span><br><span class="line"></span><br><span class="line"><span class="comment">// stu.name = "Li Ping";</span></span><br><span class="line"><span class="comment">// stu.score = 78.5;</span></span><br><span class="line">stu.set_name(<span class="string">"Li Ping"</span>);</span><br><span class="line">stu.set_score(<span class="number">78.5</span>);</span><br><span class="line">stu.print(); <span class="comment">// print(&amp;stu);</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; stu.get_name() &lt;&lt; <span class="string">" "</span> &lt;&lt; stu.get_score() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>构造函数</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">构造函数： 函数名和类名相同且无返回类型的成员函数。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">student</span>&#123;</span></span><br><span class="line"><span class="built_in">string</span> name;</span><br><span class="line"><span class="keyword">double</span> score;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="comment">// student()&#123;&#125; // 默认构造函数 </span></span><br><span class="line">student()&#123;</span><br><span class="line">name = <span class="string">"NO"</span>;</span><br><span class="line">score = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"构造函数\n"</span>;</span><br><span class="line">&#125;</span><br><span class="line">student(<span class="built_in">string</span> n,<span class="keyword">double</span> s)&#123; <span class="comment">//不是默认构造函数</span></span><br><span class="line">name = n; score = s;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"构造函数\n"</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="keyword">this</span>-&gt;name &lt;&lt; <span class="string">" "</span> &lt;&lt; <span class="keyword">this</span>-&gt;score &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//student stu;//在创建一个类对象时会自动调用称为“构造函数”的成员函数</span></span><br><span class="line"><span class="function">student <span class="title">stu</span><span class="params">(<span class="string">"LiPing"</span>,<span class="number">80.5</span>)</span></span>; </span><br><span class="line">stu.print();</span><br><span class="line"><span class="comment">//student students[3]; 数组必须有默认构造函数 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h5 id="9-运算重载符"><a href="#9-运算重载符" class="headerlink" title="9. 运算重载符"></a>9. 运算重载符</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">运算符重载：针对用户定义类型重新定义运算符函数</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">student</span> &#123;</span></span><br><span class="line"><span class="built_in">string</span> name;</span><br><span class="line"><span class="keyword">double</span> score;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">student(<span class="built_in">string</span> n, <span class="keyword">double</span> s) &#123;</span><br><span class="line">name = n; score = s;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//友元函数</span></span><br><span class="line"><span class="keyword">friend</span> ostream&amp; <span class="keyword">operator</span>&lt;&lt;(ostream &amp;o, student s);</span><br><span class="line"><span class="keyword">friend</span> istream&amp; <span class="keyword">operator</span>&gt;&gt;(istream &amp;in, student &amp;s);</span><br><span class="line">&#125;;</span><br><span class="line">ostream&amp; <span class="keyword">operator</span>&lt;&lt;(ostream &amp;o, student s) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; s.name &lt;&lt; <span class="string">","</span> &lt;&lt; s.score &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br><span class="line">istream&amp; <span class="keyword">operator</span>&gt;&gt;(istream &amp;in, student &amp;s) &#123;</span><br><span class="line">in &gt;&gt; s.name &gt;&gt; s.score;</span><br><span class="line"><span class="keyword">return</span> in;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="function">student <span class="title">stu</span><span class="params">(<span class="string">"LiPing"</span>, <span class="number">80.5</span>)</span></span>;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; stu; <span class="comment">//operator&gt;&gt;(cin,stu)</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; stu; <span class="comment">//operator&lt;&lt;(cout,stu)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Point</span>&#123;</span></span><br><span class="line"><span class="keyword">double</span> x, y;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="keyword">double</span> <span class="keyword">operator</span>[](<span class="keyword">int</span> i) <span class="keyword">const</span>&#123; <span class="comment">//const函数</span></span><br><span class="line"><span class="keyword">if</span> (i == <span class="number">0</span>) <span class="keyword">return</span> x;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (i == <span class="number">1</span>) <span class="keyword">return</span> y;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">throw</span> <span class="string">"下标非法!"</span>; <span class="comment">//抛出异常</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">double</span>&amp; <span class="keyword">operator</span>[](<span class="keyword">int</span> i) &#123;</span><br><span class="line"><span class="keyword">if</span> (i == <span class="number">0</span>) <span class="keyword">return</span> x;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (i == <span class="number">1</span>) <span class="keyword">return</span> y;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">throw</span> <span class="string">"下标非法!"</span>; <span class="comment">//抛出异常</span></span><br><span class="line">&#125;</span><br><span class="line">Point(<span class="keyword">double</span> x_,<span class="keyword">double</span> y_) &#123;</span><br><span class="line">x = x_; y = y_;</span><br><span class="line">&#125;</span><br><span class="line">Point <span class="keyword">operator</span>+(<span class="keyword">const</span> Point q) &#123;</span><br><span class="line"><span class="keyword">return</span> Point(<span class="keyword">this</span>-&gt;x+q[<span class="number">0</span>],<span class="keyword">this</span>-&gt;y + q[<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//友元函数</span></span><br><span class="line"><span class="keyword">friend</span> ostream &amp; <span class="keyword">operator</span>&lt;&lt;(ostream &amp;o, Point p);</span><br><span class="line"><span class="keyword">friend</span> istream &amp; <span class="keyword">operator</span>&gt;&gt;(istream &amp;i, Point &amp;p);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">ostream &amp; <span class="keyword">operator</span>&lt;&lt;(ostream &amp;o, Point p) &#123;</span><br><span class="line">o &lt;&lt;p.x &lt;&lt; <span class="string">" "</span> &lt;&lt; p.y&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">istream &amp; <span class="keyword">operator</span>&gt;&gt;(istream &amp;i, Point &amp;p) &#123;</span><br><span class="line">i &gt;&gt; p.x &gt;&gt; p.y;</span><br><span class="line"><span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> 0</span></span><br><span class="line">Point <span class="keyword">operator</span>+(<span class="keyword">const</span> Point p,<span class="keyword">const</span> Point q) &#123;</span><br><span class="line"><span class="keyword">return</span> Point(p[<span class="number">0</span>] + q[<span class="number">0</span>], p[<span class="number">1</span>] + q[<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">Point p(3.5, 4.8),q(2.0,3.0);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> 0</span></span><br><span class="line"><span class="comment">// cin &gt;&gt; p;</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p[<span class="number">0</span>] &lt;&lt; <span class="string">"-"</span> &lt;&lt; p[<span class="number">1</span>] &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//p.operator[](0)</span></span><br><span class="line">p[<span class="number">0</span>] = <span class="number">3.45</span>; p[<span class="number">1</span>] = <span class="number">5.67</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p&lt;&lt;q;</span><br><span class="line">Point s = p + q; <span class="comment">//p.operator+(q) vs operator+(p,q)</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="10-String类、拷贝构造函数、析构函数"><a href="#10-String类、拷贝构造函数、析构函数" class="headerlink" title="10. String类、拷贝构造函数、析构函数"></a>10. String类、拷贝构造函数、析构函数</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">String</span> &#123;</span></span><br><span class="line"><span class="keyword">char</span> *data; <span class="comment">//C风格的字符串</span></span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">~String() &#123; <span class="comment">//析构函数，析构的顺序与创建的相反顺序 </span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt;n&lt;&lt; <span class="string">" 析构函数\n"</span>;</span><br><span class="line"><span class="keyword">if</span>(data)</span><br><span class="line"><span class="keyword">delete</span>[] data;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> 1</span></span><br><span class="line">String(<span class="keyword">const</span> String &amp;s) &#123; <span class="comment">//硬拷贝</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"拷贝构造函数\n"</span>;</span><br><span class="line">data = <span class="keyword">new</span> <span class="keyword">char</span>[s.n + <span class="number">1</span>];</span><br><span class="line">n = s.n;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">data[i] = s.data[i];</span><br><span class="line">data[n] = <span class="string">'\0'</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">String(<span class="keyword">const</span> <span class="keyword">char</span> *s=<span class="number">0</span>) &#123; <span class="comment">//构造函数 </span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"构造函数\n"</span>;</span><br><span class="line"><span class="keyword">if</span> (s == <span class="number">0</span>) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"s==0\n"</span>;</span><br><span class="line">data = <span class="number">0</span>; n = <span class="number">0</span>; <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span> *p = s;</span><br><span class="line"><span class="keyword">while</span> (*p) p++;</span><br><span class="line">n = p - s; <span class="comment">//String长 </span></span><br><span class="line">data = <span class="keyword">new</span> <span class="keyword">char</span>[n + <span class="number">1</span>]; <span class="comment">//分配内存空间 </span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)<span class="comment">//存储String </span></span><br><span class="line">data[i] = s[i];</span><br><span class="line">data[n] = <span class="string">'\0'</span>; </span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> n; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> <span class="keyword">operator</span>[](<span class="keyword">int</span> i)<span class="keyword">const</span> &#123; <span class="comment">//下标运算符，不可以修改 </span></span><br><span class="line"><span class="keyword">if</span> (i&lt;<span class="number">0</span> || i&gt;=n ) <span class="keyword">throw</span> <span class="string">"下标非法"</span>;</span><br><span class="line"><span class="keyword">return</span> data[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">char</span>&amp; <span class="keyword">operator</span>[](<span class="keyword">int</span> i) &#123; <span class="comment">//下标运算符，可以修改 </span></span><br><span class="line"><span class="keyword">if</span> (i &lt; <span class="number">0</span> || i &gt;= n) <span class="keyword">throw</span> <span class="string">"下标非法"</span>;</span><br><span class="line"><span class="keyword">return</span> data[i];</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">ostream &amp; <span class="keyword">operator</span>&lt;&lt;(ostream &amp;o, String s) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.size(); i++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; s[i];</span><br><span class="line"><span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String str,str2(<span class="string">"hello world"</span>);</span><br><span class="line">str2[<span class="number">1</span>] = <span class="string">'E'</span>;</span><br><span class="line"><span class="comment">// cout &lt;&lt; str2 &lt;&lt; endl;</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> 1</span></span><br><span class="line">String s3 = str2; <span class="comment">//拷贝构造函数</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; s3 &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">s3[<span class="number">3</span>] = <span class="string">'L'</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; s3 &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; str2 &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">f();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="11-类模板"><a href="#11-类模板" class="headerlink" title="11. 类模板"></a>11. 类模板</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*类</span></span><br><span class="line"><span class="comment">模拟vector&lt;int&gt;的类Vector</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">student</span> &#123;</span></span><br><span class="line"><span class="built_in">string</span> name;</span><br><span class="line"><span class="keyword">double</span> score;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">student(<span class="built_in">string</span> n=<span class="string">"no"</span>, <span class="keyword">double</span> s=<span class="number">0</span>) &#123; <span class="comment">//申请了空间必须要默认 </span></span><br><span class="line">name = n; score = s;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">friend</span> ostream&amp; <span class="keyword">operator</span>&lt;&lt;(ostream &amp;o, student s);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">ostream&amp; <span class="keyword">operator</span>&lt;&lt;(ostream &amp;o, student s) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; s.name &lt;&lt; <span class="string">","</span> &lt;&lt; s.score &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//类模板</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt; <span class="comment">//加这一句，改数据类型 </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vector</span> &#123;</span></span><br><span class="line">T *data;</span><br><span class="line"><span class="keyword">int</span> capacity;</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">Vector(<span class="keyword">int</span> cap=<span class="number">3</span>) &#123;</span><br><span class="line">data = <span class="keyword">new</span> T[cap];</span><br><span class="line"><span class="keyword">if</span> (data == <span class="number">0</span>) &#123;</span><br><span class="line">cap = <span class="number">0</span>; n = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line">capacity = cap;</span><br><span class="line">n = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">push_back</span><span class="params">(T e)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (n == capacity) &#123;<span class="comment">//空间已经满</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"增加容量！\n"</span>;</span><br><span class="line">T *p = <span class="keyword">new</span> T[<span class="number">2</span> * capacity];</span><br><span class="line"><span class="keyword">if</span> (p) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">p[i] = data[i];</span><br><span class="line"><span class="keyword">delete</span>[] data;</span><br><span class="line">data = p;</span><br><span class="line">capacity = <span class="number">2</span>*capacity;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">data[n] = e;</span><br><span class="line">n++;</span><br><span class="line">&#125;</span><br><span class="line">T <span class="keyword">operator</span>[](<span class="keyword">int</span> i) <span class="keyword">const</span>&#123;</span><br><span class="line"><span class="keyword">if</span> (i &lt; <span class="number">0</span> || i &gt;= n) <span class="keyword">throw</span> <span class="string">"下标非法!"</span>;</span><br><span class="line"><span class="keyword">return</span> data[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> n;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">Vector&lt;student&gt; v;</span><br><span class="line">v.push_back(student(<span class="string">"Li"</span>,<span class="number">45.7</span>));</span><br><span class="line">v.push_back(student(<span class="string">"Wang"</span>, <span class="number">45.7</span>));</span><br><span class="line">v.push_back(student(<span class="string">"zhao"</span>, <span class="number">45.7</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; v.size(); i++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; v[i] ;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">v.push_back(student(<span class="string">"zhang"</span>, <span class="number">45.7</span>));</span><br><span class="line">v.push_back(student(<span class="string">"Liu"</span>, <span class="number">45.7</span>));</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; v.size(); i++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; v[i];</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> 0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> 1</span></span><br><span class="line">Vector&lt;<span class="keyword">int</span>&gt; v;</span><br><span class="line">v.push_back(<span class="number">3</span>);</span><br><span class="line">v.push_back(<span class="number">4</span>);</span><br><span class="line">v.push_back(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i&lt;v.size();i++)</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;v[i]&lt;&lt;<span class="string">'\t'</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">v.push_back(<span class="number">6</span>);</span><br><span class="line">v.push_back(<span class="number">7</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; v.size(); i++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; v[i] &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">Vector&lt;<span class="built_in">string</span>&gt; v;</span><br><span class="line">v.push_back(<span class="string">"hello"</span>);</span><br><span class="line">v.push_back(<span class="string">"world"</span>);</span><br><span class="line">v.push_back(<span class="string">"sdfasdf"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; v.size(); i++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; v[i] &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">v.push_back(<span class="string">"ggg"</span>);</span><br><span class="line">v.push_back(<span class="string">"hhh"</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; v.size(); i++)</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; v[i] &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>学习来源：b站：从C到C++快速入门(2019版)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;1-标准名字空间-amp-C-标准输入输出流&quot;&gt;&lt;a href=&quot;#1-标准名字空间-amp-C-标准输入输出流&quot; class=&quot;headerlink&quot; title=&quot;1. 标准名字空间 &amp;amp; C++标准输入输出流&quot;&gt;&lt;/a&gt;1. 标准名字空间 &amp;amp; 
      
    
    </summary>
    
    
      <category term="C/C++" scheme="http://minhzou.top/tags/C-C/"/>
    
      <category term="学习笔记" scheme="http://minhzou.top/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习的数学基础</title>
    <link href="http://minhzou.top/2019/04/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
    <id>http://minhzou.top/2019/04/14/机器学习的数学基础/</id>
    <published>2019-04-14T09:15:42.691Z</published>
    <updated>2019-04-14T15:27:17.668Z</updated>
    
    <content type="html"><![CDATA[<p>预备了一下学习机器学习需要的数学基础知识，补充了一些不清楚的数学知识</p><h5 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h5><div class="table-container"><table><thead><tr><th>算法或理论</th><th>用到的数学知识</th></tr></thead><tbody><tr><td>贝叶斯分类器</td><td>随机变量，贝叶斯公式，随机变量独立性，正太分布，最大似然估计</td></tr><tr><td>决策树</td><td>概率，熵，Gini系数</td></tr><tr><td>KNN算法</td><td>距离函数</td></tr><tr><td>主成分分析</td><td>协方差矩阵，散布矩阵，拉格朗日乘数法，特征值与特征向量</td></tr><tr><td>流形学习</td><td>流形，最优化，测地线，测地距离，图，特征值与特征向量</td></tr><tr><td>线性判别分析</td><td>散度矩阵，逆矩阵，拉格朗日乘数法，特征值与特征向量</td></tr><tr><td>支持向量机</td><td>点到平面的距离，Slater条件，强对偶，拉格朗日对偶，KKT条件，凸优化，核函数，Mercer条件</td></tr><tr><td>]logistics回归</td><td>概率、随机变量，最大似然估计，梯度下降法，凸优化，牛顿法</td></tr><tr><td>随机森林</td><td>抽样，方差</td></tr><tr><td>AdaBoost算法</td><td>概率，随机变量，最大似然估计，梯度下降法，凸优化，牛顿法</td></tr><tr><td>隐马尔科夫链</td><td>概率，离散型随机变量，条件概率，随机变量独立性，拉格朗日乘数法，最大似然估计</td></tr><tr><td>条件随机场</td><td>条件概率，数学期望，最大似然估计</td></tr><tr><td>高斯混合模型</td><td>正态分布，最大似然估计，Jensen不等式</td></tr><tr><td>人工神经网络</td><td>梯度下降法，链式法则</td></tr><tr><td>卷积神经网络</td><td>梯度下降法，链式法则</td></tr><tr><td>循环神经网络</td><td>梯度下降法，链式法则</td></tr><tr><td>生成对抗网络</td><td>梯度下降法，链式法则，极值定理，Kullback-Leibler散度，Jensen-Shannon散度，测地距离，条件分布，互信息</td></tr><tr><td>K-means算法</td><td>距离函数</td></tr><tr><td>强化学习</td><td>数学期望，贝尔曼方程</td></tr><tr><td>贝叶斯网络</td><td>条件概率，贝叶斯公式，图</td></tr><tr><td>VC维</td><td>Hoeffding不等式</td></tr></tbody></table></div><h5 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h5><ul><li>导数与求导公式</li><li>一阶导数与函数的单调性</li><li>一元函数极值判定法则</li><li>高阶导数</li><li>二阶导数与函数的凹凸性</li><li>一元函数泰勒展开</li><li>偏导数</li><li>梯度<br>$\nabla f(\mathrm{x})=\left(\frac{\partial f}{\partial x_{1}}, \ldots, \frac{\partial f}{\partial x_{n}}\right)^{\mathrm{T}}$</li><li>雅可比矩阵<br>多元函数的一阶偏导数组成<br>$\left[ \begin{array}{cccc}{\frac{\partial y_{1}}{\partial x_{1}}} &amp; {\frac{\partial y_{1}}{\partial x_{2}}} &amp; {\dots} &amp; {\frac{\partial y_{1}}{\partial x_{n}}} \\ {\frac{\partial y_{2}}{\partial x_{1}}} &amp; {\frac{\partial y_{2}}{\partial x_{2}}} &amp; {\dots} &amp; {\frac{\partial y_{2}}{\partial x_{n}}} \\ {\cdots} &amp; {\cdots} &amp; {\cdots} &amp; {\cdots} \\ {\frac{\partial y_{m}}{\partial x_{1}}} &amp; {\frac{\partial y_{m}}{\partial x_{2}}} &amp; {\ldots} &amp; {\frac{\partial y_{m}}{\partial x_{n}}}\end{array}\right]$</li><li>Hessian矩阵<br>多元函数的二阶导数组成<br>$\left[ \begin{array}{cccc}{\frac{\partial^{2} f}{\partial x_{1}^{2}}} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} &amp; {\dots} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{2}^{2}}} &amp; {\dots} &amp; {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}}\\ {\cdots} &amp; {\cdots} &amp; {\cdots} &amp; {\cdots}\\ {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{n} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n}^{2}}}\end{array}\right]$</li><li><p>多元函数泰勒展开<br>$f(\mathrm{x})=f\left(\mathrm{x}_{0}\right)+\left(\nabla f\left(\mathrm{x}_{0}\right)\right)^{\mathrm{T}}\left(\mathrm{x}-\mathrm{x}_{0}\right)+\frac{1}{2}\left(\mathrm{x}-\mathrm{x}_{0}\right)^{\mathrm{T}} \mathrm{H}\left(\mathrm{x}-\mathrm{x}_{0}\right)+o\left(\left\Vert \mathrm{x}-\mathrm{x}_{0}\right\Vert^{2}\right)$</p></li><li><p>多元函数极值判断法则</p><ul><li>如果Hessian矩阵正定，函数在该点有极小值</li><li>如果Hessian矩阵负定，函数在该点有极大值</li><li>如果Hessian矩阵不定，还需要看更高阶的导数</li></ul></li></ul><h5 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h5><ul><li>向量及其运算<ul><li>向量的范数<br>$\Vert\mathrm{x}\Vert_{p}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}$<br>$\Vert\mathrm{x}\Vert_{1}=\sum_{i=1}^{n}\left|x_{i}\right|$<br>$\Vert\mathrm{x}\Vert_{2}=\sqrt{\sum_{i=1}^{n}\left(x_{i}\right)^{2}}$</li></ul></li><li>矩阵及其运算</li><li>张量</li><li>行列式<ul><li>$|\mathrm{A}|=\sum_{\sigma \in S_{n}} \operatorname{sgn}(\sigma) \prod_{i=1}^{n} a_{i, \sigma(i)}$</li><li>逆序数</li></ul></li><li>二次型</li><li>特征值与特征向量</li><li>奇异值分解（SVD）<br>$\mathrm{A}=\mathrm{U} \Sigma \mathrm{V}^{\mathrm{T}}$<br>U：$AA^{T}$ 正交矩阵 mxm<br>V：$A^{T}A$ 正交矩阵 nxn<br>$\Sigma$ 对角阵 mxn</li><li>常用的矩阵与向量求导公式<br>$\nabla \mathrm{w}^{\mathrm{T}} \mathrm{x}=\mathrm{w}$<br>$\nabla \mathrm{x}^{\mathrm{T}} \mathrm{Ax}=\left(\mathrm{A}+\mathrm{A}^{\mathrm{T}}\right) \mathrm{x}$<br>$\nabla^{2} \mathrm{x}^{\mathrm{T}} \mathrm{Ax}=\mathrm{A}+\mathrm{A}^{\mathrm{T}}$ （$\nabla^{2}$ 相当于H ）</li></ul><h5 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h5><ul><li>随机事件与概率</li><li>条件概率与贝叶斯公式</li><li>随机变量</li><li>数学期望与方差</li><li>常用概率分布</li><li>随机向量</li><li>协方差与协方差矩阵<br>协方差反应随机变量线性相关的程度<ul><li>$\operatorname{cov}\left(x_{1}, x_{2}\right)=\mathrm{E}\left(\left(x_{1}-\mathrm{E}\left(x_{1}\right)\right)\left(x_{2}-\mathrm{E}\left(x_{2}\right)\right)\right)$</li><li>$\operatorname{cov}\left(x_{1}, x_{2}\right)=\mathrm{E}\left(x_{1} x_{2}\right)-\mathrm{E}\left(x_{1}\right) \mathrm{E}\left(x_{2}\right)$</li></ul></li><li>最大似然估计（MLE）<br>用来估计概率密度的参数</li></ul><h5 id="最优化方法"><a href="#最优化方法" class="headerlink" title="最优化方法"></a>最优化方法</h5><ul><li>最优化的基本概念<ul><li>最优化问题</li><li>目标函数</li><li>优化变量</li><li>可行域</li><li>等式约束</li><li>不等式约束</li><li>局部最小值</li><li>全局最小值</li></ul></li><li>迭代法<ul><li>$\lim _{k \rightarrow+\infty} \nabla f\left(\mathrm{x}_{k}\right)=0$</li><li>$\mathrm{x}_{k+1}=h\left(\mathrm{x}_{k}\right)$</li></ul></li><li>梯度下降法<br>$\mathrm{x}_{k+1}=\mathrm{x}_{k}-\gamma \nabla f\left(\mathrm{x}_{k}\right)$</li><li>牛顿法<br>$\mathrm{x}_{k+1}=\mathrm{x}_{k}-\mathrm{H}_{k}^{-1} \mathrm{g}_{k}$</li><li>坐标下降法<br>$\min f(x), x=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$<br>$\min _{x_{i}} f(x)$</li><li>优化算法面临的问题<ul><li>局部极值点问题</li><li>鞍点问题（H不定）</li></ul></li><li>拉格朗日乘数法</li><li>凸优化简介<ul><li>凸集<br>$\theta \mathrm{x}+(1-\theta) \mathrm{y} \in C$</li><li>凸函数<ul><li>凸函数定义<br>$f(\theta \mathrm{x}+(1-\theta) \mathrm{y})&lt;\theta f(\mathrm{x})+(1-\theta) f(\mathrm{y})$</li><li>一阶判别法</li><li>二阶判别法（H半正定，正定严格凸函数）</li></ul></li></ul></li><li>凸优化的性质<ul><li>局部最优解一定是全局最优解<br>$\mathrm{z}=\theta \mathrm{y}+(1-\theta) \mathrm{x} \quad \theta=\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}}$<br>$\begin{aligned}\Vert\mathrm{x}-\mathrm{z}\Vert_{2} &amp;=\Vert \mathrm{x}-\left(\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}} \mathrm{y}+\left(1-\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}}\right) \mathrm{x}\left\Vert_{2}\right.\right.\\ &amp;=\left\Vert\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}}(\mathrm{x}-\mathrm{y})\right\Vert_{2} \\ &amp;=\frac{\delta}{2} \leq \delta \end{aligned}$<br>$f(z)=f(\theta y+(1-\theta) x) \leq \theta f(y)+(1-\theta) f(x)&lt;f(x)$</li></ul></li><li>拉格朗日对偶<br>  $\min f(\mathrm{x})$<br>$\mathrm{g}_{i}(\mathrm{x}) \leq 0 \quad \mathrm{i}=1, \ldots, m$<br>$h_{i}(\mathrm{x})=0 \quad \mathrm{i}=1, \ldots, p$<br>  $L(\mathrm{x}, \lambda, v)=f(\mathrm{x})+\sum_{i=1}^{m} \lambda_{1} g_{i}(\mathrm{x})+\sum_{i=1}^{p} v_{i} h_{i}(\mathrm{x})$<ul><li>原问题<br>$\begin{aligned} p^{ \ast } &amp;=\min _{x} \max _{\lambda, v, \lambda_{i} \geq 0} L(\mathrm{x}, \lambda, v) &amp; \min _{\mathrm{x}} \theta_{P}(\mathrm{x})=\min _{\mathrm{x}} \max _{\lambda, v, \lambda_{i} \geq 0} L(\mathrm{x}, \lambda, v) \\ &amp;=\min _{\mathrm{x}} \theta_{P}(x) \end{aligned}$</li><li>对偶问题<br>$d^{ \ast }=\max _{\lambda, v, \lambda_{i} \geq 0} \min _{x} L(\mathrm{x}, \lambda, v)=\max _{\lambda, v, \lambda_{i} \geq 0} \theta_{D}(\lambda, v)$</li><li>弱对偶问题<br>$d^{ \ast }=\max _{\lambda, v, \lambda_{i} \geq 0} \min _{x} L(\mathrm{x}, \lambda, v) \leq \min _{x} \max _{\lambda, v, \lambda_{i} \geq 0} L(\mathrm{x}, \lambda, v)=p^{ \ast }$</li><li>强对偶<br>Slater条件<ul><li>凸函数</li><li>不等式严格成立（不取等号）</li></ul></li></ul></li><li>KKT条件<br>$\min f(\mathrm{x})$<br>$g_{i}(\mathrm{x}) \leq 0 \quad \mathrm{i}=1, \ldots, q$<br>$h_{i}(\mathrm{x})=0 \quad \mathrm{i}=1, \ldots, p$</li></ul><p>$L(\mathrm{x}, \lambda, \mu)=f(\mathrm{x})+\sum_{j=1}^{p} \lambda_{i} h_{j}(\mathrm{x})+\sum_{k=1}^{q} \mu_{i} g_{k}(\mathrm{x})$</p><p>$\nabla_{\mathrm{x}} L\left(\mathrm{x}^{\ast}\right)=0$<br>$\mu_{k} \geq 0$<br>$\mu_{k} g_{k}\left(\mathrm{x}^{\ast}\right)=0$<br>$h_{j}\left(\mathrm{x}^{\ast}\right)=0$<br>$g_{k}\left(\mathrm{x}^{\ast}\right) \leq 0$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;预备了一下学习机器学习需要的数学基础知识，补充了一些不清楚的数学知识&lt;/p&gt;
&lt;h5 id=&quot;概览&quot;&gt;&lt;a href=&quot;#概览&quot; class=&quot;headerlink&quot; title=&quot;概览&quot;&gt;&lt;/a&gt;概览&lt;/h5&gt;&lt;div class=&quot;table-container&quot;&gt;
&lt;
      
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://minhzou.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="基础" scheme="http://minhzou.top/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="学习笔记" scheme="http://minhzou.top/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>第二周 Logistic回归、SVM</title>
    <link href="http://minhzou.top/2019/04/06/%E7%AC%AC%E4%BA%8C%E5%91%A8%20Logistic%E5%9B%9E%E5%BD%92%E3%80%81SVM/"/>
    <id>http://minhzou.top/2019/04/06/第二周 Logistic回归、SVM/</id>
    <published>2019-04-06T12:37:26.028Z</published>
    <updated>2019-04-13T09:12:47.667Z</updated>
    
    <content type="html"><![CDATA[<h5 id="1、Logistic回归基本原理"><a href="#1、Logistic回归基本原理" class="headerlink" title="1、Logistic回归基本原理"></a>1、Logistic回归基本原理</h5><ul><li>分类<ul><li>给定训练数据$D =\{\mathbf x_i, y_i\}^N_{i=1}$，分类任务学习一个从输入x到输出y的映射f ：<br>$\hat y = f(\mathbf x) = \underset{c}{arg\ max}\ p(y = c \mid \mathbf x, D)$</li><li>其中y为离散值，其取值范围称为标签空间:$Y =\{1,2,…,C\}$</li><li>当C=2时，为两类分类问题，计算出$p(y = 1 \mid \mathbf x)$即可。此时分布为Bernoulli分布: <script type="math/tex; mode=display">p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))</script>其中$\mu (\mathbf x) = \mathbb{E}(y \mid \mathbf x) = p(y = 1 \mid \mathbf x)$<a id="more"></a></li></ul></li><li>Recall:Bernouili分布<ul><li>Bernoulli分布又名两点分布或者0-1分布。若Bernoulli试验成功，则Bernoulli随机变量X取值为1，否则X为0。记试验成功概率为θ， 我们称X服从参数为θ的Bernoulli分布，记为: 𝑋~𝐵𝑒𝑟(θ), 概率函数（pmf）为：<script type="math/tex; mode=display">p(x) = \theta ^x(1- \theta)^{1-x} = \begin{cases} \theta & if\ x = 1\\ 1 - \theta & if\ x = 0 \end{cases}</script></li><li>Bernoulli分布的均值：$\mu = \theta $</li><li>方差：$\sigma^2 = \theta \times (1-\theta)$</li></ul></li><li>Logistic回归模型<ul><li>Logistic回归模型同线性回归模型类似，也是一个线性模型，只是条件概率𝑝(𝑦|𝐱)的形式不同：<script type="math/tex; mode=display">p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf  x))</script><script type="math/tex; mode=display">\mu (\mathbf x) = \sigma(\mathbf w^T\mathbf x)</script></li><li>其中sigmoid函数（S形函数）定义为<script type="math/tex; mode=display">\sigma(a) = \frac{1}{1+exp(-a)} = \frac{exp(-a)}{exp(-a)+1}</script></li><li>亦被称为logistic函数或logit函数，将实数a变换到[0,1]区间。<ul><li>因为概率取值在[0,1]区间</li><li>Logistic回归亦被称为logit回归</li></ul></li></ul></li><li>为什么用logistic函数？<ul><li>在神经科学中<ul><li>神经元对其输入进行加权和：$f(x) = w^Tx$</li><li>如果该和大于某阈值 $f(x) &gt; \tau $，神经元发放脉冲</li></ul></li><li>在Logistic回归，定义Log Odds Ratio:<script type="math/tex; mode=display">\begin{eqnarray} LOR(x) &=& log \frac {p(y=1 \mid \mathbf  x, \mathbf f w)}{p(y = 0 \mid \mathbf x, \mathbf w)}   &=& log [\frac{1}{1+exp(-\mathbf  w^T\mathbf x)} \frac {1+exp(-\mathbf w^T\mathbf x)}{exp(-\mathbf w^T\mathbf x)}] \\  &=& log [exp(\mathbf w^T \mathbf x)]\\  &=& \mathbf w^T\mathbf x \end{eqnarray}</script></li><li>因此，$iff LOR(\mathbf x) = \mathbf w^T \mathbf x &gt; 0$神经元发放脉冲，即<br>$p(y=1 \mid \mathbf x, \mathbf w) &gt; p(y=0 \mid \mathbf x, \mathbf w)$</li></ul></li><li>线性决策函数<ul><li>在Logistic回归中<ul><li>$LOR(\bf x) = w^Tx &gt; 0, \hat y = 1$</li><li>$LOR(\bf x) = w^Tx &lt;0, \hat y = 0$</li><li>$\bf w^T \bf x = 0$:决策面</li></ul></li><li>因此$a(\bf x) = w^Tx$分类决策面<ul><li>因此Logistic回归是一个线性分类器</li></ul></li></ul></li><li>极大似然估计<ul><li>Logistic回归：$p(y \mid \mathbf {x,w}) = Ber(y \mid \mu (x)),  \mu (\mathbf x) = \sigma (\mathbf w^T\mathbf x)$</li><li>令$\mu_i = \mu(\mathbf x_i)$，则负log似然为<br>$\begin{eqnarray} J(w) = NLL(\mathbf w) &amp;=&amp; - \sum_{i=1}^N log \left[(\mu_i)^{y_i} \times (1-\mu_i)^{1-y_i}\right]\\<br>  &amp;=&amp; \sum_{i=1}^N- \left[y_i log(\mu_i)+(1-y_i)log(1-u_i) \right]<br>  \end{eqnarray}$</li><li>极大似然估计等价于最小Logistic损失</li><li>优化求解：梯度下降／牛顿法</li></ul></li><li>梯度<ul><li>目标函数为<br>$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$</li><li>梯度为<br>$\begin{eqnarray} g(\bf w) &amp;=&amp; \frac{\partial J(\bf w)}{\partial \bf w} = \frac{\partial}{\partial \bf w}[\sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]]\\<br>&amp;=&amp; \sum_{i=1}^N[\mu(\mathbf x_i) - y_i] \mathbf x_i \\<br>&amp;=&amp; \bf X^T(\mu - y)<br>\end{eqnarray}$</li><li>二阶Hessian矩阵为<br>$\begin{eqnarray} H(w) &amp;=&amp; \frac{\partial}{\partial \mathbf w}[\mathbf g( \mathbf w)^T] = \sum_{i=1}^N(\frac {\partial}{\partial \mathbf w}\mu_i) \mathbf x_i^T\\<br>&amp;=&amp; \sum_{i=1}{N}\mu_i(1-\mu_i)\bf x_ix_i^T = X^T\underset{S}{ \underbrace{diag(\mu_i(1-\mu_i)}}X = X^TSX<br>\end{eqnarray}$</li></ul></li><li>牛顿法<ul><li>亦称牛顿-拉夫逊（ Newton-Raphson ）方法<ul><li>牛顿在17世纪提出的一种近似求解方程的方法</li><li>使用函数f(x)的泰勒级数的前面几项来寻找方程f(x) = 0的根</li></ul></li><li>在求极值问题中，求$g(\mathbf w) = \frac {\partial J(\mathbf w)}{\partial w} = 0$的根<ul><li>对应$J(\mathbf w)$处取极值</li></ul></li><li>将导数$g(\mathbf w)$在 $w^t$处进行Taylor展开：<br>$0 = \bf g(\hat w) = g(w^t)+(\hat w - w^t)H(w^t) + Op(\hat w - w^t)$</li><li>去掉高阶无穷小$Op(\bf \hat w - w^t)$，从而得到<br>$g(\bf w^t)+(\hat w - w^t)H(w^t) = 0 \Rightarrow \hat w = w^t - H^{-1}(w^t)g(w^t)$</li><li>因此迭代机制为：<br>$\bf w^{t+1} = w^t - H^{-1}(w^t)g(w^t)$<ul><li>也被称为二阶梯度下降法，移动方向:$\bf d = -(H(w^t))^{-1}g(w^t)$</li><li>Vs. 一阶梯度法，移动方向:$\bf d = -g(w^t)$移动</li></ul></li></ul></li><li>Iteratively Reweighted Least Squares（IRLS）<ul><li>引入记号：<br>$\bf g^t(w) = X^T(\mu^t - y), \mu_i^t = \sigma((w^t)^Tx_i)$<br>$\bf H^t(w) = X^TS^tX$,$ S^t:diag(\mu_i^t(1-\mu_1^t),…,\mu_N^t(1-\mu_N^t))$</li><li>根据牛顿法的结果：<br>$w^{t+1} = w^t - (H^t)^{-1}g^t = (X^TS^tX)^{-1}X^TS^tz$</li><li>回忆最小二乘问题：<ul><li>目标函数：$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T(y - Xw)$</li><li>解：$\hat w = (X^TX)^{-1}X^Ty$</li></ul></li><li>回忆加权最小二乘问题：<ul><li>目标函数:$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T\Sigma^{-1}(y - Xw)$</li><li>解：$\hat w = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}y$</li></ul></li><li>IRLS中，$\bf w^{t+1} = (X^TS^tX)^{-1}X^TS^t[Xw^t + (S^t)^{-1}(y - \mu ^t)]$<ul><li>相当于权重矩阵为 $\Sigma^{-1} = \bf S^t$</li><li>由于$S^t$是对角阵，$S^t$相当于给每个样本的权重$S_{ii}^t = \mu_i^t(1-\mu_i^t), \mathbf z_i^t = (\mathbf w^t)^T\mathbf x_i + \frac {y_i - \mu_i^t}{S_{ii}^t}$</li></ul></li></ul></li><li>拟牛顿法<ul><li>牛顿法比一般的梯度下降法收敛速度快，但是在高维情况下，计算目标函数的二阶偏导数的复杂度很大，而且有时候目标函数的海森矩阵无法保持正定，不存在逆矩阵，此时牛顿法将不再能使用。</li><li>因此，人们提出了拟牛顿法。其基本思想是：不用二阶偏导数而构造出可以近似Hessian矩阵(或Hessian矩阵的逆矩阵)的正定对称矩阵，进而再逐步优化目标函数。不同的构造方法就产生了不同的拟牛顿法（Quasi-Newton Methods）<ul><li>BFGS／LBFGS／Newton-CG</li></ul></li></ul></li><li>正则化的Logistic回归<ul><li>若损失函数取logistic损失，则Logistic回归的目标函数为<br>$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$</li><li>同线性回归类似，Logistic回归亦可加上L2正则<br>$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \Vert \mathbf w\Vert ^2_2$</li><li>或L1正则<br>$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \vert \mathbf w\vert $</li><li>L2正则的Logistic回归求解<ul><li>梯度为:<br>$g_{I 2}(\mathbf{w})=g(\mathbf{w})+\lambda \mathbf{w}=\sum_{i=1}^{N}\left(\mu\left(\mathbf{x}_{i}\right)-y_{i}\right) \mathbf{x}_{i}+\lambda \mathbf{w}=\mathbf{X}^{T}(\mathbf{\mu}-\mathbf{y})+\lambda \mathbf{w}$</li><li>Hessian矩阵为：$\mathbf{H}_{L 2}(\mathbf{w})=\mathbf{H}(\mathbf{w})+\lambda \mathbf{I}=\mathbf{X}^{T} \mathbf{S} \mathbf{X}+\lambda \mathbf{I}$</li><li>类似不带正则的Logistic回归，可采用（随机）梯度下降、牛顿法或拟牛顿法求解。</li></ul></li><li>L1正则的Logistic回归求解<ul><li>L1正则项的在0处不可导</li><li>在此我们L1正则的Logistic回归的牛顿法（IRLS）求解<ul><li>随机梯度下降（在线学习）在CTR预估部分讲解</li></ul></li><li>Recall：IRLS<script type="math/tex; mode=display">\mathbf{w}^{t+1}=\left(\mathbf{X}^{T} \mathbf{S}^{t} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{S}^{t} \mathbf{z}=\underset{\mathbf{w}}{\arg \min }\left\|\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{X} \mathbf{w}-\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{z}\right\|_{2}^{2}</script></li><li>L1正则的Logistic回归在每次迭代中可视为一个再加权的Lasso问题：<script type="math/tex; mode=display">\mathbf{w}^{t+1}=\underset{\mathbf{w}}{\arg \min }\left\|\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{X} \mathbf{w}-\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{z}\right\|_{2}^{2}, s . t .\|\mathbf{w}\|_{1}<t</script></li></ul></li></ul></li><li>小结<ul><li>Logistic回归：<ul><li>损失函数：负log似然损失</li><li>正则：L2/L1正则</li><li>优化：梯度下降／牛顿法／拟牛顿法</li></ul></li></ul></li></ul><h5 id="2、Softmax分类器"><a href="#2、Softmax分类器" class="headerlink" title="2、Softmax分类器"></a>2、Softmax分类器</h5><ul><li>多类分类任务<ul><li>一对所有(One-vs-all /One-vs-rest)：<br>$f_{w}^{c}(x) = p(y=c \mid \mathbf x, \mathbf W), c =1, 2, 3$<br>如果是正则LR，每类的模型都有自己正则参数</li><li>One-vs-all<ul><li>对每个类别c，训练一个logistic回归分类器$f_w^c(x)$，预测概率$y=c$</li><li>对新的输入x，选择使得$f_w^c(x)$最大的类别作为其预测：$\underset{c}{max} f_w^c(\mathbf x)$</li></ul></li></ul></li><li>Softmax分类器<ul><li>从sigmoid（对应二项分布）扩展为softmax函数（对应多项分布Cat）：<br>$p(y=c \mid \mathbf x, \mathbf W) = Cat(y \mid S(\bf W^Tx))$</li><li>Softmax 函数类似取最大函数：<br>$S(\mathbf \eta)_c = \frac {exp(\eta_c)}{\sum^C_{c\prime = 1}\  exp(\eta_{c\prime})}$</li><li>综合起来：<br>$p(y = c \mid \mathbf x, \mathbf W) = \frac {exp(\mathbf w_c^T\mathbf x)}{\sum^C_{c\prime = 1}\  exp(\mathbf w_{c\prime}^T\mathbf x)}$</li></ul></li><li>Softmax回归<ul><li>引入记号：<br>$\mu_{ic} = p(y_i = c \mid \mathbf x_i, \mathbf W) = S(\eta_i)_c$<br>$\eta_i = \mathbf W^T\mathbf x_i$  C $\times$ vector<br>$y_{ic} = \prod (y_i =c)$</li><li>则负似然函数为：$ \begin{eqnarray}J(\mathbf W)<br>&amp;=&amp;NLL(\mathbf W) \\<br>&amp;=&amp; -l(\mathbf W) \\<br>&amp;=&amp; -log\prod_{i=1}^N \prod_{c=1}^C \mu_{ic}^{y_{ic}} \\<br>&amp;=&amp; - \sum_{i=1}^N\sum_{c=1}^Cy_{ic}log \mu_{ic}\\<br>&amp;=&amp; -\sum_{i=1}^N[(\sum_{c=1}^Cy_{ic}\mathbf w_c^T \mathbf x_i) - log(\sum_{c \prime = 1}^C exp(\mathbf w^T_{c \prime} \mathbf x_i))]<br>\end{eqnarray}$<ul><li>梯度:$g = [\nabla J(\mathbf w_i),…,\nabla J(\mathbf w_c)] = [\mathbf g_1,…,\mathbf g_c]$<br>$\mathbf g_c = \sum_{i=1}^N(\mu_{ic}-y_{ic})\mathbf x_i$</li><li>Hessian矩阵为正定：$\mathbf H = \sum_{i=1}^N(diag(\mathbf \mu_i)-\mathbf \mu_i \mathbf \mu_i^T)\otimes \mathbf x_i \mathbf x_i^T$</li></ul></li></ul></li><li>小结：Softmax分类器Logistic回归<ul><li>Softmax分类器能实现多类分类，是对Logistic回归在两类分类任务上的扩展</li><li>优化算法和正则与两类分类Logistic回归类似</li></ul></li></ul><h5 id="3、Scikit-learn中的Logistic回归实现"><a href="#3、Scikit-learn中的Logistic回归实现" class="headerlink" title="3、Scikit learn中的Logistic回归实现"></a>3、Scikit learn中的Logistic回归实现</h5><ul><li>Scikit learn 中的LogisticRegression实现<ul><li>Scikit learn提供的LogisticRegression实现为：<br>LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’liblinear’, max_iter=100, multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1)<ul><li>Logistic回归的正则参数：penalty、C</li><li>优化求解参数： dual、solver、max_iter、tol、warm_start</li><li>模型参数： multi_class、fit_intercept、intercept_scaling</li><li>样本均衡参数：class_weight</li></ul></li></ul></li><li>LogisticRegression参数列表</li></ul><div class="table-container"><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>penalty</td><td>惩罚函数／正则函数，支持L2正则和L1正则，缺省：L2</td></tr><tr><td>dual</td><td>原问题（primal）还是对偶问题求解。对偶只支持L2正则和liblinear solver。当样本数n_samples&gt;特征数目n_features时，缺省：False</td></tr><tr><td>tol</td><td>迭代终止判据的误差范围。缺省:1e-4</td></tr><tr><td>C</td><td>C=1/$\lambda$ , 缺省：1</td></tr><tr><td>fit_intercept</td><td>是否在决策函数中加入截距项。如果数据已经中心化，可以不用。缺省：True</td></tr><tr><td>intercept_scaling</td><td>截距缩放因子，当fit_intercept为True且liblinear solver有效所以还是对y做标准化预处理</td></tr><tr><td>class_weight</td><td>不同类别样本的权重，用户指定每类样本权重或‘balanced’（每类样本权重与该类样本出现比例成反比）。缺省：None</td></tr><tr><td>random_state</td><td>混合数据的伪随机数。缺省：None</td></tr><tr><td>solver</td><td>优化求解算法，可为‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’。缺省：liblinear</td></tr><tr><td>max_iter</td><td>最大迭代次数，当newton-cg, sag and lbfgs solvers时有效。缺省：100</td></tr><tr><td>multi_class</td><td>多类分类处理策略，可为‘ovr’, ‘multinomial’。‘ovr’为1对多，将多类分类转化为多个两类分类问题，multinomial为softmax分类。缺省：‘ovr’</td></tr><tr><td>verbose</td><td>是否详细输出</td></tr><tr><td>warm_start</td><td>是否热启动（用之前的结果作为初始化），对liblinear solver无效。缺省：False</td></tr><tr><td>n_jobs</td><td>多线程控制。缺省值-1，算法自动检测可用CPU核，并使用全部核</td></tr></tbody></table></div><ul><li>多分类问题<ul><li>multi_class参数决定了多类分类的实现方式</li><li>‘ovr’ ：即1对其他（one-vs-rest，OvR），将多类分类转化为多个二类分类任务。为了完成第c类的分类决策，将所有第c类的样本作为正例，除了第c类样本以外的所有样本都作为负例。</li><li>‘multinomial’ ：多对多（many-vs-many，MvM），即softmax回归模型。</li><li>OvR相对简单，但分类效果相对略差<ul><li>大多数情况，不排除某些情况下OvR更好</li></ul></li><li>MvM分类相对精确，但分类速度较OvR慢</li><li>multi_class选择会影响优化算法solver参数的选择<ul><li>OvR：可用所有的slover</li><li>Multinomial： 只能选择newton-cg, lbfgs和sag／saga</li></ul></li></ul></li><li>优化求解算法solver<ul><li>liblinear：使用了开源的liblinear库实现，使用坐标轴下降法来迭代优化损失函数</li><li>sag：随机平均梯度下降（Stochastic Average Gradient），是梯度下降法的变种，每次迭代仅用一部分的样本来计算梯度，适合于样本多的情况</li><li>saga： sag的增强版本</li><li>lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵（Hessian矩阵）来迭代优化损失函数</li><li>newton-cg：牛顿法家族的一种（ 共轭梯度）</li><li>对小数据集，‘liblinear’ 是一个很好的选择，而‘sag’ 和‘saga’ 对大数据集更快</li><li>对多类分类问题，只有‘newton-cg’, ‘sag’, ‘saga’ 和‘lbfgs’支持MvM（multinomial）， ‘liblinear’ 只支持OvR（one-versus-rest） 的方式</li><li>‘newton-cg’, ‘lbfgs’ 和‘sag’ 支持L2正则，而‘liblinear’ 和‘saga’ 支持L1正则</li><li>注意： ‘sag’ 和‘saga’ 只有当特征有类似的尺度（scale）时能保证快速收敛。（对数据做标准化预处理）</li></ul></li><li>优化求解算法solver选择</li></ul><div class="table-container"><table><thead><tr><th>正则</th><th>求解算法</th><th>应用场景</th></tr></thead><tbody><tr><td>L1</td><td>liblinear</td><td>如果模型的特征非常多，希望一些不重要的特征系数归零，从而让模型系数稀疏的话，可以使用L1正则化。liblinear适用于小数据集</td></tr><tr><td>L1</td><td>saga</td><td>当数据量较大，且选择L1，只能采用saga</td></tr><tr><td>L2</td><td>liblinear</td><td>libniear只支持多元逻辑回归的OvR，不支持多项分布损失（MvM），但MVM相对精确</td></tr><tr><td>L2</td><td>lbfgs/newton-cg/sag</td><td>较大数据集，支持OvR和MvM两种多元logit回归</td></tr><tr><td>L2</td><td>sag／saga</td><td>如果样本量非常大，sag／sga是第一选择</td></tr></tbody></table></div><p>对于大数据集，可以考虑使用SGDClassifier，并使用logloss</p><ul><li>类别权重class_weight<ul><li>class_weight用于不同类别样本数目不均衡的情况</li></ul></li></ul><h5 id="4、不平衡数据分类学习"><a href="#4、不平衡数据分类学习" class="headerlink" title="4、不平衡数据分类学习"></a>4、不平衡数据分类学习</h5><ul><li>不平衡数据的出现场景<ul><li>搜索引擎的点击预测<ul><li>点击的网页往往占据很小的比例</li></ul></li><li>电子商务领域的商品推荐<ul><li>推荐的商品被购买的比例很低</li></ul></li><li>信用卡欺诈检测</li><li>信用卡欺诈检测</li><li>…</li></ul></li><li>解决方案<ul><li>从数据的角度：抽样，从而使得数据相对均衡</li><li>从算法的角度：考虑不同误分类情况代价的差异性对算法进行优化</li></ul></li><li>采样<ul><li>随机欠采样：从多数类中随机选择少量样本再合并原有少数类样本作为新的训练数据集<ul><li>有放回采样</li><li>无放回采样</li><li>会造成一些信息缺失，选取的样本可能有偏差</li></ul></li><li>随机过采样：随机复制少数类来样本<ul><li>扩大了数据集，造成模型训练复杂度加大，另一方面也容易造成模型的过拟合问题</li></ul></li></ul></li><li>集成学习算法<ul><li>EasyEnsemble算法：<ul><li>对于多数类样本，通过n次有放回抽样生成n份子集</li><li>少数类样本分别和这n份样本合并训练一个模型：n个模型</li><li>最终模型：n个模型预测结果的平均值</li></ul></li><li>BalanceCascade（级联）算法：<ul><li>从多数类中有效地选择一些样本与少数类样本合并为新的数据集进行训练</li><li>训练好的模型每个多数类样本进行预测。若预测正确，则不考虑将其作为下一轮的训练样本</li><li>依次迭代直到满足某一停止条件，最终的模型是多次迭代模型的组合</li></ul></li></ul></li><li>SMOTE: Synthetic Minority Over-sampling Technique<ul><li>基本思想：基于“插值”来为少数类合成新的样本</li><li>对少数类的一个样本$i$ ，其特征向量为$x_i$,：<ul><li><ol><li>从少数类的全部N 个样本中找到样本$x_i$的K个近邻（如欧氏距离），记为$x_{i(near)}, near \in \{1, …, K\}$</li></ol></li><li><ol><li>从这K个近邻中随机选择一个样本$x_{i(nn)}$，再生成一个0到1之间的随机数$\zeta$ ，从而合成一个新样本$x_{i1}$：<ul><li>$x_{i1} = (1-\zeta)x_i + \zeta x_{inn}$</li><li>新样本$x_{i1}$相当于是表示样本xi和表示样本$x_{i(nn)}$ 的点之间所连线段上的一个点： 插值</li></ul></li></ol></li></ul></li><li>SMOTE算法摒弃了随机过采样复制样本的做法，可以防止随机过采样易过拟合的问题。实践证明此方法可以提高分类器的性能</li><li>SMOTE 对高维数据不是很有效</li><li>当生成合成性实例时，SMOTE 并不会把来自其他类的相邻实例考虑进来，这导致了类重叠的增加，并会引入额外的噪音。为了解决SMOTE算法的这一缺点提出一些改进算法，如Borderline-SMOTE算法</li></ul></li><li>代价敏感学习<ul><li>在算法层面上解决不平衡数据学习的方法主要是基于代价敏感学习算法(Cost-Sensitive Learning)</li><li>代价敏感学习方法的核心要素是代价矩阵：不同类型的误分类情况导致的代价不一样</li><li>基于代价矩阵分析，代价敏感学习方法主要有以下三种实现方式：</li><li>从贝叶斯风险理论出发，把代价敏感学习看成是分类结果的一种后处理，按照传统方法学习到一个模型，以实现损失最小为目标对结果进行调整<ul><li>不依赖所用具体的分类器</li><li>但是缺点要求分类器输出值为概率</li></ul></li><li>从学习模型出发，对具体学习方法的改造，使之能适应不平衡数据下的学习<ul><li>代价敏感的支持向量机，决策树，神经网络</li><li>从预处理的角度出发，将代价用于权重的调整，使得分类器满足代价敏感的特性</li></ul></li></ul></li><li>Scikit learn中的不均衡样本分类处理<ul><li>类别权重class_weight<ul><li>class_weight参数用于标示分类模型中各类别样本的权重</li><li><ol><li>不考虑权重，即所有类别的权重相同</li></ol></li><li><ol><li>balanced：自动计算类别权重<ul><li>某类别的样本量越多，其权重越低；样本量越少，则权重越高</li><li>类权重计算方法为：n_samples / (n_classes * np.bincount(y))<ul><li>n_samples为样本数，n_classes为类别数量，np.bincount(y)输出每个类的样本数</li></ul></li></ul></li></ol></li><li><ol><li>手动指定各个类别的权重<ul><li>如对于0,1二类分类问题，可以定义class_weight={0:0.9, 1:0.1}，即类别0的权重为90%，而类别1的权重为10%</li></ul></li></ol></li></ul></li><li>样本权重sample_weight<ul><li>模型训练：$fit(X, y, sample_weight=None)$<ul><li>其中参数sample_weight为样本权重参数</li></ul></li><li>当样本高度失衡时，样本不是总体样本的无偏估计，可能导致模型预测能力下降</li><li>解决方案：调节样本权重<ul><li>一种是在class_weight使用balanced</li><li>另一种是在调用fit函数时，通过sample_weight来调节每个样本权重</li><li>如果两种方法都用了，那么样本的真正权重是class_weight*sample_weight</li></ul></li></ul></li></ul></li><li>小结：Logistic回归<ul><li>不均衡样本分类<ul><li>样本采样：过采样、欠采样</li><li>分类器：代价敏感函数<ul><li>样本权重、类别权重</li></ul></li></ul></li></ul></li></ul><h5 id="5、分类模型的评价"><a href="#5、分类模型的评价" class="headerlink" title="5、分类模型的评价"></a>5、分类模型的评价</h5><ul><li>分类模型性能评价<ul><li>损失函数可以作为评价指标(log_loss、zero_one_loss、hinge_loss)</li><li>logistic／负log似然损失（log_loss）：<ul><li>logloss $=-\frac{1}{N} \sum_{i=0}^{N} \sum_{j=0}^{M} y_{i j} \log p_{i j}$<ul><li>M为类别数，$y_{ij}$为二值，当第i个样本为第j类时$y_{ij}$ = 1，否则取0；$p_{ij}$为模型预测的第i个样本为第j类的概率</li><li>当M=2时, $\operatorname{logloss}=-\frac{1}{N} \sum_{i=0}^{N}\left(y_{i} \log p_{i}+\left(1-y_{i}\right) \log \left(1-p_{i}\right)\right)$<ul><li>$y_{i}$为第i个样本类别，$p_{i}$为模型预测的第i个样本为第1类的概率</li></ul></li></ul></li><li>0-1损失(zero_one_loss) （错误率、正确率评价指标均与此有关）<ul><li>$\mathrm{MCE}=-\frac{1}{N} \sum_{\hat{y}_{i} \neq y_{i}} 1$</li></ul></li></ul></li></ul></li><li>两类分类任务中更多评价指标<ul><li>ROC／AUC</li><li>PR曲线</li><li>MAP@n</li></ul></li><li>False Positive &amp; False Negative<ul><li>0-1损失：假设两种错误的代价相等<ul><li>False Positive （FP） &amp; False Negative（FN）</li></ul></li><li>有些任务中可能某一类错误的代价更大<ul><li>如医疗诊断中将病例误分为正常，错过诊疗时机</li><li>因此单独列出每种错误的比例：混淆矩阵</li></ul></li><li>混淆矩阵（confusion matrix）<ul><li>真正的正值（true positives）</li><li>假的正值（false positives）</li><li>真正的负值（true negatives）</li><li>假的负值（false negatives ）</li><li>Scikit-learn实现了多类分类任务的混淆矩阵</li><li>sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)<ul><li>y_true： N个样本的标签观测值／真值</li><li>y_pred： N个样本的预测标签值</li><li>labels：C个类别在矩阵的索引顺序<ul><li>缺省为y_true或y_pred类别出现的顺序</li></ul></li><li>sample_weight： N个样本的权重</li></ul></li></ul></li></ul></li></ul><div class="table-container"><table><thead><tr><th></th><th>$\hat y = 1$</th><th>$\hat y = 0$</th><th>$\Sigma$</th></tr></thead><tbody><tr><td>y = 1</td><td>#TP</td><td>#FN</td><td>$N_{+}$</td></tr><tr><td>y = 0</td><td>#FP</td><td>#TN</td><td>$N_{-}$</td></tr><tr><td>$\Sigma$</td><td>$\hat N_{+}$</td><td>$\hat N_{-}$</td></tr></tbody></table></div><ul><li>Receiver Operating Characteristic (ROC)<br>  $\operatorname{accuracy}=\frac{T P+T N}{N}$<br>  error rate $=\frac{F P+F N}{N}$<ul><li>PPV - positive predictive value, precision 预测结果为真的样本中真正为真的比例</li><li>TPR - true positive rate, sensitivity, recall, hit rate 预测结果召回了多少真正的真样本</li><li>FPR – False positive rate, false alarm, fallout 预测结果将多少假的样本预测预测成了真</li><li>下面我们讨论给定阈值τ的TPR和FPR</li><li>如果不是只考虑一个阈值，而是在一些列阈值上运行检测器，并画出TPR和FPR为阈值τ的隐式函数，得到ROC曲线。<br><img src="/2019/04/06/第二周 Logistic回归、SVM/ROC曲线.png" alt="ROC曲线"><ul><li>$T P R=\frac{T P}{N_{+}}$</li><li>$F P R=\frac{F P}{N_{-}}$</li></ul></li></ul></li></ul><div class="table-container"><table><thead><tr><th></th><th>$\hat y = 1$</th><th>$\hat y = 0$</th><th>$\Sigma$</th></tr></thead><tbody><tr><td>y = 1</td><td>#TP</td><td>#FN</td><td>$N_{+}$</td></tr><tr><td>y = 0</td><td>#FP</td><td>#TN</td><td>$N_{-}$</td></tr><tr><td>$\Sigma$</td><td>$\hat N_{+}$</td><td>$\hat N_{-}$</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th></th><th>y = 1</th><th>y = 0</th><th>y = 1</th><th>y = 0</th></tr></thead><tbody><tr><td>$\hat y = 1$</td><td>$T P / \hat{N}_{+}=$ precision</td><td>$F P / \hat{N}_{+}=\mathrm{FDP}$</td><td>$T P / N_{+}=\mathrm{TPR}$</td><td>$F P / N_{-}=\mathrm{FPR}$</td></tr><tr><td>$\hat y = 0$</td><td>$F N / \hat{N}_{-}$</td><td>$T N / \hat{N}_{-}=\mathrm{NPV}$</td><td>$F N / N_{+}=\mathrm{FNR}$</td><td>$T N / N_{-}=\mathrm{TNR}$</td></tr></tbody></table></div><ul><li>PR曲线<ul><li>Precision and Recall (PR曲线)：用于稀有事件检测，如目标检测、信息检索<ul><li>负样本非常多，因此$F P R=F P / N_{-}$很小，比较TPR和FPR不是很有信息（ROC曲线中只有左边很小一部分有意义）$\rightarrow$ 只讨论正样本</li><li>Precision（精度，查准率）：以信息检索为例，对于一个查询，返回了一系列的文档，正确率指的是返回结果中相关文档占的比例<ul><li>Precision=返回结果中相关文档的数目/返回结果的数目</li></ul></li><li>Recall（召回率，查全率）：返回结果中相关文档占所有相关文档的比例<ul><li>Recall=返回结果中相关文档的数目/所有相关文档的数目</li></ul></li></ul></li><li>Precision and Recall (PR曲线)<ul><li>阈值变化时的P和R</li><li>Precsion $=T P / \hat{N}_{+}$ ：检测结果真正为正的比例</li><li>$\mathrm{Recall}=T P / N_{+}$：被正确检测到的正样本的比例<br><img src="/2019/04/06/第二周 Logistic回归、SVM/PR曲线.png" alt="PR曲线"></li></ul></li></ul></li><li>AP<ul><li>Precision只考虑了返回结果中相关文档的个数，没有考虑文档之间的序。</li><li>对一个搜索引擎或推荐系统而言，返回的结果是有序的，且越相关的文档越靠前越好，于是有了AP的概念。</li><li>AP: Average Precision，对不同召回率点上的精度进行平均<ul><li>$A P=\int_{0}^{1} p(k) d r=\sum_{k=0}^{n} p(k) \Delta r(k)$</li><li>即PR曲线下的面积（Recall： AUC为ROC下的面积）</li><li>其中k为返回文档中的序位，n为返回文档的数目，$p(k)$ 为列表中k截止点的precision， $\Delta r(k)$ 表示从k-1到k Recall的变化。</li></ul></li><li>上述离散求和表示等价于：$A P=\sum_{k=0}^{n} p(k) r e l(k) /$ 相关文档数目，其中<br>$r e l(k)$为示性函数，即第k个位置为相关文档取1，否则取0.<ul><li>计算每个位置上的precision，如果该位置的文档是不相关的则该位置precision=0</li><li>然后对所有的位置的precision再求平均</li></ul></li></ul></li><li>Mean Average Precision<ul><li>多个查询的AP平均：</li><li>$M A P=\left(\sum_{q=0}^{Q} A P(q)\right) /(Q)$</li><li>其中Q为查询的数目，n为文档数目</li></ul></li><li>MAP@K （MAPK）<ul><li>在现代web信息检索中，recall其实已经没有意义，因为相关文档有成千上万个，很少有人会关心所有文档。</li><li>Precision@K：在第K个位置上的Precision<ul><li>对于搜索引擎，考虑到大部分作者只关注前一、两页的结果，所以Precision @10， Precision @20对大规模搜索引擎非常有效</li></ul></li><li>MAP@K：多个查询Precision@K的平均</li></ul></li><li>F1 分数<ul><li>亦被称为F1 score, balanced F-score or F-measure</li><li>Precision 和Recall 调和平均：<br>$F 1=\frac{(2 \ast  Precision   \ast   Recall) }{ (Precision + Recall) }$<ul><li>最好为1，最差为0</li><li>多类：每类的F1平均值</li></ul></li></ul></li><li>模型性能评价<ul><li>Scikit learn 提供3 不同的API，用于评估模型预测的性能：<ul><li>Estimator score method: 模型自带的分数方法（score函数）提供一个缺省的评估准则。</li><li>Scoring parameter: 采用交叉验证的模型评估工具（ model_selection.cross_val_score and model_selection.GridSearchCV、以及一些xxxCV类）有scoring 参数（最佳参数为最大scoring模型对应的参数）</li><li>Metric functions: metrics模块提供评价预测性能的功能<ul><li>Classification metrics,</li><li>Multilabel ranking metrics</li><li>Regression metrics</li><li>Clustering metrics</li></ul></li></ul></li></ul></li><li>分类模型性能评价<ul><li>对分类模型，缺省的score函数返回的是正确率（Mean accuracy）</li><li>scoring参数<ul><li>交叉验证中可设置scoring参数，规定模型性能的评价指标</li><li>注意：scoring越大的模型性能越好，所以如果采用损失／误差，需要加neg，如‘neg_log_loss’</li></ul></li><li>可以自定义评价函数<ul><li>有些指标还需要额外的参数，而没有在scoring出现，或者某个任务需要特殊的指标，scikit learn支持自定义scoring函数</li></ul></li><li>Scikit learn中Classification metrics 模块针对两类分类问题提供的性能评价指标有</li></ul></li></ul><h5 id="6、Logistic回归之模型选择-参数调优"><a href="#6、Logistic回归之模型选择-参数调优" class="headerlink" title="6、Logistic回归之模型选择_参数调优"></a>6、Logistic回归之模型选择_参数调优</h5><ul><li>网格搜索（Grid Search）<ul><li>不同超参数下的模型性能不同。</li><li>为了找到最佳模型，通常对这些超参数设定搜索范围</li><li>多个超参数可以联合一起优化，得到超参数的搜索网格<ul><li>如：LogisticRegression中的超参数penalty和C一起优化<ul><li>penalty可取‘l2’或‘l1’</li><li>C假设取值范围为： 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000</li></ul></li><li>则搜索网格为：</li></ul></li></ul></li></ul><div class="table-container"><table><thead><tr><th>‘l2’</th><th>0.001</th><th>0.01</th><th>0.1</th><th>1</th><th>10</th><th>100</th><th>1000</th></tr></thead><tbody><tr><td>‘l1’</td><td>0.001</td><td>0.01</td><td>0.1</td><td>1</td><td>10</td><td>100</td><td>1000</td></tr></tbody></table></div><ul><li>LogisticRegression超参数调优<ul><li>超参数调优需先确定超参数的搜索网格，然后对每个可能的超参数组合评估其性能</li><li>对LogisticRegression的超参数调优，scikit learn提供给两种实现方式：<ul><li><ol><li>同其他estimator一样，调用GridSearchCV （集成了网格搜索和交叉验证）：设置候选参数集合、根据候选参数集合构造GridSearchCV、调用GridSearchCV 的fit函数；</li></ol></li><li><ol><li>LogisticRegressionCV 类内置的LR的交叉验证，用于找到最优的C参数</li></ol></li></ul></li></ul></li><li>LogisticRegressionCV<ul><li>LogisticRegressionCV 使用了内置的Logistic回归的交叉验证，用于找到最优的C参数。（正则参数penalty可设为‘l1’ 或‘l2’ ）</li><li>对于多分类问题<ul><li>如果multi_class参数设置为“ovr”，对于每个类都获得一个最优的C；</li><li>如果multi_class设置为”multinomial”, 将获得一个最优的C，它使得交叉熵的loss（corss-entropy loss）最小。</li></ul></li></ul></li></ul><h5 id="7、Logistic回归-Otto商品分类代码"><a href="#7、Logistic回归-Otto商品分类代码" class="headerlink" title="7、Logistic回归-Otto商品分类代码"></a>7、Logistic回归-Otto商品分类代码</h5><h5 id="8、支持向量机"><a href="#8、支持向量机" class="headerlink" title="8、支持向量机"></a>8、支持向量机</h5><ul><li>SVM基本原理<ul><li>SVM as 最大间隔分类器<ul><li>最大间隔原则：最大化两个类最近点之间的距离<ul><li>这个距离被称为间隔(margin)</li><li>边缘上的点被称为支持向量(support vectors)</li></ul></li><li>我们先假设分类器是线性可分的</li></ul></li><li>间隔<ul><li>线性分类面：$f(\mathbf{x}) = \mathbf {w}^{\mathrm{T}} \mathbf {x}+w_{0}$</li><li>则有 $\mathbf{x}=\mathbf{x}_{\mathrm{p}}+r \frac{\mathbf{w}}{|\mathbf{w}|}$<ul><li>其中x到分类面的距离r</li></ul></li><li>代入得到 $f(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \mathbf{x}+w_{0}=\mathbf{w}^{\mathrm{T}}\left(x_{\mathrm{p}}+r \frac{\mathbf{w}}{|\mathbf{w}|}\right)+w_{0}$<br>$=\mathbf{w}^{\mathrm{T}} x_{\mathrm{p}}+r \frac{\mathbf{w}^{\mathrm{T}} \mathbf{w}}{|\mathbf{w}|}+w_{0}$<br>$\Rightarrow r=\frac{f(\mathbf{x})}{|\mathbf{w}|}$</li><li>当x=0时，原点到分类面的距离<br>$r_{0}=\frac{f(\mathbf{0})}{|\mathbf{w}|}=\frac{w_{0}}{|\mathbf{w}|}$</li></ul></li><li>线性判别函数<ul><li>线性判别函数利用一个超平面把特征空间分隔成两个区域。</li><li>超平面的方向由法向量w确定，它的位置由阈值$w_{0}$确定。</li><li>判别函数f(x)正比于x点到超平面的代数距离（带正负号）<ul><li>当x点在超平面的正侧时， f(x) &gt;0；</li><li>当x点在超平面的负侧时， f(x) &lt;0</li><li>x点到超平面的距离$r y_{i}=\frac{y_{i} f(\mathbf{x})}{|\mathbf{w}|}$可视为对x判别的“置信度”<br>$y_{i} \in\{1,-1\}$</li></ul></li></ul></li><li>SVM 符号表示</li><li>间隔计算</li><li>SVM：最大间隔<ul><li>最大化间隔的超平面为<br>$\max _{w_{0}, \mathbf{w}} \frac{2}{|\mathbf{w}|}, \quad$ subject to $\quad y_{i}\left(w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right) \geq 1, \quad \forall i$</li><li>等价于<br>$\min _{w_{0}, \mathbf{w}} \frac{1}{2}|\mathbf{w}|^{2}, \quad$ subject to $\quad y_{i}\left(w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right) \geq 1, \quad \forall i$<ul><li>二次规划问题(目标函数为二次函数，约束为线性约束)</li><li>变量数为D+1，约束项的数目为N</li></ul></li></ul></li></ul></li><li>对偶表示(Dual Representation)<ul><li>凸优化理论告诉我们可以将该优化问题等价地写成其对偶形式(dual formulation) 。</li><li>定义拉格朗日函数<br>$L\left(\boldsymbol{a}, w_{0}, \mathbf{w}\right)=\frac{1}{2} \mathbf{w}^{T} \mathbf{w}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right)-1\right), \quad \alpha_{i} \geq 0$</li><li>求使得目标$L\left(\boldsymbol{\alpha}, w_{0}, \mathbf{w}\right)$最小的对w0和w：<br>$\frac{\partial L}{\partial \mathbf{w}}=0 \Rightarrow \mathbf{w}=\sum_{i=1}^{N} \alpha_{i} y_{i} \mathbf{x}_{i}$<br>$\frac{\partial L}{\partial w_{0}}=0 \Rightarrow \sum_{i=1}^{N} \alpha_{i} y_{i}=0$</li><li>将$w_{0}, \mathbf{w}$从$L\left(\boldsymbol{\alpha}, w_{0}, \mathbf{w}\right)$消去，得到对偶表示</li></ul></li><li>Karush-Kuhn-Tucker (KKT) Conditions<ul><li>如果强对偶条件成立，则对最优的  $\mathbf{x}^{ \ast } , \lambda^{ \ast } , \mathbf{\mu}^{ \ast } $，必须满足下述KKT条件</li><li>原问题的可行域：$f_{i}\left(\mathbf{x}^{ \ast } \right) \leq 0, h_{j}\left(\mathbf{x}^{ \ast }\right)=0$</li><li>对偶问题的可行域：$\lambda^{\ast } \geq 0$</li><li>平稳条件：$\Delta_{x} L(\mathbf{x}, \lambda, \boldsymbol{\mu})=0$</li><li>互补松弛条件：$\lambda_{i}^{\ast } f_{i}\left(\mathbf{x}^{\ast }\right)=0$</li><li>如果$\mathbf{x}^{+}, \lambda^{+}, \mathbf{\mu}^{+}$满足凸问题的KKT条件，则其是最优的。</li></ul></li><li>SVM – Duality<ul><li>原问题：$P=\min _{w} \frac{1}{2} \mathbf{w}^{T} \mathbf{w}$<br>s.t. $y_{i}\left(w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right) \geq 1$</li><li>拉格朗日函数:$L(\mathbf{x}, \boldsymbol{a})=\frac{1}{2} \mathbf{w}^{T} \mathbf{w}-\sum_{i} \alpha_{i}\left(y_{i}\left(w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}\right)-1\right)$</li><li>对偶问题：$D=\max _{\alpha}\left(\mathbf{1}^{T} \boldsymbol{\alpha}-\boldsymbol{\alpha}^{T} \mathbf{y K y}\right),$ where $K_{i j}=\left\langle x_{i}, x_{j}\right\rangle$<br>S.t. $\alpha_{i} \geq 0$</li></ul></li><li>对偶性<ul><li>拉格朗日对偶通常是凹的（即使原问题非凸） ，可能更容易优化求解</li><li>弱对偶性：$\mathrm{P} \geq \mathrm{D}$<ul><li>总是成立</li></ul></li><li>强对偶性：P = D<ul><li>并不总是成立</li><li>对凸问题通常成立</li><li>对SVM QP成立</li></ul></li></ul></li><li>SVM – KKT Conditions<ul><li>拉格朗日函数:$L(\mathbf{x}, \boldsymbol{\alpha})=\frac{1}{2} \mathbf{w}^{T} \mathbf{w}-\boldsymbol{\alpha}^{T}\left(\mathbf{y}\left(\mathbf{x}^{T} \mathbf{w}+w_{0} \mathbf{1}\right)-1\right)$</li><li>对偶问题的可行域：$\alpha_{i}^{\ast } \geq 0$</li><li>原问题的可行域：$y_{i}\left(w_{0}^{\ast }+\mathbf{w}^{\ast_{T}} \mathbf{x}_{i}\right)-1 \geq 0$</li><li>互补松弛条件：$\alpha_{i}^{\ast }\left[y_{i}\left(w_{0}^{\ast }+\mathbf{w}^{\ast T} \mathbf{x}_{i}\right)-1\right]=0$</li><li>平稳条件：$\begin{aligned} \Delta L_{\mathrm{w}} &amp;=0 \Rightarrow \mathbf{w}^{\ast }=\mathbf{x} y \boldsymbol{\alpha} \\ \Delta L_{w_{0}} &amp;=0 \Rightarrow \boldsymbol{\alpha}^{\ast } y \mathbf{1}=0 \end{aligned}$</li></ul></li><li>α的稀疏性<ul><li>根据KKT条件，对每个点<br>$\alpha_{i}^{\ast}=0 \quad$ or $\quad y_{i}\left(w_{0}^{\ast }+\mathbf{w}^{\ast } x_{i}\right)=1$</li><li>当$\alpha_{i}^{\ast }=0$时，该点在决策函数中不起作用<br>$f(\mathbf{x})=w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}=w_{0}+\sum_{i} \alpha_{i} y_{i}\left\langle\mathbf{x}, \mathbf{x}_{i}\right\rangle$</li><li>其他点称为支持向量，满足$y_{i}\left(w_{0}^{\ast }+\mathbf{w}^{\ast T} \mathbf{x}_{i}\right)=1$</li><li>对应位于最大间隔超平面上的点<ul><li>模型训练好后，大多数点可以抛掉，只需保留支持向量</li></ul></li></ul></li><li>w0的计算<ul><li>由于支持向量满足 $y_{i}\left(w_{0}+\mathbf{w}^{T} \mathbf{x}_{i}\right)=1$</li><li>将 $f(\mathbf{x})=w_{0}+\mathbf{w}^{\mathrm{T}} \mathbf{x}=w_{0}+\sum \alpha_{i} y_{i}\left\langle\mathbf{x}, \mathbf{x}_{i}\right\rangle$</li><li>代入，得到<br>$y_{i}\left[\sum_{m \in S} \alpha_{m} y_{m}\left\langle\mathbf{x}_{i}, \mathbf{x}_{m}\right\rangle+ w_{0}\right]=1$<ul><li>用任意一个支持向量即可求得$w_{0}$</li></ul></li><li>为了得到更稳定的解，两边同乘以$y_{i}, y_{i}^{2}=1$</li><li>并对所有的支持向量求平均，得到<br>$w_{0}=\frac{1}{N_{S}} \sum_{m \in \mathcal{S}}\left[y_{i}-\sum_{m \in \mathcal{S}} \alpha_{m} y_{m}\left\langle\mathbf{x}_{i}, \mathbf{x}_{m}\right\rangle\right]$</li></ul></li><li>小结<ul><li>SVM基本原理<ul><li>最大间隔原则</li><li>对偶表示(Dual Representation)</li><li>KKT条件<h5 id="9、带松弛因子的C-SVM"><a href="#9、带松弛因子的C-SVM" class="headerlink" title="9、带松弛因子的C-SVM"></a>9、带松弛因子的C-SVM</h5><h5 id="10、核方法"><a href="#10、核方法" class="headerlink" title="10、核方法"></a>10、核方法</h5><h5 id="11、支持向量回归（SVR）"><a href="#11、支持向量回归（SVR）" class="headerlink" title="11、支持向量回归（SVR）"></a>11、支持向量回归（SVR）</h5><h5 id="12、sklearn中的SVM实现"><a href="#12、sklearn中的SVM实现" class="headerlink" title="12、sklearn中的SVM实现"></a>12、sklearn中的SVM实现</h5><h5 id="13、SVM-Otto"><a href="#13、SVM-Otto" class="headerlink" title="13、SVM-Otto"></a>13、SVM-Otto</h5></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;1、Logistic回归基本原理&quot;&gt;&lt;a href=&quot;#1、Logistic回归基本原理&quot; class=&quot;headerlink&quot; title=&quot;1、Logistic回归基本原理&quot;&gt;&lt;/a&gt;1、Logistic回归基本原理&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;分类&lt;ul&gt;
&lt;li&gt;给定训练数据$D =\{\mathbf x_i, y_i\}^N_{i=1}$，分类任务学习一个从输入x到输出y的映射f ：&lt;br&gt;$\hat y = f(\mathbf x) = \underset{c}{arg\ max}\ p(y = c \mid \mathbf x, D)$&lt;/li&gt;
&lt;li&gt;其中y为离散值，其取值范围称为标签空间:$Y =\{1,2,…,C\}$&lt;/li&gt;
&lt;li&gt;当C=2时，为两类分类问题，计算出$p(y = 1 \mid \mathbf x)$即可。此时分布为Bernoulli分布: &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))&lt;/script&gt;其中$\mu (\mathbf x) = \mathbb{E}(y \mid \mathbf x) = p(y = 1 \mid \mathbf x)$&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="http://minhzou.top/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="人工智能" scheme="http://minhzou.top/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="Logistic回归" scheme="http://minhzou.top/tags/Logistic%E5%9B%9E%E5%BD%92/"/>
    
      <category term="SVM" scheme="http://minhzou.top/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>Hexo + GitHub Pages + Next在windows下搭建个人博客</title>
    <link href="http://minhzou.top/2019/04/01/Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://minhzou.top/2019/04/01/Hexo搭建博客/</id>
    <published>2019-04-01T03:43:22.257Z</published>
    <updated>2019-04-02T08:05:09.298Z</updated>
    
    <content type="html"><![CDATA[<p>才搭好博客，发现在博客发布文章确实比微信公众号方便很多，这里简略说下用 Hexo + GitHub Pages + Next搭建个人博客的课程，大部分经验都是来自于网络，我会在整个过程后面附上参考的文章，一来总结搭建博客的过程，二来减少后来人踩坑。<br><a id="more"></a></p><h5 id="整个过程："><a href="#整个过程：" class="headerlink" title="整个过程："></a>整个过程：</h5><ul><li>1、注册Github账号及创建仓库</li><li>2、安装Git for Windows</li><li>3、配置Git</li><li>4、安装node.js</li><li>5、安装Hexo</li><li>6、使用next设计个性化博客</li><li>7、连接Hexo和Github Pages及部署博客</li><li>8、购买域名并解析<br>以上就是全部的过程，当然具体还有很多细节，比如更换配置、设置文章字数的单位，阅读时常的单位，设置评论区，具体的东西还是要依据个人的喜好调整，但是next主题里面基本都集成了这些功能，只要稍微调整下就行。</li></ul><h5 id="参考的文章："><a href="#参考的文章：" class="headerlink" title="参考的文章："></a>参考的文章：</h5><ul><li><a href="https://blog.csdn.net/wapchief/article/details/70801995" target="_blank" rel="noopener">参考的整个过程</a></li><li><a href="https://blog.csdn.net/qq_33699981/article/details/72716951" target="_blank" rel="noopener">各种个性化小功能</a></li><li><a href="https://blog.csdn.net/wangxw725/article/details/71602256?utm_source=itdadao&amp;utm_medium=referral" target="_blank" rel="noopener">给统计量添加单位</a></li><li><a href="https://www.jianshu.com/p/efbeddc5eb19?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation" target="_blank" rel="noopener">各种个性化设置</a></li><li><a href="https://www.jianshu.com/p/35e197cb1273" target="_blank" rel="noopener">文章发布</a></li><li><a href="https://blog.csdn.net/weixin_41196185/article/details/79234078" target="_blank" rel="noopener">GitHub/Coding双线部署</a></li><li><a href="https://www.jianshu.com/p/1edb4b42ff72" target="_blank" rel="noopener">小书匠Markdown使用手册</a></li><li><a href="https://www.jianshu.com/p/a0aa94ef8ab2" target="_blank" rel="noopener">在Markdown中输入数学公式(MathJax)</a></li><li><a href="https://blog.csdn.net/wgshun616/article/details/81019687" target="_blank" rel="noopener">Hexo 的 Next 主题中渲染 MathJax 数学公式</a></li><li><a href="https://www.leiyawu.com/2018/02/28/hexo-fs-SyncWriteStream-is-deprecated/" target="_blank" rel="noopener">报错：hexo fs.SyncWriteStream is deprecated</a></li><li><a href="https://www.jianshu.com/p/6f77c96b7eff" target="_blank" rel="noopener">Hexo Next主题博客功能完善</a></li><li><a href="https://blog.csdn.net/luyaxige/article/details/80193409" target="_blank" rel="noopener">MathJax语法</a></li><li><a href="https://www.cnblogs.com/linxd/p/4955530.html" target="_blank" rel="noopener">MathJax与LaTex介绍</a></li><li><a href="http://wangwlj.com/2017/10/08/mathjax_basic/" target="_blank" rel="noopener">MathJax(Markdown中的公式)的基本使用语法</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;才搭好博客，发现在博客发布文章确实比微信公众号方便很多，这里简略说下用 Hexo + GitHub Pages + Next搭建个人博客的课程，大部分经验都是来自于网络，我会在整个过程后面附上参考的文章，一来总结搭建博客的过程，二来减少后来人踩坑。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="总结" scheme="http://minhzou.top/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Hexo" scheme="http://minhzou.top/tags/Hexo/"/>
    
      <category term="GitHub Pages" scheme="http://minhzou.top/tags/GitHub-Pages/"/>
    
  </entry>
  
  <entry>
    <title>第一周 机器学习简介与线性回归</title>
    <link href="http://minhzou.top/2019/03/31/%E7%AC%AC%E4%B8%80%E5%91%A8%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://minhzou.top/2019/03/31/第一周 机器学习简介和线性回归/</id>
    <published>2019-03-31T12:37:15.065Z</published>
    <updated>2019-04-06T04:03:56.736Z</updated>
    
    <content type="html"><![CDATA[<h5 id="1-1-一个Kaggle竞赛优胜解决方案"><a href="#1-1-一个Kaggle竞赛优胜解决方案" class="headerlink" title="1.1 一个Kaggle竞赛优胜解决方案"></a>1.1 一个Kaggle竞赛优胜解决方案</h5><ul><li>一个Kaggle竞赛优胜解决方案<ul><li>任务：Avazu点击率预估竞赛</li><li>Rank 2nd Owen Zhang的解法<a id="more"></a></li><li>优胜算法的特点<ul><li>特征工程</li><li>融合大法<ul><li>多层</li><li>多种不同模型的组合</li></ul></li><li>所以：<ul><li>基础模型很重要（线性模型）</li><li>集成学习模型单模型性能好（GBDT）</li><li>特定问题的模型贡献大（FM）</li><li>模型融合很重要</li></ul></li></ul></li></ul></li><li>课程内容安排<ul><li>基本模型<ul><li>线性模型： 线性回归， logistic回归， SVM</li><li>非线性模型： （线性模型核化）、分类回归树</li><li>集成学习模型（随机森林、GBDT）</li><li>数据预处理：数据清洗，特征工程，降维，聚类</li></ul></li><li>模型融合</li><li>推荐系统/点击率预估问题特定解决方案</li></ul></li></ul><h5 id="1-2-机器学习任务类型"><a href="#1-2-机器学习任务类型" class="headerlink" title="1.2  机器学习任务类型"></a>1.2  机器学习任务类型</h5><ul><li>定义</li><li>数据<ul><li>数据通常以二维数据表形式给出<ul><li>每一行： 一个样本</li><li>每一列：一个属性/特征</li></ul></li><li>例：Boston房价预测数据，根据某地区房屋属性，预测该地区预测房价<ul><li>506行， 506个样本</li><li>14列</li></ul></li></ul></li><li>机器学习任务类型<ul><li>监督学习（Supervised Learning）<ul><li>分类（classfication）</li><li>回归（regression）</li><li>排序（ranking）</li></ul></li><li>非监督学习（unsupervised learning）<ul><li>聚类（clustering）</li><li>降维（dimensionality reduction）</li><li>概率密度估计（density estimation）</li></ul></li><li>增强学习（reinforcement learning）</li><li>半监督学习（semi-supervised learning）</li><li>迁移学习（transfer learning）</li><li>……</li></ul></li><li>监督学习<br>  学习一个x-&gt;y 的映射f, 从而对新输入的x进行预测f（x）<script type="math/tex; mode=display">D = \{X_i,y_i\}^N_{i=1}</script>  D：训练数据集<br>  N：训练样本数目<br>  $X_i$: 第i个样本的输入，亦被称为特征、属性或协变量<br>  $y_i$: 第i个训练样本的输出，亦被称为响应，如类别标签、序号或数值<br>  例：波士顿房价预测<ul><li>回归<ul><li>若输出y∈R为连续值，则我们称之为一个回归（regression）任务<br>例： 房价预测，预测二手车的价格</li><li>假设回归模型：$y = f(\mathbf x|\theta)$<ul><li>如在线性回归中，$f(\mathbf x|w) = \mathbf w^T \mathbf x$</li></ul></li><li>训练：根据训练数据 $D = \{\mathbf X_i,y_i\}^N_{i=1}$ 学习映射</li><li>预测：对新的测试数据x进行预测：$\hat f = f(x)$ y带帽表示预测</li><li>学习目标：训练集上预测值与真值之间的差异最小<ul><li>损失函数：度量模型预测值与真值之间的差异，如<script type="math/tex; mode=display">L(f(\mathbf x),y) = \frac 12(f(x) - y)^2</script></li><li>目标函数为：$J(\mathbf \theta) = \frac1N \sum_{i = 1}^N L(f(\mathbf x_i|\mathbf \theta), y_i)$</li></ul></li></ul></li><li>分类<br>   若输出y为离散值，则我们称之为一个分类，标签空间y = {1,2, … C}<br>   例：信用评分<ul><li>分类： 学习从输入x到输出y的映射f:概率问题<br>$\hat y = f(\mathbf x) = \underset{c}  {arg\ max} \ p(y = c\mid \mathbf x, D)$</li><li>学习目标：<ul><li>损失函数：01损失 <script type="math/tex; mode=display">l_{0/1}(y, \hat y) = \begin {cases} 0 & y = \hat y \\ 1 & otherwise \end{cases}</script></li></ul></li><li>需要预测的概率：</li><li>预测：最大后验估计（Maximum a Posteriori, MAP）<br>$\hat y = f(\mathbf x) = \underset{c} {arg\ max}\ p(y = c\mid \mathbf x, D)$</li></ul></li><li>排序（Rank）<br>  排序学习是推荐、搜素、广告的核心方法<br>  排序学习中需要首先根据查询q及其文档集合进行标注（data labeling） 和提取特征（feature extraction） 才能得到D = {….}</li></ul></li><li>非监督学习<br>  发现数据中的“有意义的模式”， 亦被称为知识发现<br>  训练数据不包含标签<br>  标签在训练数据中为隐含变量<br>  $ D = \{ \bf X_i\}_{ i= 1}^ N $<ul><li>聚类<br>例：人的“类型”<br>分多少类？ 模型选择<br>$ K^* = arg\ max _K\ p(K \mid D)$<br>某个样本属于哪个类？</li><li>降维<br>多维特征，有些特征之间会相关而存在冗余<br>很多算法中，降维算法成为了数据预处理的一部分， 如主成分分析（Principal Components Analysis, PCA）</li></ul></li><li>半监督学习<br>  当标注数据“昂贵”时有用<br>  例：标注3D姿态、 蛋白质功能等等</li><li>多标签学习</li><li>有歧义标签学习</li><li>多实例学习</li><li>增强学习<br>从行为的反馈(奖励或惩罚)中学习<ul><li>设计一个回报函数（reward function）， 如果learning agent(如机器人、围棋ai程序)，在决定一步之后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负</li><li>增强学习的任务：找到一条回报值最大的路径</li></ul></li></ul><h5 id="1-3-一个典型的机器学习案例-对鱼进行分类"><a href="#1-3-一个典型的机器学习案例-对鱼进行分类" class="headerlink" title="1.3 一个典型的机器学习案例-对鱼进行分类"></a>1.3 一个典型的机器学习案例-对鱼进行分类</h5><ul><li>根据一些光学传感器对传送带上的鱼进行分类</li><li>形式化为机器学习问题<ul><li>训练数据<ul><li>每条鱼的测量向量</li><li>每条鱼的标签</li></ul></li><li>测试<ul><li>给定一个新的特征向量x</li><li>预测对应的标签y </li></ul></li><li>将长度作为特征进行分类（直方图）<ul><li>需要先做一个决策边界<ul><li>最小化平均损失</li></ul></li></ul></li><li>将亮度作为特征进行分类 （直方图）</li><li>将长度和亮度一起作为特征（二维散点图）<ul><li>线性决策函数</li><li>二次决策函数</li><li>更复杂的决策边界<br>训练集上的误差 ≠ 测试集上的误差<br>数据过拟合（overfitting）<br>推广性（generalization）差</li></ul></li></ul></li><li>小结：设计一个鱼的分类器<ul><li>选择特征<ul><li>可能是最重要的步骤！（收集训练数据）</li></ul></li><li>选择模型（如决策边界的形状）</li><li>根据训练数据估计模型</li><li>利用模型对新样本进行分类</li></ul></li></ul><h5 id="1-4-机器学习算法的组成部分"><a href="#1-4-机器学习算法的组成部分" class="headerlink" title="1.4 机器学习算法的组成部分"></a>1.4 机器学习算法的组成部分</h5><ul><li>机器学习任务的一般步骤<ul><li>确定特征<ul><li>可能是最重要的步骤！（收集训练数据）</li></ul></li><li>确定模型<ul><li>目标函数/决策边界形状</li></ul></li><li>模型训练：根据训练数据估计模型参数<ul><li>优化计算</li></ul></li><li>模型评估：在校验集上评估模型预测性能</li><li>模型应用/预测 </li></ul></li><li>模型<ul><li>监督学习任务：$D = \{X_i, y_i\} _{i = 1} ^ N $</li><li>模型：对给定的输入x, 如何预测其标签$ \hat y$<ul><li>不同模型对数据的假设不同</li><li>最简单的模型：线性模型$ f(x) = \sum_j w_j x_j = \bf w^T \bf x$</li></ul></li><li>确定模型类别后，模型训练转化为求解模型参数<ul><li>如对线性模型参数为$\theta = \{w_j \mid j = 1,…, D\}$,其中D为特征维数</li></ul></li><li>求解模型参数：目标函数最小化</li></ul></li><li>非线性模型<ul><li>基函数： $x^2$, log, exp, 样条函数，决策树….</li><li>核化：将原问题转化为对偶问题，将对偶问题中的向量积$\langle x_i, x_j\rangle$ 换成核函数$k(x_i,x_j)$</li></ul></li><li>目标函数：通常包含两项：损失函数和正则项<script type="math/tex; mode=display">J(\theta) = \frac 1N \sum_{i=1}^N\ L(f(x_i; \theta), y_i) + R(\theta)</script><ul><li>损失函数<ul><li>损失函数 - 回归<ul><li>损失函数：度量模型预测值与真值之间的差异</li><li>对回归问题：令残差 $r = f(\bf x) - y$<ul><li>L2损失：连续，但对噪声敏感<script type="math/tex; mode=display">L_2 (r) = \frac 12 r ^2</script></li><li>L1损失：不连续，对噪声不敏感<script type="math/tex; mode=display">L_1(r) = |r|</script></li><li>Huber 损失： 连续，对噪声不敏感<script type="math/tex; mode=display">L_\delta (r) = \begin{cases}  \frac 12 r^2 & if|r| \le \delta\\ \delta |r| - \frac 12 \delta^2 & if|r| \ge \delta\end{cases}</script></li></ul></li></ul></li><li>损失函数 - 分类<ul><li>损失函数：度量模型预测值与真值之间的差异</li><li>对分类问题<ul><li>0-1损失：$l_{0/1}(y,f(x)) = \begin{cases} 1 &amp; yf(x) \lt 0 \\ 0 &amp; othereise\end{cases}$</li><li>logistic损失：亦称负log似然损失<br>  $l_{log}(y,f(x)) = log(1 + exp(-yf(x)))$</li><li>指数损失：$l_{exp}(y,f(x)) = exp(-yf(x))$</li><li>合页损失：$l_{hinge}(y,f(x)) = max(0, 1 - yf(x))$</li></ul></li></ul></li></ul></li><li>正则项<br>  复杂模型（预测）不稳定：方差大<br>  正则项对复杂模型施加惩罚<ul><li>正则项的必要性<br>例：sin曲线拟合</li><li>增加L2正则<br>岭回归：最小化RSS</li><li>欠拟合：模型太简单/对复杂性惩罚太多</li><li>样本数目增多时，可以考虑更复杂的模型</li><li>常见正则项<ul><li>L2正则: $R(\theta) = \lambda ||\theta||^2_2 = \lambda \sum^D_{j=1} \theta_j^2$</li><li>L1正则: $R(\theta) = \lambda |\theta| = \lambda \sum ^D_{j=1}|\theta_j|$</li><li>L0正则: $R(\theta) = \lambda||\theta||_ 0$<ul><li>非0参数的数目</li><li>不好优化，通常用L1正则近似</li></ul></li></ul></li><li>常见线性模型的损失和正则项组合</li></ul></li></ul></li></ul><div class="table-container"><table><thead><tr><th></th><th>L2损失</th><th>L1损失</th><th>Huber损失</th><th>Logistic损失</th><th>合页损失</th><th>e-insensitive损失</th></tr></thead><tbody><tr><td>L2正则</td><td>岭回归</td><td></td><td></td><td>L2正则 Logistic回归</td><td>SVM</td><td>SVR</td></tr><tr><td>L1正则</td><td>LASSO</td><td></td><td></td><td>L1正则 Logistic回归</td><td></td><td></td></tr><tr><td>L2+L1正则</td><td>Elastic</td><td></td><td></td><td></td><td></td></tr></tbody></table></div><ul><li>模型训练<ul><li>在训练数据上求目标函数极小值：优化</li><li>简单目标函数直接求解<ul><li>如小数据集上的线性回归</li></ul></li><li>更复杂问题：凸优化<ul><li>（随机）梯度下降</li><li>牛顿法/拟牛顿法</li><li>… </li></ul></li></ul></li><li>梯度下降（Gradient Descent）算法<ul><li>梯度下降/最速下降算法：快速寻找函数局部极小值</li><li>梯度下降算法：求函数J（θ）的最小值<ul><li>给定初始值$θ^0$</li><li>更新θ，使得J（θ）越来越小<ul><li>$θ^t = θ^{t-1} - \eta\nabla J(θ)$ ( $\eta$ : 学习率 )</li></ul></li><li>直到收敛到 / 达到预先设定的最大迭代次数</li><li>下降的步伐太小（学习率）非常重要：如果太小，收敛速度慢； 如果太大，可能会出现overshoot the minimum的现象</li><li>梯度下降求得的只是局部最小值<ul><li>二阶导数 &gt; 0, 则目标函数为凸函数，局部极小值即为全局最小值</li><li>随机选择多个初始值，得到函数的多个局部极小值点。多个局部极小值点的最小值为函数的全局最小值</li></ul></li><li>梯度下降算法每次学习都使用整个训练集，这样对大的训练数据集合，每次学习时间过长，对大的训练集需要消耗大量的内存。此时可采用随机梯度下降（Stochastic gradient descent, SGD), 每次从训练集中随机选择一部分样本进行学习。</li><li>更多（随机）梯度下降算法的改进版<ul><li>动量（Momentum）</li><li>Nesterov accelerated gradient (NAG)</li><li>Adagrad</li><li>RMSprop</li><li>Adaptive Moment Estimation (Adam)…</li></ul></li></ul></li></ul></li><li>模型选择与模型评估<ul><li>同一个问题有不同的解决方案<br>  如线性回归 vs. 决策树</li><li>哪个更好？ 模型评估与模型选择<ul><li>在新数据点的预测误差最小</li></ul></li><li>模型评估：已经选定最终的模型，估计它在新数据上的预测误差</li><li>模型选择：估计不同模型的性能，选出最好的模型</li></ul></li><li>样本足够多：训练集和校验集</li><li>样本不够多：重采样技术来模拟校验集：交叉验证和bootstrap<ul><li>K-折交叉验证<ul><li>交叉验证（Cross Validation, CV）： 将训练数据分成容量大致相等的K份（通常K = 5/10）</li><li>交叉验证估计的误差为：<script type="math/tex; mode=display">CV(M)= \frac1K \sum  ^K_{k = 1} E_k(M)</script></li></ul></li></ul></li><li>模型选择<ul><li>对多个不同模型，计算其对应的误差CV（M）， 最佳模型为CV（M）最小的模型</li><li>模型复杂度和泛化误差的关系通常是U形曲线：</li></ul></li></ul><h5 id="1-5-学习环境简介"><a href="#1-5-学习环境简介" class="headerlink" title="1.5 学习环境简介"></a>1.5 学习环境简介</h5><ul><li>编程语言 Python</li><li>数据处理工具包<ul><li>Numpy</li><li>SciPy</li><li>pandas</li></ul></li><li>数据可视化工具包<ul><li>Matplotlib</li><li>Seaborn</li></ul></li><li>机器学习工具包<ul><li>scikit learn</li></ul></li><li>示例代码：INotebook </li><li>NumPy<ul><li>NumPy(Numeric Python)是Python的开源数值计算扩展，可用来存储和处理大型矩阵</li><li>Numpy包括：<ul><li>N维数组(ndarray)</li><li>实用的线性代数、傅里叶变换和随机数生成函数</li></ul></li><li>Numpy和稀疏矩阵运算包SciPy配合使用更加方便</li></ul></li><li>SciPy<ul><li>SciPy是建立在NumPy的基础上、是科学和工程设计的Python工具包，提供统计、优化和数值微积分计算等功能</li><li>NumPy 处理$10^6$级别的数据通常没有大问题，但当数据量达到$10^7$级别时速度开始发慢，内存受到限制（具体情况取决于实际内存的大小）</li><li>当处理超大规模数据集，比如$10^{10}$级别，且数据中包含大量的0时，可采用稀疏矩阵可显著的提高速度和效率</li></ul></li><li>Pandas(<strong>Pan</strong>del <strong>da</strong>ta structures)<ul><li>Pandas是Python语言的“关系型数据库”数据结构和数据分析工具，非常高效且易于使用<ul><li>基于NumPy补充了大量数据操作功能，能实现统计、分组、排序、透视表(SQL语句的大部分功能)</li><li>Pandas主要有2种重要的数据类型<ul><li>series：一维序列</li><li>DataFrame：二维表(机器学习数据的常用数据结构)</li></ul></li></ul></li></ul></li><li>Matplotlib<ul><li>Matplotlib是Python语言的2D图形绘制工具</li></ul></li><li>Seaborn<ul><li>Seaborn是一个基于Matplotlib的Python可视化工具包，提供更高层次的用户接口，可以给出漂亮的数据统计</li></ul></li><li>Scikit - Learn<ul><li>Machine Learning in Python</li><li>Scikit-Learn是基于Python的开源机器学习模块，最早于2007年由David Cournapeau发起</li><li>基本功能有六部分：分类（Classification），回归（Regression），聚类（Clustering），数据降维（Dimensionality reduction），模型选择（Model Selection），数据预处理（Preprocessing）</li><li>对于具体的机器学习问题，通常可以分为三个步骤<ul><li>数据准备与预处理（Preprocessing, Dimensionality reduction）</li><li>模型选择与训练（Classification, Regression, Clustering）</li><li>模型验证与参数调优（Model Selection）</li></ul></li></ul></li><li>各种机器学习模型有统一的接口</li><li>模型既有默认参数，也提供多种参数调优方法</li><li>卓越的文档</li><li>丰富的随附任务功能集合</li><li>活跃的社区提供开发和支持</li></ul><h5 id="1-6-线性回归模型"><a href="#1-6-线性回归模型" class="headerlink" title="1.6 线性回归模型"></a>1.6 线性回归模型</h5><ul><li>目标函数通常包含两项：损失函数和正则项<script type="math/tex; mode=display">J(\bf \theta) = \frac1N \sum_{i = 1}^N L(f(\bf x_i|\bf \theta), y_i) + \lambda R(\bf \theta)</script></li><li>对回归问题，损失函数可以采用L2损失，得到<script type="math/tex; mode=display">\begin{eqnarray}J(\theta)    &=&\sum_{i=1}^NL(y_i,\hat y_i) \\   &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\   &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2  \end{eqnarray}</script>  残差平方和（residual sum of squares, RSS）</li><li>由于线性模型比较简单，实际应用中有时正则项为空，得到最小二乘线性回归（Ordinary Least Square, OLS）<script type="math/tex; mode=display">\begin{eqnarray}J(\theta)   &=&\sum_{i=1}^NL(y_i,\hat y_i)   &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\  &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2  \end{eqnarray}</script></li><li>正则项可以为L2正则，得到岭回归（Ridge Regression）<script type="math/tex; mode=display">J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda ||w||^2_2</script></li><li><p>正则项也可以选L1正则，得到Lasso模型：</p><script type="math/tex; mode=display">J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda |w|</script><ul><li>当$\lambda$取合适值时，Lasso（Least absolute shrinkage and selection operator）的结果是稀疏的（w的某些元素系数为0），起到特征选择作用</li></ul></li><li><p>为什么L1正则的解是稀疏的？</p></li><li>线性回归模型的概率解释<ul><li>最小二乘（线性）回归等价于极大似然估计<ul><li>假设：$ y = f(\bf x) + \epsilon = w^Tx + \epsilon $<br>其中$\epsilon$为线性预测和真值之间的残差<br>我们通常假设残差的分布为$\epsilon \sim N(0,\sigma ^2)$,因此线性回归可写成：$p(y|x,\theta) \sim N(y| \bf w^T \bf x, \sigma^2)$,其中$ \bf \theta = (\bf w, \sigma ^2)$</li></ul></li><li>正则（线性）回归等价于高斯先验（L2正则）或Laplace先验下（L1正则）的贝叶斯估计</li></ul></li><li>Recall：极大似然估计<ul><li>极大似然估计（Maximize Likelihood Estimator, MLE）定义为<script type="math/tex; mode=display">\hat \theta = \underset {\theta}  {arg\ max}\ log\ p(D\mid \theta)</script></li><li>其中（log）似然函数为<script type="math/tex; mode=display">l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta)</script></li><li>表示在参数为$\theta$的情况下，数据$D ={\bf x_i,y_i}^N_{i=1}$</li><li>极大似然：选择数据出现概率最大的参数</li></ul></li><li>线性回归的MLE<script type="math/tex; mode=display">p(y_i|x_i,\bf w,\sigma ^2) \sim N(y_i\mid \bf w^T \bf x_i, \sigma^2) = \frac 1{\sqrt{2\pi}\sigma} exp(-\frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))</script><ul><li>OLS的似然函数为<script type="math/tex; mode=display">l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta)</script></li><li>极大似然可等价地写成极小负log似然损失（negative log likelihood, NLL）</li></ul></li></ul><script type="math/tex; mode=display">\begin{eqnarray}{NLL(\bf \theta)}    &=& \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) \\    &=& - \sum_{i=1}^N log ((\frac 1{2 \pi \sigma^2})^ \frac 12 exp(- \frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))) \\    &=& \frac N2 log(2\pi \sigma ^2) + \frac 1{2 \sigma^2} \sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2    \end{eqnarray}</script><ul><li>正则回归等价于贝叶斯估计<ul><li>假设残差的分布为$\epsilon \sim N(0, \sigma ^2)$,线性回归可写成：<br>$p(y_i \mid \bf x_i, \theta) \sim N(y_i \mid \bf w^T \bf x_i，\sigma ^2)$<br>$p(y\mid \bf X, \bf w, \sigma ^2) = N(\bf y \mid \bf X \bf w, \sigma ^2 \bf I_N) \propto exp(- \frac 1{2\sigma ^2}((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w)))$</li><li>若假设参数为w的先验分布为 $w_j \sim N(0, \tau ^2)$<ul><li>偏向较小的系数值，从而得到的曲线也比较平滑<br>$p(\bf w) =\prod_{j=1}^{D} N(w_j \mid 0, \tau ^2) \propto exp(- \frac 1{2\tau^2} \sum_{j=1}^D \bf w_j^2 = exp(- \frac 1{2\tau^2} ( \bf w^T \bf w ) )) $</li><li>其中$1/\tau ^2$控制先验的强度</li></ul></li><li>根据贝叶斯公式，得到参数的后验分布为<br>$p(y\mid \bf X, \bf w, \sigma ^2) = \propto exp(- \frac 1{2\sigma ^2} ((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w) ) - \frac 1{2 \tau^2} ( w^Tw ) )$</li><li>则最大后验估计(MAP)等价于最小目标函数<br>$J(\bf w) = (\bf y - \bf X\bf w)^T(\bf y - \bf X\bf w) + \frac {\sigma ^2}{\tau^2} \bf w^T \bf w $</li><li>对比岭回归的目标函数<br>$J(\bf w) = \sum_{i=1}^N(y_i -\bf w^T\bf x_i)^2 + \lambda \Vert \bf w\Vert ^2_2$</li></ul></li><li>小结<ul><li>线性回归模型可以放到机器学习一般框架<ul><li>损失函数：L2损失，…</li><li>正则：无正则， L2正则，L1正则…</li></ul></li><li>正则回归模型可视为先验为正则、似然为高斯分布的贝叶斯估计<ul><li>L2正则：先验分布为高斯分布</li><li>L1正则：先验分布为Laplace分布</li></ul></li></ul></li></ul><h5 id="1-7-线性回归模型-优化算法"><a href="#1-7-线性回归模型-优化算法" class="headerlink" title="1.7 线性回归模型-优化算法"></a>1.7 线性回归模型-优化算法</h5><ul><li>线性回归的目标函数<ul><li>无正则的最小二乘线性回归（Ordinary Least Square, OLS）：<script type="math/tex; mode=display">J(w) = \sum_{i=1}^N(y_i - w^Tx_i)^2</script></li><li>L2正则的岭回归（Ridge Regression）模型：<script type="math/tex; mode=display">J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D w_j^2</script></li><li>L1正则的Lasso模型：<script type="math/tex; mode=display">J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D |w_j|</script></li></ul></li><li>模型训练：<br>  根据训练数据求目标函数取极小值的参数：<br>  $\hat w = \underset {w} {arg\ min} J(\bf w)$<ul><li>目标函数的最小值：<ul><li>一阶的导数为0：$\frac{\partial J(w)} {\partial w}$</li><li>二阶导数&gt;0：$\frac{\partial J^2(w)} {\partial w^2}$</li></ul></li></ul></li><li>OLS的优化求解：<ul><li>OLS的优化求解<ul><li>OLS的目标函数写成矩阵形式：<br>$J(w) = \sum ^N_{i=1}(y_i - w^Tx_i)^2 = (y - Xw)^T(y - Xw)$</li><li>只取与w有关的项，得到<br>$J(w) = w^T(X^TX)w - 2w^T(X^Ty)$</li><li>求导  $\frac{\partial J(w)} {\partial w} = 2X^TXw - 2X^Ty = 0 \Rightarrow X^TXw = X^Ty$`<br>$\hat w_{OLS} = (X^TX)^{-1}X^Ty$</li></ul></li><li>OLS的优化求解 ——SVD<ul><li>OLS目标函数：$J(w) = \Vert y - Xw\Vert_2^2$</li><li>相当于求 $y = Xw$</li><li>如果X为方阵，可求逆：$w = X^{-1}y$</li><li>如果𝐗不是方阵，可求Moore-Penrose广义逆：$𝐰 = 𝐗^{\dagger }𝐲$。</li><li>Moore-Penrose广义逆可采用奇异值分解(Singular Value Decomposition)<br>实现：<br>奇异值分解：$X = U \Sigma V^T$<br>$X^{\dagger } = V \Sigma ^{\dagger} U^T$<br>其中 $\Sigma = \begin{pmatrix}<br>{\sigma_1}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>{0}&amp;{\sigma_2}&amp;{\cdots}&amp;{0}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{0}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>\end{pmatrix}$,$\Sigma ^{\dagger} = \begin{pmatrix}<br>{\frac {1}{\sigma_1}}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>{0}&amp;{\frac{1}{\sigma_2}}&amp;{\cdots}&amp;{0}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{0}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>\end{pmatrix}$</li></ul></li><li>OLS的优化求解——梯度下降<ul><li>OLS目标函数：<br>$J(w) = (y - Xw)^T(y - Xw)$<br>梯度：$\nabla_w = - 2X^T(y - Xw^t)$<br>参数更新：<br>$w^{t+1} = w^t - \eta\nabla_w = w^t + 2\eta X^T(y - Xw^t)$</li></ul></li></ul></li><li>岭回归的优化求解<ul><li>岭回归的目标函数与OLS只相差一个正则项（也是w的二次函数）</li><li>岭回归的优化求解——SVD</li></ul></li><li>Lasso的优化条件<ul><li>软&amp; 硬阈值</li><li>Lasso的优化求解——坐标轴下降法<ul><li>为了找到一个函数的局部极小值，在每次迭代中可以在当前点处沿一个坐标方向进行一维搜索。</li></ul></li><li>整个过程中循环使用不同的坐标方向。一个周期的一维搜索迭代过程相当于一个梯度迭代。</li><li>注意：<ul><li>梯度下降方法是利用目标函数的导数（梯度）来确定搜索方向的，而该梯度方向可能不与任何坐标轴平行。</li><li>而坐标轴下降法是利用当前坐标系统进行搜索，不需要求目标函数的导数，只按照某一坐标方向进行搜索最小值。（在稀疏矩阵上的计算速度非常快，同时也是Lasso回归最快的解法）</li></ul></li></ul></li><li>小结<ul><li>线性回归模型比较简单<ul><li>当数据规模比较小时，可直接解析求解<ul><li>scikit learn中的实现采用SVD分解实现</li></ul></li><li>当数据规模较大时，可采用随机梯度下降<ul><li>scikit learn提供一个SGDRegression类</li></ul></li></ul></li><li>岭回归求解类似OLS，采用SVD分解实现</li><li>Lasso优化求解采用坐标轴下降法</li></ul></li></ul><h5 id="1-8-线性回归模型-模型选择"><a href="#1-8-线性回归模型-模型选择" class="headerlink" title="1.8 线性回归模型-模型选择"></a>1.8 线性回归模型-模型选择</h5><ul><li>模型评估与模型选择<ul><li>模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果<ul><li>校验集划分（train_test_split、交叉验证）</li><li>评价指标（sklearn.metrics）</li></ul></li><li>模型选择：<ul><li>模型中通常有一些超参数，需要通过模型选择来确定<ul><li>线性回归模型中的正则参数</li><li>OLS中的特征的数目</li></ul></li><li>参数搜索范围：网格搜索（GridSearch）</li></ul></li><li>Scikit learn将交叉验证与网格搜索合并为一个函数</li></ul></li><li>评价准则<ul><li>模型训练好后，可用一些度量准则检查模型拟合的效果<ul><li>开方均方误差（rooted mean squared error，RMSE）:$RMSE = \sqrt{\frac 1N \sum_{i=1}^N(\hat y_i - y_i)^2}$`</li><li>平均绝对误差（mean absolute error，MAE）：$MAE = \frac 1N \sum_{i=1}^N|\hat y_i - y_i|$</li><li>R2 score：既考虑了预测值与真值之间的差异，也考虑了问题本身真值之<br>间的差异（ scikit learn 线性回归模型的缺省评价准则）$SS_{res} = \sum_{i=1}^N(\hat y_i - y_i)^2, SStot = \sum_{i=1}^N(y_i - \bar{y})^2, R^2 = 1 - \frac {SS_{res}}{SS_{tot}})$</li></ul></li><li>也可以检查残差的分布</li><li>还可以打印预测值与真值的散点图</li></ul></li><li>线性回归中的模型选择<br>Scikit learn中的model selection模块提供模型选择功能<ul><li>对于线性模型，留一交叉验证（N折交叉验证，亦称为leave-oneout cross-validation，LOOCV）有更简便的计算方式，因此Scikit learn提供了RidgeCV类和LassoCV类实现了这种方式</li><li>后续课程将讲述一般模型的交叉验证和参数调优GridSearchCV</li><li>RidgeCV<ul><li>RidgeCV中超参数λ用alpha表示</li><li>RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_c<br>v_values=False)</li></ul></li><li>LassoCV<ul><li>LassoCV的使用与RidgeCV类似</li><li>Scikit learn还提供一个与Lasso类似的LARS（least angle regression，最小角回归），二者仅仅是优化方法不同，目<br>标函数相同。</li><li>当数据集中特征维数很多且存在共线性时，LassoCV更合适。</li></ul></li></ul></li><li>小结：线性回归之模型选择<ul><li>采用交叉验证评估模型预测性能，从而选择最佳模型<ul><li>回归性能的评价指标</li><li>线性模型的交叉验证通常直接采用广义线性模型的留一交叉验证进行快速模型评估<ul><li>Scikit learn中对RidgeCV和LassoCV实现该功能</li></ul></li></ul></li></ul></li></ul><h5 id="1-9-波士顿房价预测案例详解——数据探索"><a href="#1-9-波士顿房价预测案例详解——数据探索" class="headerlink" title="1.9 波士顿房价预测案例详解——数据探索"></a>1.9 波士顿房价预测案例详解——数据探索</h5><ul><li>第一步：理解任务，准备数据<ul><li>数据读取<ul><li>Pandas支持多种格式的数据</li></ul></li><li>数据探索&amp;特征工程<ul><li>数据规模</li><li>确定数据类型，是否需要进一步编码<ul><li>特征编码</li></ul></li><li>数据是否有缺失值<ul><li>数据填补</li></ul></li><li>查看数据分布，是否有异常数据点<ul><li>离群点处理</li></ul></li><li>查看两两特征之间的关系，看数据是否有冗余/相关<ul><li>降维</li></ul></li></ul></li><li>数据概览<ul><li>pandas:DataFrame<ul><li>Head():数据前5行，可查看每一列的名字及数据类型</li><li>Info():<ul><li>数据规模：行数&amp;列数</li><li>每列的数据类型、是否有空值</li><li>占用存储量</li></ul></li><li>shape:行数&amp;列数</li></ul></li></ul></li><li>各属性的统计特性<ul><li>直方图<br>  每个取值在数据集中出现的样本数目 </li><li>离群点<ul><li>离群点：奇异点（outlier）,指远离大多数样本的样本点。通常认为这些点是噪声，对模型有坏影响</li></ul></li><li>相关性<ul><li>相关性：相关性可以通过计算相关系数或打印散点图来发现</li><li>相关系数：</li><li>散点图<ul><li>可以通过两个变量之间的散点图直观感受二者的相关性</li></ul></li><li>数据预处理<ul><li>数据标准化（ Standardization ）<ul><li>某个特征的所有样本取值为0均值、1方差</li></ul></li><li>数据归一化（ Scaling ）<ul><li>某个特征的所有样本取值在规定范围内</li></ul></li><li>数据正规化（ Normalization ）<ul><li>每个样本模长为1</li></ul></li><li>数据二值化<ul><li>根据特征值取值是否大于阈值将特征值变为0或1，可用类Binarizer 实现</li></ul></li><li>数据缺失</li><li>数据类型变换<ul><li>有些模型只能处理数值型数据。如果给定的数据是不同的类型，必须先将数据<br>变成数值型。</li></ul></li></ul></li></ul></li></ul></li></ul></li><li>第二步：模型确定和模型训练<ul><li>1、确定模型类型<ul><li>目标函数（损失函数、正则）</li></ul></li><li>2、模型训练<ul><li>优化算法（解析法，梯度下降、随机梯度下降…）</li></ul></li></ul></li><li>第三步：模型评估与模型选择<ul><li>模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果<ul><li>校验集划分（train_test_split、交叉验证）</li><li>评价指标 （sklearn.metics）</li><li>也可以检查残差的分布</li><li>还可以打印预测值与真值的散点图</li></ul></li><li>模型选择：选择预测性能最好的模型<ul><li>模型中通常有一些超参数，需要通过模型选择来确定</li><li>参数搜索范围：网格搜索（GridSearch）</li></ul></li></ul></li></ul><h5 id="1-10-波士顿房价预测-数据探索代码"><a href="#1-10-波士顿房价预测-数据探索代码" class="headerlink" title="1.10 波士顿房价预测-数据探索代码"></a>1.10 波士顿房价预测-数据探索代码</h5><figure class="highlight python"><figcaption><span>python 3.7</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">"boston_housing.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据探索</span></span><br><span class="line">print(data.head())</span><br><span class="line">data.info()</span><br><span class="line">print(data.isnull().sum())</span><br><span class="line">print(data.describe())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标y(房屋价格)的直方图/分布</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">sns.distplot(data.MEDV.values, bins=<span class="number">30</span>, kde=<span class="literal">True</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Median value of owner_occupied homes'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单个特征散点图</span></span><br><span class="line">plt.scatter(range(data.shape[<span class="number">0</span>]), data[<span class="string">"MEDV"</span>].values, color=<span class="string">'purple'</span>)</span><br><span class="line">plt.title(<span class="string">"Distribution of Price"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除y大于50的样本</span></span><br><span class="line">data = data[data.MEDV &lt; <span class="number">50</span>]</span><br><span class="line">print(data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入属性的直方图／分布</span></span><br><span class="line"><span class="comment"># 犯罪率特征</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">sns.distplot(data.CRIM.values, bins=<span class="number">30</span>, kde=<span class="literal">False</span>)</span><br><span class="line">plt.xlabel(<span class="string">'crime rate'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否靠近charles river</span></span><br><span class="line">sns.countplot(data.CHAS, order=[<span class="number">0</span>, <span class="number">1</span>]);</span><br><span class="line">plt.xlabel(<span class="string">'Charles River'</span>);</span><br><span class="line">plt.ylabel(<span class="string">'Number of occurrences'</span>);</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 靠近高速</span></span><br><span class="line">sns.countplot(data.RAD)</span><br><span class="line">plt.xlabel(<span class="string">'index of accessibility to radial highways'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两两特征之间的相关性</span></span><br><span class="line"><span class="comment"># 获得所有列的名字</span></span><br><span class="line">cols = data.columns</span><br><span class="line"><span class="comment"># 计算相关性</span></span><br><span class="line">data_corr = data.corr().abs()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相关性热图</span></span><br><span class="line">plt.subplots(figsize=(<span class="number">13</span>, <span class="number">9</span>))</span><br><span class="line">sns.heatmap(data_corr, annot=<span class="literal">True</span>)</span><br><span class="line">sns.heatmap(data_corr, mask=data_corr &lt; <span class="number">1</span>, cbar=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">'house_coor.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出强相关对</span></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">corr_list = []</span><br><span class="line">size = data_corr.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, size):</span><br><span class="line">        <span class="keyword">if</span> (data_corr.iloc[i,j] &gt;= threshold <span class="keyword">and</span> data_corr.iloc[i, j] &lt; <span class="number">1</span>) <span class="keyword">or</span> (data_corr.iloc[i, j] &lt; <span class="number">0</span> <span class="keyword">and</span> data_corr.iloc &lt;= -threshold):</span><br><span class="line">            corr_list.append([data_corr.iloc[i, j], i, j])</span><br><span class="line">s_corr_list = sorted(corr_list, key=<span class="keyword">lambda</span> x: -abs(x[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> v, i, j <span class="keyword">in</span> s_corr_list:</span><br><span class="line">    print(<span class="string">"%s and %s = %.2f"</span> % (cols[i], cols[j], v))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> v, i, j <span class="keyword">in</span> s_corr_list:</span><br><span class="line">    sns.pairplot(data, height=<span class="number">6</span>, x_vars=cols[i], y_vars=cols[j])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h5 id="1-11-波士顿房价预测案例详解"><a href="#1-11-波士顿房价预测案例详解" class="headerlink" title="1.11 波士顿房价预测案例详解"></a>1.11 波士顿房价预测案例详解</h5><h5 id="1-12-波士顿房价预测案例详解-代码讲解"><a href="#1-12-波士顿房价预测案例详解-代码讲解" class="headerlink" title="1.12 波士顿房价预测案例详解-代码讲解"></a>1.12 波士顿房价预测案例详解-代码讲解</h5><figure class="highlight python"><figcaption><span>python 3.7</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 波士顿房价预测案例——线性回归分析</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 矩阵操作</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># SQL数据处理</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score <span class="comment"># 评价回归预测模型的性能</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">"boston_housing.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、数据准备</span></span><br><span class="line"><span class="comment"># 从原始数据中分离输入特征x和输出y</span></span><br><span class="line">y = data[<span class="string">'MEDV'</span>].values</span><br><span class="line">X = data.drop(<span class="string">'MEDV'</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于后续显示权重系数对应的特征</span></span><br><span class="line">columns = X.columns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据较少，将数据分割训练数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机采样20%的数据构建测试样本，其余作为训练样本</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">33</span>,test_size=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># print(X_train.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、数据预处理/特征工程</span></span><br><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别初始化对特征和目标值的标准化器</span></span><br><span class="line">ss_X = StandardScaler()</span><br><span class="line">ss_y = StandardScaler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别对训练和测试数据的特征以及目标值进行标准化处理</span></span><br><span class="line">X_train = ss_X.fit_transform(X_train)</span><br><span class="line">X_test = ss_X.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对y标准化不是必须</span></span><br><span class="line"><span class="comment"># 对y标准化的好处是不同的问题的w差异不太大，同时正则参数的范围也有限</span></span><br><span class="line">y_train = ss_y.fit_transform(y_train.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">y_test = ss_y.transform(y_test.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、确定模型类型</span></span><br><span class="line"><span class="comment"># 3.1 尝试缺省参数的线性回归</span></span><br><span class="line"><span class="comment"># 线性回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用默认配置初始化</span></span><br><span class="line">lr = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型参数</span></span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_test_pred_lr = lr.predict(X_test)</span><br><span class="line">y_train_pred_lr = lr.predict(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看各特征的权重系数，系数的绝对值大小可视为该特征的重要性</span></span><br><span class="line">fs = pd.DataFrame(&#123;<span class="string">"columns"</span>: list(columns), <span class="string">"coef"</span>: list((lr.coef_.T))&#125;)</span><br><span class="line">fs.sort_values(by=[<span class="string">'coef'</span>], ascending=<span class="literal">False</span>)</span><br><span class="line">print(fs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">print(<span class="string">'The r2 score of LinearRegression on test is'</span>, r2_score(y_test, y_test_pred_lr))</span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">print(<span class="string">'The r2 score of LinearRegression on train is'</span>, r2_score(y_train, y_train_pred_lr))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练集上观察残差的分布，看是否符合模型假设：噪声为0均值的高斯噪声</span></span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">7</span>, <span class="number">5</span>))</span><br><span class="line">f.tight_layout()</span><br><span class="line">ax.hist(y_train - y_train_pred_lr, bins=<span class="number">40</span>, label=<span class="string">'Residuals Linear'</span>, color=<span class="string">'b'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">ax.set_title(<span class="string">"Histogram of Residuals"</span>)</span><br><span class="line">ax.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 还可以观察预测值与真值的散点图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">plt.scatter(y_train, y_train_pred_lr)</span><br><span class="line">plt.plot([<span class="number">-3</span>, <span class="number">3</span>],[<span class="number">-3</span>, <span class="number">3</span>], <span class="string">'--k'</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'True price'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Predicted price'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性模型，随机梯度下降优化模型参数</span></span><br><span class="line"><span class="comment"># 随机梯度下降一般在大数据集上应用，其实本项目不适合用</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment">#  使用默认配置初始化线</span></span><br><span class="line">sgdr = SGDRegressor(max_iter=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练：参数估计</span></span><br><span class="line">sgdr.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">sgdr.coef_</span><br><span class="line">print(<span class="string">'The value of default measurement of SGDRegressor on test is'</span>, sgdr.score(X_test, y_test))</span><br><span class="line">print(<span class="string">'The value of default measurement of SGDRegressor on train is'</span>, sgdr.score(X_train, y_train))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 正则化的线性回归（L2正则--&gt;岭回归）</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置超参数（正则参数）范围</span></span><br><span class="line">alphas = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个RidgeCV</span></span><br><span class="line">ridge = RidgeCV(alphas=alphas, store_cv_values=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">ridge.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_test_pred_ridge = ridge.predict(X_test)</span><br><span class="line">y_train_pred_ridge = ridge.predict(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估，使用r2_score评价模型在测试集和训练集上的性能</span></span><br><span class="line">print(<span class="string">'The r2 score of RidgeCV on test is'</span>, r2_score(y_test, y_test_pred_ridge))</span><br><span class="line">print(<span class="string">'The r2 score of RidgeCV on test is'</span>, r2_score(y_train, y_train_pred_ridge))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">mse_mean = np.mean(ridge.cv_values_, axis=<span class="number">0</span>)</span><br><span class="line">plt.plot(np.log10(alphas), mse_mean.reshape(len(alphas), <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])</span></span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'log(alpha)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'mse'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'alpha is:'</span>, ridge.alpha_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看各特征的权重系数，系数的绝对值大小可视为该特制的重要性</span></span><br><span class="line">fs = pd.DataFrame(&#123;<span class="string">"columns"</span>: list(columns), <span class="string">"coef_lr"</span>: list(lr.coef_.T), <span class="string">"coef_ridge"</span>: list(ridge.coef_.T)&#125;)</span><br><span class="line">fs.sort_values(by=[<span class="string">'coef_lr'</span>], ascending=<span class="literal">False</span>)</span><br><span class="line">print(fs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.3 正则化的线性回归（L1正则--&gt;Lasso）</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个LassoCV实例</span></span><br><span class="line">lasso = LassoCV()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练（内含CV）</span></span><br><span class="line">lasso.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">y_test_pred_lasso = lasso.predict(X_test)</span><br><span class="line">y_train_pred_lasso = lasso.predict(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估， 使用r2_score评价模型在测试集和训练集上的性能</span></span><br><span class="line">print(<span class="string">'The r2 score of LassoCV on test is'</span>, r2_score(y_test, y_test_pred_lasso))</span><br><span class="line">print(<span class="string">'The r2 score of LassoCV on train is'</span>, r2_score(y_train, y_train_pred_lasso))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">mses = np.mean(lasso.mse_path_, axis=<span class="number">1</span>)</span><br><span class="line">plt.plot(np.log10(lasso.alphas_), mses)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])</span></span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'log(alpha)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'mse'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'alpha is:'</span>, lasso.alpha_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看各特征的权重系数，系数的绝对值大小可视为该特制的重要性</span></span><br><span class="line">fs = pd.DataFrame(&#123;<span class="string">"columns"</span>: list(columns), <span class="string">"coef_lr"</span>: list(lr.coef_.T), <span class="string">"coef_ridge"</span>: list(lasso.coef_.T)&#125;)</span><br><span class="line">fs.sort_values(by=[<span class="string">'coef_lr'</span>], ascending=<span class="literal">False</span>)</span><br><span class="line">print(fs)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;1-1-一个Kaggle竞赛优胜解决方案&quot;&gt;&lt;a href=&quot;#1-1-一个Kaggle竞赛优胜解决方案&quot; class=&quot;headerlink&quot; title=&quot;1.1 一个Kaggle竞赛优胜解决方案&quot;&gt;&lt;/a&gt;1.1 一个Kaggle竞赛优胜解决方案&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;一个Kaggle竞赛优胜解决方案&lt;ul&gt;
&lt;li&gt;任务：Avazu点击率预估竞赛&lt;/li&gt;
&lt;li&gt;Rank 2nd Owen Zhang的解法&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://minhzou.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习笔记" scheme="http://minhzou.top/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="人工智能" scheme="http://minhzou.top/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="线性回归" scheme="http://minhzou.top/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>计算机视觉基础入门 学习笔记</title>
    <link href="http://minhzou.top/2019/03/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/"/>
    <id>http://minhzou.top/2019/03/30/计算机视觉基础入门/</id>
    <published>2019-03-30T11:22:46.082Z</published>
    <updated>2019-04-04T15:45:41.583Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、-计算机视觉和深度学习概述"><a href="#一、-计算机视觉和深度学习概述" class="headerlink" title="一、 计算机视觉和深度学习概述"></a>一、 计算机视觉和深度学习概述</h5><ol><li>计算机视觉回顾<ol><li>计算机视觉（computer vision）定义<ul><li>数据（静态图片，视频）</li><li>算法（机器学习算法，神经网络）本质上是一个回归+分类</li></ul></li><li>计算机视觉的重要性<a id="more"></a><ul><li>三大任务：图像识别（image classification）<br>车牌识别，人脸识别</li><li>三大任务：目标检测（object detection = classification + localization）<br>行人检测和车辆检测</li><li>三大任务：图像分割<br>图像语义分割<br>个体分割 = 检测 + 分割</li><li>视觉目标跟踪（tracking）</li><li>视频分割</li><li>图像风格迁移</li><li>生成对抗网络（GAN）</li><li>视频生成</li></ul></li></ol></li><li>深度学习介绍<br> 2006 Hinton bp(反向传播)<br> 2012 Krizhevsky A 深度学习 深度卷积<br> RNN<br> LSTM 持续信息<br> 视觉识别，语音识别，DeepMind, AlphaGo<br> 人脸识别：LFW 错误率5% -&gt; 0.5%<br> 图像分割<br> VGGNet, GoogleNet, ResNet, DenseNet<ul><li>常见的深度学习开发平台<br>  Torch, TensorFlow, MatConvNetTheano, Caffe</li></ul></li><li>课程介绍<ul><li>图像识别：<br>Alexnet, VGGnet, GoogleNet, ResNet, DenseNet</li><li>目标检测<br>Fast-rcnn, faster-rcnn, Yolo, Retina-Net</li><li>图像分割<br>FCN, Mask-Rcnn</li><li>目标跟踪<br>GORURN， ECO</li><li>图像生成<br>GAN， WGAN</li><li>光流<br>FlowNet</li><li>视频分割<br>Segnet <h5 id="二、-图像分类与深度卷积网络的模型"><a href="#二、-图像分类与深度卷积网络的模型" class="headerlink" title="二、 图像分类与深度卷积网络的模型"></a>二、 图像分类与深度卷积网络的模型</h5></li></ul></li><li>图像分类<ul><li>图像分类的挑战<br>光照变化<br>形变<br>类内变化</li><li>图像分类定义</li><li>目标分类框架</li><li>泛化能力<br>如何提高泛化能力？ 需要用图像特征来描述图像</li><li>训练和测试的流程</li><li>图像特征<br>  color: Qutantize RGB values<br>  global shape: PCA space<br>  local shape: shape context<br>  texture: Filter banks<br>  SIFT, Hog, LBP, Harr</li><li>支持向量机（SVM）<br>   超平面与支持向量<br>   最大化间隔<br>   svm分类（python）以lris兰花分类为例<br>   程序实现</li><li>更好的特征<br>  CNN特征<br>  学习出来的<br>  如何学习？ 构造神经网络</li></ul></li><li>神经网络原理<ul><li>神经网络做图像分类</li><li>神经网络搭建</li><li>神经网络的基本单元：神经元</li><li>激励函数<br>  Sigmoid、tanh、ReLU、Leaky ReLU、Maxout、ELU</li><li>卷积层</li><li>卷积滤波的计算</li><li>卷积层可视化</li><li>池化层（pooling layer）<br>  特征表达更加紧凑，同时具有位移不变性</li><li>全连接层</li><li>损失函数<br>  交叉熵损失函数（SIGMOID_CROSS_ENTROPY_LOSS) 应用于二分类问题<br>  Softmax 损失函数（SOFTMAX_LOSS)  多分类问题<br>  欧式距离损失函数（EUCLIDEAN_LOSS）回归问题<br>  对比损失函数（Contrastive loss）用来计算两个图像之间的相似度<br>  Triplet loss</li><li>训练网络</li><li>网络训练和测试</li></ul></li><li>卷积神经网络介绍<br> Alexnet, VGGnet, GoogleNet, ResNet, DenseNet<ul><li>训练技巧， 防止过拟合（泛化能力不强）<ul><li>数据增强（Data augmentation）<br>  水平翻转， 随机裁剪和平移变换，颜色、光照变换</li><li>Dropout</li></ul></li><li>其他有助于训练的手段<ul><li>L1， L2正则化</li><li>Batch Normalization</li></ul></li></ul></li><li>利用caffe搭建深度网络做图像分类</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;一、-计算机视觉和深度学习概述&quot;&gt;&lt;a href=&quot;#一、-计算机视觉和深度学习概述&quot; class=&quot;headerlink&quot; title=&quot;一、 计算机视觉和深度学习概述&quot;&gt;&lt;/a&gt;一、 计算机视觉和深度学习概述&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;计算机视觉回顾&lt;ol&gt;
&lt;li&gt;计算机视觉（computer vision）定义&lt;ul&gt;
&lt;li&gt;数据（静态图片，视频）&lt;/li&gt;
&lt;li&gt;算法（机器学习算法，神经网络）本质上是一个回归+分类&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;计算机视觉的重要性&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="基础" scheme="http://minhzou.top/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="计算机视觉" scheme="http://minhzou.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="学习笔记" scheme="http://minhzou.top/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
