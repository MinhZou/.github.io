<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Minh&#39;s Blog</title>
  
  <subtitle>成长的路上每一步都算数</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://minhzou.top/"/>
  <updated>2019-04-02T08:05:09.298Z</updated>
  <id>http://minhzou.top/</id>
  
  <author>
    <name>Minh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hexo + GitHub Pages + Next在windows下搭建个人博客</title>
    <link href="http://minhzou.top/2019/04/01/Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://minhzou.top/2019/04/01/Hexo搭建博客/</id>
    <published>2019-04-01T03:43:22.257Z</published>
    <updated>2019-04-02T08:05:09.298Z</updated>
    
    <content type="html"><![CDATA[<p>才搭好博客，发现在博客发布文章确实比微信公众号方便很多，这里简略说下用 Hexo + GitHub Pages + Next搭建个人博客的课程，大部分经验都是来自于网络，我会在整个过程后面附上参考的文章，一来总结搭建博客的过程，二来减少后来人踩坑。<br><a id="more"></a></p><h5 id="整个过程："><a href="#整个过程：" class="headerlink" title="整个过程："></a>整个过程：</h5><ul><li>1、注册Github账号及创建仓库</li><li>2、安装Git for Windows</li><li>3、配置Git</li><li>4、安装node.js</li><li>5、安装Hexo</li><li>6、使用next设计个性化博客</li><li>7、连接Hexo和Github Pages及部署博客</li><li>8、购买域名并解析<br>以上就是全部的过程，当然具体还有很多细节，比如更换配置、设置文章字数的单位，阅读时常的单位，设置评论区，具体的东西还是要依据个人的喜好调整，但是next主题里面基本都集成了这些功能，只要稍微调整下就行。</li></ul><h5 id="参考的文章："><a href="#参考的文章：" class="headerlink" title="参考的文章："></a>参考的文章：</h5><ul><li><a href="https://blog.csdn.net/wapchief/article/details/70801995" target="_blank" rel="noopener">参考的整个过程</a></li><li><a href="https://blog.csdn.net/qq_33699981/article/details/72716951" target="_blank" rel="noopener">各种个性化小功能</a></li><li><a href="https://blog.csdn.net/wangxw725/article/details/71602256?utm_source=itdadao&amp;utm_medium=referral" target="_blank" rel="noopener">给统计量添加单位</a></li><li><a href="https://www.jianshu.com/p/efbeddc5eb19?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation" target="_blank" rel="noopener">各种个性化设置</a></li><li><a href="https://www.jianshu.com/p/35e197cb1273" target="_blank" rel="noopener">文章发布</a></li><li><a href="https://blog.csdn.net/weixin_41196185/article/details/79234078" target="_blank" rel="noopener">GitHub/Coding双线部署</a></li><li><a href="https://www.jianshu.com/p/1edb4b42ff72" target="_blank" rel="noopener">小书匠Markdown使用手册</a></li><li><a href="https://www.jianshu.com/p/a0aa94ef8ab2" target="_blank" rel="noopener">在Markdown中输入数学公式(MathJax)</a></li><li><a href="https://blog.csdn.net/wgshun616/article/details/81019687" target="_blank" rel="noopener">Hexo 的 Next 主题中渲染 MathJax 数学公式</a></li><li><a href="https://www.leiyawu.com/2018/02/28/hexo-fs-SyncWriteStream-is-deprecated/" target="_blank" rel="noopener">报错：hexo fs.SyncWriteStream is deprecated</a></li><li><a href="https://www.jianshu.com/p/6f77c96b7eff" target="_blank" rel="noopener">Hexo Next主题博客功能完善</a></li><li><a href="https://blog.csdn.net/luyaxige/article/details/80193409" target="_blank" rel="noopener">MathJax语法</a></li><li><a href="https://www.cnblogs.com/linxd/p/4955530.html" target="_blank" rel="noopener">MathJax与LaTex介绍</a></li><li><a href="http://wangwlj.com/2017/10/08/mathjax_basic/" target="_blank" rel="noopener">MathJax(Markdown中的公式)的基本使用语法</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;才搭好博客，发现在博客发布文章确实比微信公众号方便很多，这里简略说下用 Hexo + GitHub Pages + Next搭建个人博客的课程，大部分经验都是来自于网络，我会在整个过程后面附上参考的文章，一来总结搭建博客的过程，二来减少后来人踩坑。&lt;br&gt;
    
    </summary>
    
      <category term="总结" scheme="http://minhzou.top/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Hexo" scheme="http://minhzou.top/tags/Hexo/"/>
    
      <category term="GitHub Pages" scheme="http://minhzou.top/tags/GitHub-Pages/"/>
    
  </entry>
  
  <entry>
    <title>人工智能工程师 第一周 机器学习简介</title>
    <link href="http://minhzou.top/2019/03/31/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E7%AC%AC%E4%B8%80%E5%91%A8%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/"/>
    <id>http://minhzou.top/2019/03/31/人工智能工程师 第一周 机器学习简介/</id>
    <published>2019-03-31T12:37:15.065Z</published>
    <updated>2019-04-05T13:05:36.022Z</updated>
    
    <content type="html"><![CDATA[<h5 id="1-1-一个Kaggle竞赛优胜解决方案"><a href="#1-1-一个Kaggle竞赛优胜解决方案" class="headerlink" title="1.1 一个Kaggle竞赛优胜解决方案"></a>1.1 一个Kaggle竞赛优胜解决方案</h5><ul><li>一个Kaggle竞赛优胜解决方案<ul><li>任务：Avazu点击率预估竞赛</li><li>Rank 2nd Owen Zhang的解法<a id="more"></a></li><li>优胜算法的特点<ul><li>特征工程</li><li>融合大法<ul><li>多层</li><li>多种不同模型的组合</li></ul></li><li>所以：<ul><li>基础模型很重要（线性模型）</li><li>集成学习模型单模型性能好（GBDT）</li><li>特定问题的模型贡献大（FM）</li><li>模型融合很重要</li></ul></li></ul></li></ul></li><li>课程内容安排<ul><li>基本模型<ul><li>线性模型： 线性回归， logistic回归， SVM</li><li>非线性模型： （线性模型核化）、分类回归树</li><li>集成学习模型（随机森林、GBDT）</li><li>数据预处理：数据清洗，特征工程，降维，聚类</li></ul></li><li>模型融合</li><li>推荐系统/点击率预估问题特定解决方案</li></ul></li></ul><h5 id="1-2-机器学习任务类型"><a href="#1-2-机器学习任务类型" class="headerlink" title="1.2  机器学习任务类型"></a>1.2  机器学习任务类型</h5><ul><li>定义</li><li>数据<ul><li>数据通常以二维数据表形式给出<ul><li>每一行： 一个样本</li><li>每一列：一个属性/特征</li></ul></li><li>例：Boston房价预测数据，根据某地区房屋属性，预测该地区预测房价<ul><li>506行， 506个样本</li><li>14列</li></ul></li></ul></li><li>机器学习任务类型<ul><li>监督学习（Supervised Learning）<ul><li>分类（classfication）</li><li>回归（regression）</li><li>排序（ranking）</li></ul></li><li>非监督学习（unsupervised learning）<ul><li>聚类（clustering）</li><li>降维（dimensionality reduction）</li><li>概率密度估计（density estimation）</li></ul></li><li>增强学习（reinforcement learning）</li><li>半监督学习（semi-supervised learning）</li><li>迁移学习（transfer learning）</li><li>……</li></ul></li><li>监督学习<br>  学习一个x-&gt;y 的映射f, 从而对新输入的x进行预测f（x）<script type="math/tex; mode=display">D = \{X_i,y_i\}^N_{i=1}</script>  D：训练数据集<br>  N：训练样本数目<br>  $X_i$: 第i个样本的输入，亦被称为特征、属性或协变量<br>  $y_i$: 第i个训练样本的输出，亦被称为响应，如类别标签、序号或数值<br>  例：波士顿房价预测<ul><li>回归<ul><li>若输出y∈R为连续值，则我们称之为一个回归（regression）任务<br>例： 房价预测，预测二手车的价格</li><li>假设回归模型：$y = f(\mathbf x|\theta)$<ul><li>如在线性回归中，$f(\mathbf x|w) = \mathbf w^T \mathbf x$</li></ul></li><li>训练：根据训练数据 $D = \{\mathbf X_i,y_i\}^N_{i=1}$ 学习映射</li><li>预测：对新的测试数据x进行预测：$\hat f = f(x)$ y带帽表示预测</li><li>学习目标：训练集上预测值与真值之间的差异最小<ul><li>损失函数：度量模型预测值与真值之间的差异，如<script type="math/tex; mode=display">L(f(\mathbf x),y) = \frac 12(f(x) - y)^2</script></li><li>目标函数为：$J(\mathbf \theta) = \frac1N \sum_{i = 1}^N L(f(\mathbf x_i|\mathbf \theta), y_i)$</li></ul></li></ul></li><li>分类<br>   若输出y为离散值，则我们称之为一个分类，标签空间y = {1,2, … C}<br>   例：信用评分<ul><li>分类： 学习从输入x到输出y的映射f:概率问题<br>$\hat y = f(\mathbf x) = \underset{c}  {arg\ max} \ p(y = c\mid \mathbf x, D)$</li><li>学习目标：<ul><li>损失函数：01损失 <script type="math/tex; mode=display">l_{0/1}(y, \hat y) = \begin {cases} 0 & y = \hat y \\ 1 & otherwise \end{cases}</script></li></ul></li><li>需要预测的概率：</li><li>预测：最大后验估计（Maximum a Posteriori, MAP）<br>$\hat y = f(\mathbf x) = \underset{c} {arg\ max}\ p(y = c\mid \mathbf x, D)$</li></ul></li><li>排序（Rank）<br>  排序学习是推荐、搜素、广告的核心方法<br>  排序学习中需要首先根据查询q及其文档集合进行标注（data labeling） 和提取特征（feature extraction） 才能得到D = {….}</li></ul></li><li>非监督学习<br>  发现数据中的“有意义的模式”， 亦被称为知识发现<br>  训练数据不包含标签<br>  标签在训练数据中为隐含变量<br>  $ D = \{ \bf X_i\}_{ i= 1}^ N $<ul><li>聚类<br>例：人的“类型”<br>分多少类？ 模型选择<br>$ K^* = arg\ max _K\ p(K \mid D)$<br>某个样本属于哪个类？</li><li>降维<br>多维特征，有些特征之间会相关而存在冗余<br>很多算法中，降维算法成为了数据预处理的一部分， 如主成分分析（Principal Components Analysis, PCA）</li></ul></li><li>半监督学习<br>  当标注数据“昂贵”时有用<br>  例：标注3D姿态、 蛋白质功能等等</li><li>多标签学习</li><li>有歧义标签学习</li><li>多实例学习</li><li>增强学习<br>从行为的反馈(奖励或惩罚)中学习<ul><li>设计一个回报函数（reward function）， 如果learning agent(如机器人、围棋ai程序)，在决定一步之后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负</li><li>增强学习的任务：找到一条回报值最大的路径</li></ul></li></ul><h5 id="1-3-一个典型的机器学习案例-对鱼进行分类"><a href="#1-3-一个典型的机器学习案例-对鱼进行分类" class="headerlink" title="1.3 一个典型的机器学习案例-对鱼进行分类"></a>1.3 一个典型的机器学习案例-对鱼进行分类</h5><ul><li>根据一些光学传感器对传送带上的鱼进行分类</li><li>形式化为机器学习问题<ul><li>训练数据<ul><li>每条鱼的测量向量</li><li>每条鱼的标签</li></ul></li><li>测试<ul><li>给定一个新的特征向量x</li><li>预测对应的标签y </li></ul></li><li>将长度作为特征进行分类（直方图）<ul><li>需要先做一个决策边界<ul><li>最小化平均损失</li></ul></li></ul></li><li>将亮度作为特征进行分类 （直方图）</li><li>将长度和亮度一起作为特征（二维散点图）<ul><li>线性决策函数</li><li>二次决策函数</li><li>更复杂的决策边界<br>训练集上的误差 ≠ 测试集上的误差<br>数据过拟合（overfitting）<br>推广性（generalization）差</li></ul></li></ul></li><li>小结：设计一个鱼的分类器<ul><li>选择特征<ul><li>可能是最重要的步骤！（收集训练数据）</li></ul></li><li>选择模型（如决策边界的形状）</li><li>根据训练数据估计模型</li><li>利用模型对新样本进行分类</li></ul></li></ul><h5 id="1-4-机器学习算法的组成部分"><a href="#1-4-机器学习算法的组成部分" class="headerlink" title="1.4 机器学习算法的组成部分"></a>1.4 机器学习算法的组成部分</h5><ul><li>机器学习任务的一般步骤<ul><li>确定特征<ul><li>可能是最重要的步骤！（收集训练数据）</li></ul></li><li>确定模型<ul><li>目标函数/决策边界形状</li></ul></li><li>模型训练：根据训练数据估计模型参数<ul><li>优化计算</li></ul></li><li>模型评估：在校验集上评估模型预测性能</li><li>模型应用/预测 </li></ul></li><li>模型<ul><li>监督学习任务：$D = \{X_i, y_i\} _{i = 1} ^ N $</li><li>模型：对给定的输入x, 如何预测其标签$ \hat y$<ul><li>不同模型对数据的假设不同</li><li>最简单的模型：线性模型$ f(x) = \sum_j w_j x_j = \bf w^T \bf x$</li></ul></li><li>确定模型类别后，模型训练转化为求解模型参数<ul><li>如对线性模型参数为!$\theta = \{w_j \mid j = 1,…, D\}$,其中D为特征维数</li></ul></li><li>求解模型参数：目标函数最小化</li></ul></li><li>非线性模型<ul><li>基函数： $x^2$, log, exp, 样条函数，决策树….</li><li>核化：将原问题转化为对偶问题，将对偶问题中的向量积$\langle x_i, x_j\rangle$ 换成核函数$k(x_i,x_j)$</li></ul></li><li>目标函数：通常包含两项：损失函数和正则项<script type="math/tex; mode=display">J(\theta) = \frac 1N \sum_{i=1}^N\ L(f(x_i; \theta), y_i) + R(\theta)</script><ul><li>损失函数<ul><li>损失函数 - 回归<ul><li>损失函数：度量模型预测值与真值之间的差异</li><li>对回归问题：令残差 $r = f(\bf x) - y$<ul><li>L2损失：连续，但对噪声敏感<script type="math/tex; mode=display">L_2 (r) = \frac 12 r ^2</script></li><li>L1损失：不连续，对噪声不敏感<script type="math/tex; mode=display">L_1(r) = |r|</script></li><li>Huber 损失： 连续，对噪声不敏感<script type="math/tex; mode=display">L_\delta (r) = \begin{cases}  \frac 12 r^2 & if|r| \le \delta\\ \delta |r| - \frac 12 \delta^2 & if|r| \ge \delta\end{cases}</script></li></ul></li></ul></li><li>损失函数 - 分类<ul><li>损失函数：度量模型预测值与真值之间的差异</li><li>对分类问题<ul><li>0-1损失：$l_{0/1}(y,f(x)) = \begin{cases} 1 &amp; yf(x) \lt 0 \\ 0 &amp; othereise\end{cases}$</li><li>logistic损失：亦称负log似然损失<br>  $l_{log}(y,f(x)) = log(1 + exp(-yf(x)))$</li><li>指数损失：$l_{exp}(y,f(x)) = exp(-yf(x))$</li><li>合页损失：$l_{hinge}(y,f(x)) = max(0, 1 - yf(x))$</li></ul></li></ul></li></ul></li><li>正则项<br>  复杂模型（预测）不稳定：方差大<br>  正则项对复杂模型施加惩罚<ul><li>正则项的必要性<br>例：sin曲线拟合</li><li>增加L2正则<br>岭回归：最小化RSS</li><li>欠拟合：模型太简单/对复杂性惩罚太多</li><li>样本数目增多时，可以考虑更复杂的模型</li><li>常见正则项<ul><li>L2正则: $R(\theta) = \lambda ||\theta||^2_2 = \lambda \sum^D_{j=1} \theta_j^2$</li><li>L1正则: $R(\theta) = \lambda |\theta| = \lambda \sum ^D_{j=1}|\theta_j|$</li><li>L0正则: $R(\theta) = \lambda||\theta||_ 0$<ul><li>非0参数的数目</li><li>不好优化，通常用L1正则近似</li></ul></li></ul></li><li>常见线性模型的损失和正则项组合</li></ul></li></ul></li></ul><div class="table-container"><table><thead><tr><th></th><th>L2损失</th><th>L1损失</th><th>Huber损失</th><th>Logistic损失</th><th>合页损失</th><th>e-insensitive损失</th></tr></thead><tbody><tr><td>L2正则</td><td>岭回归</td><td></td><td></td><td>L2正则 Logistic回归</td><td>SVM</td><td>SVR</td></tr><tr><td>L1正则</td><td>LASSO</td><td></td><td></td><td>L1正则 Logistic回归</td><td></td><td></td></tr><tr><td>L2+L1正则</td><td>Elastic</td><td></td><td></td><td></td><td></td></tr></tbody></table></div><ul><li>模型训练<ul><li>在训练数据上求目标函数极小值：优化</li><li>简单目标函数直接求解<ul><li>如小数据集上的线性回归</li></ul></li><li>更复杂问题：凸优化<ul><li>（随机）梯度下降</li><li>牛顿法/拟牛顿法</li><li>… </li></ul></li></ul></li><li>梯度下降（Gradient Descent）算法<ul><li>梯度下降/最速下降算法：快速寻找函数局部极小值</li><li>梯度下降算法：求函数J（θ）的最小值<ul><li>给定初始值θ0</li><li>跟新θ，使得J（θ）越来越小<ul><li>$θ_t = θ ^{t-1} - \eta\nabla J(θ)$ ( $\eta$ : 学习率 )</li></ul></li><li>直到收敛到 / 达到预先设定的最大迭代次数</li><li>下降的步伐太小（学习率）非常重要：如果太小，收敛速度慢； 如果太大，可能会出现overshoot the minimum的现象</li><li>梯度下降求得的只是局部最小值<ul><li>二阶导数 &gt; 0, 则目标函数为凸函数，局部极小值即为全局最小值</li><li>随机选择多个初始值，得到函数的多个局部极小值点。多个局部极小值点的最小值为函数的全局最小值</li></ul></li><li>梯度下降算法每次学习都使用整个训练集，这样对大的训练数据集合，每次学习时间过长，对大的训练集需要消耗大量的内存。此时可采用随机梯度下降（Stochastic gradient descent, SGD), 每次从训练集中随机选择一部分样本进行学习。</li><li>更多（随机）梯度下降算法的改进版<ul><li>动量（Momentum）</li><li>Nesterov accelerated gradient (NAG)</li><li>Adagrad</li><li>RMSprop</li><li>Adaptive Moment Estimation (Adam)…</li></ul></li></ul></li></ul></li><li>模型选择与模型评估<ul><li>同一个问题有不同的解决方案<br>  如线性回归 vs. 决策树</li><li>哪个更好？ 模型评估与模型选择<ul><li>在新数据点的预测误差最小</li></ul></li><li>模型评估：已经选定最终的模型，估计它在新数据上的预测误差</li><li>模型选择：估计不同模型的性能，选出最好的模型</li></ul></li><li>样本足够多：训练集和校验集</li><li>样本不够多：重采样技术来模拟校验集：交叉验证和bootstrap<ul><li>K-折交叉验证<ul><li>交叉验证（Cross Validation, CV）： 将训练数据分成容量大致相等的K份（通常K = 5/10）</li><li>交叉验证估计的误差为：<script type="math/tex; mode=display">CV(M)= \frac1K \sum  ^K_{k = 1} E_k(M)</script></li></ul></li></ul></li><li>模型选择<ul><li>对多个不同模型，计算其对应的误差CV（M）， 最佳模型为CV（M）最小的模型</li><li>模型复杂度和泛化误差的关系通常是U形曲线：<h5 id="1-5-学习环境简介"><a href="#1-5-学习环境简介" class="headerlink" title="1.5 学习环境简介"></a>1.5 学习环境简介</h5><ul><li>编程语言 Python</li><li>数据处理工具包</li><li>Numpy</li><li>SciPy</li><li>pandas</li></ul></li></ul></li><li>数据可视化工具包<ul><li>Matplotlib</li><li>Seaborn</li></ul></li><li>机器学习工具包<ul><li>scikit learn</li></ul></li><li>示例代码：INotebook </li><li>NumPy<ul><li>NumPy(Numeric Python)是Python的开源数值计算扩展，可用来存储和处理大型矩阵</li><li>Numpy包括：<ul><li>N维数组(ndarray)</li><li>实用的线性代数、傅里叶变换和随机数生成函数</li></ul></li><li>Numpy和稀疏矩阵运算包SciPy配合使用更加方便</li></ul></li><li>SciPy<ul><li>SciPy是建立在NumPy的基础上、是科学和工程设计的Python工具包，提供统计、优化和数值微积分计算等功能</li><li>NumPy 处理$10^6$级别的数据通常没有大问题，但当数据量达到$10^7$级别时速度开始发慢，内存受到限制（具体情况取决于实际内存的大小）</li><li>当处理超大规模数据集，比如$10^10$级别，且数据中包含大量的0时，可采用稀疏矩阵可显著的提高速度和效率</li></ul></li><li>Pandas(<strong>Pan</strong>del <strong>da</strong>ta structures)<ul><li>Pandas是Python语言的“关系型数据库”数据结构和数据分析工具，非常高效且易于使用<ul><li>基于NumPy补充了大量数据操作功能，能实现统计、分组、排序、透视表(SQL语句的大部分功能)</li><li>Pandas主要有2种重要的数据类型<ul><li>series：一维序列</li><li>DataFrame：二维表(机器学习数据的常用数据结构)</li></ul></li></ul></li></ul></li><li>Matplotlib<ul><li>Matplotlib是Python语言的2D图形绘制工具</li></ul></li><li>Seaborn<ul><li>Seaborn是一个基于Matplotlib的Python可视化工具包，提供更高层次的用户接口，可以给出漂亮的数据统计</li></ul></li><li>Scikit - Learn<ul><li>Machine Learning in Python</li><li>Scikit-Learn是基于Python的开源机器学习模块，最早于2007年由David Cournapeau发起</li><li>基本功能有六部分：分类（Classification），回归（Regression），聚类（Clustering），数据降维（Dimensionality reduction），模型选择（Model Selection），数据预处理（Preprocessing）</li><li>对于具体的机器学习问题，通常可以分为三个步骤<ul><li>数据准备与预处理（Preprocessing, Dimensionality reduction）</li><li>模型选择与训练（Classification, Regression, Clustering）</li><li>模型验证与参数调优（Model Selection）</li></ul></li></ul></li><li>各种机器学习模型有统一的接口</li><li>模型既有默认参数，也提供多种参数调优方法</li><li>卓越的文档</li><li>丰富的随附任务功能集合</li><li>活跃的社区提供开发和支持</li></ul><h5 id="1-6-线性回归模型"><a href="#1-6-线性回归模型" class="headerlink" title="1.6 线性回归模型"></a>1.6 线性回归模型</h5><ul><li>目标函数通常包含两项：损失函数和正则项<script type="math/tex; mode=display">J(\bf \theta) = \frac1N \sum_{i = 1}^N L(f(\bf x_i|\bf \theta), y_i) + \lambda R(\bf \theta)</script></li><li>对回归问题，损失函数可以采用L2损失，得到<script type="math/tex; mode=display">\begin{eqnarray}J(\theta)    &=&\sum_{i=1}^NL(y_i,\hat y_i) \\   &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\   &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2  \end{eqnarray}</script>  残差平方和（residual sum of squares, RSS）</li><li>由于线性模型比较简单，实际应用中有时正则项为空，得到最小二乘线性回归（Ordinary Least Square, OLS）<script type="math/tex; mode=display">\begin{eqnarray}J(\theta)    &=&\sum_{i=1}^NL(y_i,\hat y_i)    &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\   &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2  \end{eqnarray}</script></li><li>正则项可以为L2正则，得到岭回归（Ridge Regression）<script type="math/tex; mode=display">J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda ||w||^2_2</script></li><li><p>正则项也可以选L1正则，得到Lasso模型：</p><script type="math/tex; mode=display">J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda |w|</script><ul><li>当$\lambda$取合适值时，Lasso（Least absolute shrinkage and selection operator）的结果是稀疏的（w的某些元素系数为0），起到特征选择作用</li></ul></li><li><p>为什么L1正则的解是稀疏的？</p></li><li>线性回归模型的概率解释<ul><li>最小二乘（线性）回归等价于极大似然估计<ul><li>假设：$ y = f(\bf x) + \epsilon = w^T + \epsilon $<br>其中$\epsilon$为线性预测和真值之间的残差<br>我们通常假设残差的分布为$\epsilon \sim N(0,\sigma ^2)$,因此线性回归可写成：$p(y|x,\theta) \sim N(y| \bf w^T \bf x, \sigma^2)$,其中$ \bf \theta = (\bf w, \sigma ^2)$</li></ul></li><li>正则（线性）回归等价于高斯先验（L2正则）或Laplace先验下（L1正则）的贝叶斯估计</li></ul></li><li>Recall：极大似然估计<ul><li>极大似然估计（Maximize Likelihood Estimator, MLE）定义为<script type="math/tex; mode=display">\hat \theta = \underset {\theta}  {arg\ max}\ log p(D\mid \theta)</script></li><li>其中（log）似然函数为<script type="math/tex; mode=display">l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta)</script></li><li>表示在参数为$\theta$的情况下，数据$D ={\bf x_i,y_i}^N_{i=1}$</li><li>极大似然：选择数据出现概率最大的参数</li></ul></li><li>线性回归的MLE<script type="math/tex; mode=display">p(y_i|x_i,\bf w,\sigma ^2) \sim N(y_i\mid \bf w^T \bf x_i, \sigma^2) = \frac 1{\sqrt{2\pi}\sigma} exp(-\frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))</script><ul><li>OLS的似然函数为<script type="math/tex; mode=display">l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta)</script></li><li>极大似然可等价地写成极小负log似然损失（negative log likelihood, NLL）</li></ul></li></ul><script type="math/tex; mode=display">\begin{eqnarray}{NLL(\bf \theta)}    &=& \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) \\    &=& - \sum_{i=1}^N log ((\frac 1{2 \pi \sigma^2})^ \frac 12 exp(- \frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))) \\    &=& \frac N2 log(2\pi \sigma ^2) + \frac 1{2 \sigma^2} \sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2    \end{eqnarray}</script><ul><li>正则回归等价于贝叶斯估计<ul><li>假设残差的分布为$\epsilon \sim N(0, \sigma ^2)$,线性回归可写成：<br>$p(y_i \mid \bf x_i, \theta) \sim N(y_i \mid \bf w^T \bf x_i，\sigma ^2)$<br>$p(y\mid \bf X, \bf w, \sigma ^2) = N(\bf y \mid \bf X \bf w, \sigma ^2 \bf I_N) \propto exp(- \frac 1{2\sigma ^2}((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w)))$</li><li>若假设参数为w的先验分布为 $w_j \sim N(0, \tau ^2)$<ul><li>偏向较小的系数值，从而得到的曲线也比较平滑<br>$p(\bf w) =\prod_{j=1}^{D} N(w_j \mid 0, \tau ^2) \propto exp(- \frac 1{2\tau^2} \sum_{j=1}^D \bf w_j^2 = exp(- \frac 1{2\tau^2} ( \bf w^T \bf w ) )) $</li><li>其中$1/\tau ^2$控制先验的强度</li></ul></li><li>根据贝叶斯公式，得到参数的后验分布为<br>$p(y\mid \bf X, \bf w, \sigma ^2) = \propto exp(- \frac 1{2\sigma ^2} ((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w) ) - \frac 1{2 \tau^2} ( w^Tw ) )$</li><li>则最大后验估计(MAP)等价于最小目标函数<br>$J(\bf w) = (\bf y - \bf X\bf w)^T(\bf y - \bf X\bf w) + \frac {\sigma ^2}{\tau^2} \bf w^T \bf w $</li><li>对比岭回归的目标函数<br>$J(\bf w) = \sum_{i=1}^N(y_i -\bf w^T\bf x_i)^2 + \lambda \Vert \bf w\Vert ^2_2$</li></ul></li><li>小结<ul><li>线性回归模型可以放到机器学习一般框架<ul><li>损失函数：L2损失，…</li><li>正则：无正则， L2正则，L1正则…</li></ul></li><li>正则回归模型可视为先验为正则、似然为高斯分布的贝叶斯估计<ul><li>L2正则：先验分布为高斯分布</li><li>L1正则：先验分布为Laplace分布</li></ul></li></ul></li></ul><h5 id="1-7-线性回归模型-优化算法"><a href="#1-7-线性回归模型-优化算法" class="headerlink" title="1.7 线性回归模型-优化算法"></a>1.7 线性回归模型-优化算法</h5><ul><li>线性回归的目标函数<ul><li>无正则的最小二乘线性回归（Ordinary Least Square, OLS）：</li><li>L2正则的岭回归（Ridge Regression）模型：</li><li>L1正则的Lasso模型：</li></ul></li><li>模型训练：<br>  根据训练数据求目标函数取极小值的参数：<br>  $\hat w = \underset {w} {arg\ min} J(\bf w)$<ul><li>目标函数的最小值：<ul><li>一阶的导数为0：$\frac{\partial J(w)} {\partial w}$</li><li>二阶导数&gt;0：$\frac{\partial J^2(w)} {\partial w^2}$</li></ul></li></ul></li><li>OLS的优化求解：<ul><li>OLS的优化求解<ul><li>OLS的目标函数写成矩阵形式：<br>$J(w) = \sum ^N_{i=1}(y_i - w^Tx_i)^2 = (y - Xw)^T(y - Xw)$</li><li>只取与w有关的项，得到<br>$J(w) = w^T(X^TX)w - 2w^T(X^Ty)$</li><li>求导  $\frac{\partial J(w)} {\partial w} = 2X^TXw - 2X^Ty = 0 \Rightarrow X^TXw = X^Ty$`<br>$\hat w_{OLS} = (X^TX)^{-1}X^Ty$</li></ul></li><li>OLS的优化求解 ——SVD<ul><li>OLS目标函数：$J(w) = \Vert y - Xw\Vert_2^2$</li><li>相当于求 $y = Xw$</li><li>如果X为方阵，可求逆：$w = X^{-1}y$</li><li>如果𝐗不是方阵，可求Moore-Penrose广义逆：$𝐰 = 𝐗^{\dagger }𝐲$。</li><li>Moore-Penrose广义逆可采用奇异值分解(Singular Value Decomposition)<br>实现：<br>奇异值分解：$X = U \Sigma V^T$<br>$X^{\dagger } = V \Sigma ^{\dagger} U^T$<br>其中 $\Sigma = \begin{pmatrix}<br>{\sigma_1}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>{0}&amp;{\sigma_2}&amp;{\cdots}&amp;{0}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{0}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>\end{pmatrix}$,$\Sigma ^{\dagger} = \begin{pmatrix}<br>{\frac {1}{\sigma_1}}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>{0}&amp;{\frac{1}{\sigma_2}}&amp;{\cdots}&amp;{0}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{0}&amp;{0}&amp;{\cdots}&amp;{0}\\<br>\end{pmatrix}$</li></ul></li><li>OLS的优化求解——梯度下降</li></ul></li><li>岭回归的优化求解</li><li>Lasso的优化条件<ul><li>软&amp; 硬阈值</li><li>Lasso的优化求解——坐标轴下降法</li></ul></li><li>小结<ul><li>线性回归模型比较简单<ul><li>当数据规模比较小时，可直接解析求解<ul><li>scikit learn中的实现采用SVD分解实现</li></ul></li><li>当数据规模较大时，可采用随机梯度下降<ul><li>scikit learn提供一个SGDRegression类</li></ul></li></ul></li><li>岭回归求解类似OLS，采用SVD分解实现</li><li>Lasso优化求解采用坐标轴下降法</li></ul></li></ul><h5 id="1-8-线性回归模型-模型选择"><a href="#1-8-线性回归模型-模型选择" class="headerlink" title="1.8 线性回归模型-模型选择"></a>1.8 线性回归模型-模型选择</h5><ul><li>模型评估与模型选择<ul><li>模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果<ul><li>校验集划分（）</li><li>评价指标（）</li></ul></li><li>模型选择：<ul><li>模型中通常有一些超参数，需要通过模型选择来确定<ul><li>线性回归模型中的正则参数</li><li>OLS中的特征的数目</li></ul></li><li>参数搜索范围：网格搜索（GridSearch）</li></ul></li><li>Scikit learn将交叉验证与网格搜索合并为一个函数</li></ul></li><li>评价准则</li><li>线性回归中的模型选择<br>Scikit learn中的model selection模块提供模型选择功能<ul><li>对于线性模型，留一交叉验证（N折交叉验证，亦称为leave-oneout<br>cross-validation，LOOCV）有更简便的计算方式，因此Scikit<br>learn提供了RidgeCV类和LassoCV类实现了这种方式</li><li>后续课程将讲述一般模型的交叉验证和参数调优GridSearchCV</li><li>RidgeCV</li><li>LassoCV</li></ul></li><li>小结：线性回归之模型选择<ul><li>采用交叉验证评估模型预测性能，从而选择最佳模型<ul><li>回归性能的评价指标</li><li>线性模型的交叉验证通常直接采用广义线性模型的留一交叉验证进行快速模型评估<ul><li>Scikit learn中对RidgeCV和LassoCV实现该功能</li></ul></li></ul></li></ul></li></ul><h5 id="1-9-波士顿房价预测案例详解——数据探索"><a href="#1-9-波士顿房价预测案例详解——数据探索" class="headerlink" title="1.9 波士顿房价预测案例详解——数据探索"></a>1.9 波士顿房价预测案例详解——数据探索</h5><ul><li>第一步：理解任务，准备数据<ul><li>数据读取<ul><li>Pandas支持多种格式的数据</li></ul></li><li>数据探索&amp;特征工程<ul><li>数据规模</li><li>确定数据类型，是否需要进一步编码<ul><li>特征编码</li></ul></li><li>数据是否有缺失值<ul><li>数据填补</li></ul></li><li>查看数据分布，是否有异常数据点<ul><li>离群点处理</li></ul></li><li>查看两两特征之间的关系，看数据是否有冗余/相关<ul><li>降维</li></ul></li></ul></li><li>数据概览<ul><li>pandas:DataFrame<ul><li>Head():数据前5行，可查看每一列的名字及数据类型</li><li>Info():<ul><li>数据规模：行数&amp;列数</li><li>每列的数据类型、是否有空值</li><li>占用存储量</li></ul></li><li>shape:行数&amp;列数</li></ul></li></ul></li><li>各属性的统计特性<ul><li>直方图<br>  每个取值在数据集中出现的样本数目 </li><li>离群点<ul><li>离群点：奇异点（outlier）,指远离大多数样本的样本点。通常认为这些点是噪声，对模型有坏影响</li></ul></li><li>相关性<ul><li>相关性：相关性可以通过计算相关系数或打印散点图来发现</li><li>相关系数：</li><li>散点图<ul><li>可以通过两个变量之间的散点图直观感受二者的相关性</li></ul></li><li>数据预处理<ul><li>数据标准化（ Standardization ）<ul><li>某个特征的所有样本取值为0均值、1方差</li></ul></li><li>数据归一化（ Scaling ）<ul><li>某个特征的所有样本取值在规定范围内</li></ul></li><li>数据正规化（ Normalization ）<ul><li>每个样本模长为1</li></ul></li><li>数据二值化<ul><li>根据特征值取值是否大于阈值将特征值变为0或1，可用类Binarizer 实现</li></ul></li><li>数据缺失</li><li>数据类型变换<ul><li>有些模型只能处理数值型数据。如果给定的数据是不同的类型，必须先将数据<br>变成数值型。</li></ul></li></ul></li></ul></li></ul></li><li>第二步：模型确定和模型训练<ul><li>1、确定模型类型<ul><li>目标函数（损失函数、正则）</li></ul></li><li>2、模型训练<ul><li>优化算法（解析法，梯度下降、随机梯度下降…）</li></ul></li></ul></li><li>第三步：模型评估与模型选择<ul><li>模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果<ul><li>校验集划分（train_test_split、交叉验证）</li><li>评价指标 （sklearn.metics）</li><li>也可以检查残差的分布</li><li>还可以打印预测值与真值的散点图</li></ul></li><li>模型选择：选择预测性能最好的模型<ul><li>模型中通常有一些超参数，需要通过模型选择来确定</li><li>参数搜索范围：网格搜索（GridSearch）</li></ul></li></ul></li></ul></li></ul><h5 id="1-10-波士顿房价预测-数据探索代码"><a href="#1-10-波士顿房价预测-数据探索代码" class="headerlink" title="1.10 波士顿房价预测-数据探索代码"></a>1.10 波士顿房价预测-数据探索代码</h5><figure class="highlight python"><figcaption><span>python 3.7</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">"boston_housing.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据探索</span></span><br><span class="line">print(data.head())</span><br><span class="line">data.info()</span><br><span class="line">print(data.isnull().sum())</span><br><span class="line">print(data.describe())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标y(房屋价格)的直方图/分布</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">sns.distplot(data.MEDV.values, bins=<span class="number">30</span>, kde=<span class="literal">True</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Median value of owner_occupied homes'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单个特征散点图</span></span><br><span class="line">plt.scatter(range(data.shape[<span class="number">0</span>]), data[<span class="string">"MEDV"</span>].values, color=<span class="string">'purple'</span>)</span><br><span class="line">plt.title(<span class="string">"Distribution of Price"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除y大于50的样本</span></span><br><span class="line">data = data[data.MEDV &lt; <span class="number">50</span>]</span><br><span class="line">print(data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入属性的直方图／分布</span></span><br><span class="line"><span class="comment"># 犯罪率特征</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">sns.distplot(data.CRIM.values, bins=<span class="number">30</span>, kde=<span class="literal">False</span>)</span><br><span class="line">plt.xlabel(<span class="string">'crime rate'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否靠近charles river</span></span><br><span class="line">sns.countplot(data.CHAS, order=[<span class="number">0</span>, <span class="number">1</span>]);</span><br><span class="line">plt.xlabel(<span class="string">'Charles River'</span>);</span><br><span class="line">plt.ylabel(<span class="string">'Number of occurrences'</span>);</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 靠近高速</span></span><br><span class="line">sns.countplot(data.RAD)</span><br><span class="line">plt.xlabel(<span class="string">'index of accessibility to radial highways'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两两特征之间的相关性</span></span><br><span class="line"><span class="comment"># 获得所有列的名字</span></span><br><span class="line">cols = data.columns</span><br><span class="line"><span class="comment"># 计算相关性</span></span><br><span class="line">data_corr = data.corr().abs()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相关性热图</span></span><br><span class="line">plt.subplots(figsize=(<span class="number">13</span>, <span class="number">9</span>))</span><br><span class="line">sns.heatmap(data_corr, annot=<span class="literal">True</span>)</span><br><span class="line">sns.heatmap(data_corr, mask=data_corr &lt; <span class="number">1</span>, cbar=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">'house_coor.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出强相关对</span></span><br><span class="line">threshold = <span class="number">0.5</span></span><br><span class="line">corr_list = []</span><br><span class="line">size = data_corr.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, size):</span><br><span class="line">        <span class="keyword">if</span> (data_corr.iloc[i,j] &gt;= threshold <span class="keyword">and</span> data_corr.iloc[i, j] &lt; <span class="number">1</span>) <span class="keyword">or</span> (data_corr.iloc[i, j] &lt; <span class="number">0</span> <span class="keyword">and</span> data_corr.iloc &lt;= -threshold):</span><br><span class="line">            corr_list.append([data_corr.iloc[i, j], i, j])</span><br><span class="line">s_corr_list = sorted(corr_list, key=<span class="keyword">lambda</span> x: -abs(x[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> v, i, j <span class="keyword">in</span> s_corr_list:</span><br><span class="line">    print(<span class="string">"%s and %s = %.2f"</span> % (cols[i], cols[j], v))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> v, i, j <span class="keyword">in</span> s_corr_list:</span><br><span class="line">    sns.pairplot(data, size=<span class="number">6</span>, x_vars=cols[i], y_vars=cols[j])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h5 id="1-11-波士顿房价预测案例详解"><a href="#1-11-波士顿房价预测案例详解" class="headerlink" title="1.11 波士顿房价预测案例详解"></a>1.11 波士顿房价预测案例详解</h5><h5 id="1-12-波士顿房价预测案例详解-代码讲解"><a href="#1-12-波士顿房价预测案例详解-代码讲解" class="headerlink" title="1.12 波士顿房价预测案例详解-代码讲解"></a>1.12 波士顿房价预测案例详解-代码讲解</h5>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;1-1-一个Kaggle竞赛优胜解决方案&quot;&gt;&lt;a href=&quot;#1-1-一个Kaggle竞赛优胜解决方案&quot; class=&quot;headerlink&quot; title=&quot;1.1 一个Kaggle竞赛优胜解决方案&quot;&gt;&lt;/a&gt;1.1 一个Kaggle竞赛优胜解决方案&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;一个Kaggle竞赛优胜解决方案&lt;ul&gt;
&lt;li&gt;任务：Avazu点击率预估竞赛&lt;/li&gt;
&lt;li&gt;Rank 2nd Owen Zhang的解法
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="http://minhzou.top/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="人工智能" scheme="http://minhzou.top/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>计算机视觉基础入门 学习笔记</title>
    <link href="http://minhzou.top/2019/03/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/"/>
    <id>http://minhzou.top/2019/03/30/计算机视觉基础入门/</id>
    <published>2019-03-30T11:22:46.082Z</published>
    <updated>2019-04-04T15:45:41.583Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、-计算机视觉和深度学习概述"><a href="#一、-计算机视觉和深度学习概述" class="headerlink" title="一、 计算机视觉和深度学习概述"></a>一、 计算机视觉和深度学习概述</h5><ol><li>计算机视觉回顾<ol><li>计算机视觉（computer vision）定义<ul><li>数据（静态图片，视频）</li><li>算法（机器学习算法，神经网络）本质上是一个回归+分类</li></ul></li><li>计算机视觉的重要性<a id="more"></a><ul><li>三大任务：图像识别（image classification）<br>车牌识别，人脸识别</li><li>三大任务：目标检测（object detection = classification + localization）<br>行人检测和车辆检测</li><li>三大任务：图像分割<br>图像语义分割<br>个体分割 = 检测 + 分割</li><li>视觉目标跟踪（tracking）</li><li>视频分割</li><li>图像风格迁移</li><li>生成对抗网络（GAN）</li><li>视频生成</li></ul></li></ol></li><li>深度学习介绍<br> 2006 Hinton bp(反向传播)<br> 2012 Krizhevsky A 深度学习 深度卷积<br> RNN<br> LSTM 持续信息<br> 视觉识别，语音识别，DeepMind, AlphaGo<br> 人脸识别：LFW 错误率5% -&gt; 0.5%<br> 图像分割<br> VGGNet, GoogleNet, ResNet, DenseNet<ul><li>常见的深度学习开发平台<br>  Torch, TensorFlow, MatConvNetTheano, Caffe</li></ul></li><li>课程介绍<ul><li>图像识别：<br>Alexnet, VGGnet, GoogleNet, ResNet, DenseNet</li><li>目标检测<br>Fast-rcnn, faster-rcnn, Yolo, Retina-Net</li><li>图像分割<br>FCN, Mask-Rcnn</li><li>目标跟踪<br>GORURN， ECO</li><li>图像生成<br>GAN， WGAN</li><li>光流<br>FlowNet</li><li>视频分割<br>Segnet <h5 id="二、-图像分类与深度卷积网络的模型"><a href="#二、-图像分类与深度卷积网络的模型" class="headerlink" title="二、 图像分类与深度卷积网络的模型"></a>二、 图像分类与深度卷积网络的模型</h5></li></ul></li><li>图像分类<ul><li>图像分类的挑战<br>光照变化<br>形变<br>类内变化</li><li>图像分类定义</li><li>目标分类框架</li><li>泛化能力<br>如何提高泛化能力？ 需要用图像特征来描述图像</li><li>训练和测试的流程</li><li>图像特征<br>  color: Qutantize RGB values<br>  global shape: PCA space<br>  local shape: shape context<br>  texture: Filter banks<br>  SIFT, Hog, LBP, Harr</li><li>支持向量机（SVM）<br>   超平面与支持向量<br>   最大化间隔<br>   svm分类（python）以lris兰花分类为例<br>   程序实现</li><li>更好的特征<br>  CNN特征<br>  学习出来的<br>  如何学习？ 构造神经网络</li></ul></li><li>神经网络原理<ul><li>神经网络做图像分类</li><li>神经网络搭建</li><li>神经网络的基本单元：神经元</li><li>激励函数<br>  Sigmoid、tanh、ReLU、Leaky ReLU、Maxout、ELU</li><li>卷积层</li><li>卷积滤波的计算</li><li>卷积层可视化</li><li>池化层（pooling layer）<br>  特征表达更加紧凑，同时具有位移不变性</li><li>全连接层</li><li>损失函数<br>  交叉熵损失函数（SIGMOID_CROSS_ENTROPY_LOSS) 应用于二分类问题<br>  Softmax 损失函数（SOFTMAX_LOSS)  多分类问题<br>  欧式距离损失函数（EUCLIDEAN_LOSS）回归问题<br>  对比损失函数（Contrastive loss）用来计算两个图像之间的相似度<br>  Triplet loss</li><li>训练网络</li><li>网络训练和测试</li></ul></li><li>卷积神经网络介绍<br> Alexnet, VGGnet, GoogleNet, ResNet, DenseNet<ul><li>训练技巧， 防止过拟合（泛化能力不强）<ul><li>数据增强（Data augmentation）<br>  水平翻转， 随机裁剪和平移变换，颜色、光照变换</li><li>Dropout</li></ul></li><li>其他有助于训练的手段<ul><li>L1， L2正则化</li><li>Batch Normalization</li></ul></li></ul></li><li>利用caffe搭建深度网络做图像分类</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;一、-计算机视觉和深度学习概述&quot;&gt;&lt;a href=&quot;#一、-计算机视觉和深度学习概述&quot; class=&quot;headerlink&quot; title=&quot;一、 计算机视觉和深度学习概述&quot;&gt;&lt;/a&gt;一、 计算机视觉和深度学习概述&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;计算机视觉回顾&lt;ol&gt;
&lt;li&gt;计算机视觉（computer vision）定义&lt;ul&gt;
&lt;li&gt;数据（静态图片，视频）&lt;/li&gt;
&lt;li&gt;算法（机器学习算法，神经网络）本质上是一个回归+分类&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;计算机视觉的重要性
    
    </summary>
    
      <category term="学习笔记" scheme="http://minhzou.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="计算机视觉" scheme="http://minhzou.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="基础" scheme="http://minhzou.top/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="学习笔记" scheme="http://minhzou.top/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
