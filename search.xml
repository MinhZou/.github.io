<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ç¬¬äºŒå‘¨ Logisticå›žå½’ã€SVM]]></title>
    <url>%2F2019%2F04%2F06%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%20Logistic%E5%9B%9E%E5%BD%92%E3%80%81SVM%2F</url>
    <content type="text"><![CDATA[1ã€Logisticå›žå½’åŸºæœ¬åŽŸç† åˆ†ç±» ç»™å®šè®­ç»ƒæ•°æ®$D =\{\mathbf x_i, y_i\}^N_{i=1}$ï¼Œåˆ†ç±»ä»»åŠ¡å­¦ä¹ ä¸€ä¸ªä»Žè¾“å…¥xåˆ°è¾“å‡ºyçš„æ˜ å°„f ï¼š$\hat y = f(\mathbf x) = \underset{c}{arg\ max}\ p(y = c \mid \mathbf x, D)$ å…¶ä¸­yä¸ºç¦»æ•£å€¼ï¼Œå…¶å–å€¼èŒƒå›´ç§°ä¸ºæ ‡ç­¾ç©ºé—´:$Y =\{1,2,â€¦,C\}$ å½“C=2æ—¶ï¼Œä¸ºä¸¤ç±»åˆ†ç±»é—®é¢˜ï¼Œè®¡ç®—å‡º$p(y = 1 \mid \mathbf x)$å³å¯ã€‚æ­¤æ—¶åˆ†å¸ƒä¸ºBernoulliåˆ†å¸ƒ: p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))å…¶ä¸­$\mu (\mathbf x) = \mathbb{E}(y \mid \mathbf x) = p(y = 1 \mid \mathbf x)$ Recall:Bernouiliåˆ†å¸ƒ Bernoulliåˆ†å¸ƒåˆåä¸¤ç‚¹åˆ†å¸ƒæˆ–è€…0-1åˆ†å¸ƒã€‚è‹¥Bernoulliè¯•éªŒæˆåŠŸï¼Œåˆ™Bernoulliéšæœºå˜é‡Xå–å€¼ä¸º1ï¼Œå¦åˆ™Xä¸º0ã€‚è®°è¯•éªŒæˆåŠŸæ¦‚çŽ‡ä¸ºÎ¸ï¼Œ æˆ‘ä»¬ç§°Xæœä»Žå‚æ•°ä¸ºÎ¸çš„Bernoulliåˆ†å¸ƒï¼Œè®°ä¸º: ð‘‹~ðµð‘’ð‘Ÿ(Î¸), æ¦‚çŽ‡å‡½æ•°ï¼ˆpmfï¼‰ä¸ºï¼šp(x) = \theta ^x(1- \theta)^{1-x} = \begin{cases} \theta & if\ x = 1\\ 1 - \theta & if\ x = 0 \end{cases} Bernoulliåˆ†å¸ƒçš„å‡å€¼ï¼š$\mu = \theta $ æ–¹å·®ï¼š$\sigma^2 = \theta \times (1-\theta)$ Logisticå›žå½’æ¨¡åž‹ Logisticå›žå½’æ¨¡åž‹åŒçº¿æ€§å›žå½’æ¨¡åž‹ç±»ä¼¼ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡åž‹ï¼Œåªæ˜¯æ¡ä»¶æ¦‚çŽ‡ð‘(ð‘¦|ð±)çš„å½¢å¼ä¸åŒï¼šp(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))\mu (\mathbf x) = \sigma(\mathbf w^T\mathbf x) å…¶ä¸­sigmoidå‡½æ•°ï¼ˆSå½¢å‡½æ•°ï¼‰å®šä¹‰ä¸º\sigma(a) = \frac{1}{1+exp(-a)} = \frac{exp(-a)}{exp(-a)+1} äº¦è¢«ç§°ä¸ºlogisticå‡½æ•°æˆ–logitå‡½æ•°ï¼Œå°†å®žæ•°aå˜æ¢åˆ°[0,1]åŒºé—´ã€‚ å› ä¸ºæ¦‚çŽ‡å–å€¼åœ¨[0,1]åŒºé—´ Logisticå›žå½’äº¦è¢«ç§°ä¸ºlogitå›žå½’ ä¸ºä»€ä¹ˆç”¨logisticå‡½æ•°ï¼Ÿ åœ¨ç¥žç»ç§‘å­¦ä¸­ ç¥žç»å…ƒå¯¹å…¶è¾“å…¥è¿›è¡ŒåŠ æƒå’Œï¼š$f(x) = w^Tx$ å¦‚æžœè¯¥å’Œå¤§äºŽæŸé˜ˆå€¼ $f(x) &gt; \tau $ï¼Œç¥žç»å…ƒå‘æ”¾è„‰å†² åœ¨Logisticå›žå½’ï¼Œå®šä¹‰Log Odds Ratio:\begin{eqnarray} LOR(x) &=& log \frac {p(y=1 \mid \mathbf x, \mathbf f w)}{p(y = 0 \mid \mathbf x, \mathbf w)} &=& log [\frac{1}{1+exp(-\mathbf w^T\mathbf x)} \frac {1+exp(-\mathbf w^T\mathbf x)}{exp(-\mathbf w^T\mathbf x)}] \\ &=& log [exp(\mathbf w^T \mathbf x)]\\ &=& \mathbf w^T\mathbf x \end{eqnarray} å› æ­¤ï¼Œ$iff LOR(\mathbf x) = \mathbf w^T \mathbf x &gt; 0$ç¥žç»å…ƒå‘æ”¾è„‰å†²ï¼Œå³$p(y=1 \mid \mathbf x, \mathbf w) &gt; p(y=0 \mid \mathbf x, \mathbf w)$ çº¿æ€§å†³ç­–å‡½æ•° åœ¨Logisticå›žå½’ä¸­ $LOR(\bf x) = w^Tx &gt; 0, \hat y = 1$ $LOR(\bf x) = w^Tx &lt;0, \hat y = 0$ $\bf w^T \bf x = 0$:å†³ç­–é¢ å› æ­¤$a(\bf x) = w^Txåˆ†ç±»å†³ç­–é¢$ å› æ­¤Logisticå›žå½’æ˜¯ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ æžå¤§ä¼¼ç„¶ä¼°è®¡ Logisticå›žå½’ï¼š$p(y \mid \mathbf {x,w}) = Ber(y \mid \mu (x)), \mu (\mathbf x) = \sigma (\mathbf w^T\mathbf x)$ ä»¤$\mu_i = \mu(\mathbf x_i)$ï¼Œåˆ™è´Ÿlogä¼¼ç„¶ä¸º$\begin{eqnarray} J(w) = NLL(\mathbf w) &amp;=&amp; - \sum_{i=1}^N log \left[(\mu_i)^{y_i} \times (1-\mu_i)^{1-y_i}\right]\\ &amp;=&amp; \sum_{i=1}^N- \left[y_i log(\mu_i)+(1-y_i)log(1-u_i) \right] \end{eqnarray}$ æžå¤§ä¼¼ç„¶ä¼°è®¡ç­‰ä»·äºŽæœ€å°LogisticæŸå¤± ä¼˜åŒ–æ±‚è§£ï¼šæ¢¯åº¦ä¸‹é™ï¼ç‰›é¡¿æ³• æ¢¯åº¦ ç›®æ ‡å‡½æ•°ä¸º$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$ æ¢¯åº¦ä¸º$\begin{eqnarray} g(\bf w) &amp;=&amp; \frac{\partial J(\bf w)}{\partial \bf w} = \frac{\partial}{\partial \bf w}[\sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]]\\&amp;=&amp; \sum_{i=1}^N[\mu(\mathbf x_i) - y_i] \mathbf x_i \\&amp;=&amp; \bf X^T(\mu - y)\end{eqnarray}$ äºŒé˜¶HessiançŸ©é˜µä¸º$\begin{eqnarray} H(w) &amp;=&amp; \frac{\partial}{\partial \mathbf w}[\mathbf g( \mathbf w)^T] = \sum_{i=1}^N(\frac {\partial}{\partial \mathbf w}\mu_i) \mathbf x_i^T\\&amp;=&amp; \sum_{i=1}{N}\mu_i(1-\mu_i)\bf x_ix_i^T = X^T\underset{S}{ \underbrace{diag(\mu_i(1-\mu_i)}}X = X^TSX\end{eqnarray}$ ç‰›é¡¿æ³• äº¦ç§°ç‰›é¡¿-æ‹‰å¤«é€Šï¼ˆ Newton-Raphson ï¼‰æ–¹æ³• ç‰›é¡¿åœ¨17ä¸–çºªæå‡ºçš„ä¸€ç§è¿‘ä¼¼æ±‚è§£æ–¹ç¨‹çš„æ–¹æ³• ä½¿ç”¨å‡½æ•°f(x)çš„æ³°å‹’çº§æ•°çš„å‰é¢å‡ é¡¹æ¥å¯»æ‰¾æ–¹ç¨‹f(x) = 0çš„æ ¹ åœ¨æ±‚æžå€¼é—®é¢˜ä¸­ï¼Œæ±‚$g(\mathbf w) = \frac {\partial J(\mathbf w)}{\partial w} = 0$çš„æ ¹ å¯¹åº”$J(\mathbf w)$å¤„å–æžå€¼ å°†å¯¼æ•°$g(\mathbf w)$åœ¨ $w^t$å¤„è¿›è¡ŒTaylorå±•å¼€ï¼š$0 = \bf g(\hat w) = g(w^t)+(\hat w - w^t)H(w^t) + Op(\hat w - w^t)$ åŽ»æŽ‰é«˜é˜¶æ— ç©·å°$Op(\bf \hat w - w^t)$ï¼Œä»Žè€Œå¾—åˆ°$g(\bf w^t)+(\hat w - w^t)H(w^t) = 0 \Rightarrow \hat w = w^t - H^{-1}(w^t)g(w^t)$ å› æ­¤è¿­ä»£æœºåˆ¶ä¸ºï¼š$\bf w^{t+1} = w^t - H^{-1}(w^t)g(w^t)$ ä¹Ÿè¢«ç§°ä¸ºäºŒé˜¶æ¢¯åº¦ä¸‹é™æ³•ï¼Œç§»åŠ¨æ–¹å‘:$\bf d = -(H(w^t))^{-1}g(w^t)$ Vs. ä¸€é˜¶æ¢¯åº¦æ³•ï¼Œç§»åŠ¨æ–¹å‘:$\bf d = -g(w^t)$ç§»åŠ¨ Iteratively Reweighted Least Squaresï¼ˆIRLSï¼‰ å¼•å…¥è®°å·ï¼š$\bf g^t(w) = X^T(\mu^t - y), \mu_i^t = \sigma((w^t)^Tx_i)$$\bf H^t(w) = X^TS^tX$, S^t:diag(\mu_i^t(1-\mu_1^t),â€¦,\mu_N^t(1-\mu_N^t)) æ ¹æ®ç‰›é¡¿æ³•çš„ç»“æžœï¼š$w^{t+1} = w^t - (H^t)^{-1}g^t = (X^TS^tX)^{-1}X^TS^tz$ å›žå¿†æœ€å°äºŒä¹˜é—®é¢˜ï¼š ç›®æ ‡å‡½æ•°ï¼š$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T(y - Xw)$ è§£ï¼š$\hat w = (X^TX)^{-1}X^Ty$ å›žå¿†åŠ æƒæœ€å°äºŒä¹˜é—®é¢˜ï¼š ç›®æ ‡å‡½æ•°:$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T\Sigma^{-1}(y - Xw)$ è§£ï¼š$\hat w = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}y$ IRLSä¸­ï¼Œ$\bf w^{t+1} = (X^TS^tX)^{-1}X^TS^t[Xw^t + (S^t)^{-1}(y - \mu ^t)]$ ç›¸å½“äºŽæƒé‡çŸ©é˜µä¸º \Sigma^{-1} = \bf S^t ç”±äºŽ$S^t$æ˜¯å¯¹è§’é˜µï¼Œ$S^t$ç›¸å½“äºŽç»™æ¯ä¸ªæ ·æœ¬çš„æƒé‡$S_{ii}^t = \mu_i^t(1-\mu_i^t), \mathbf z_i^t = (\mathbf w^t)^T\mathbf x_i + \frac {y_i - \mu_i^t}{S_{ii}^t}$ æ‹Ÿç‰›é¡¿æ³• ç‰›é¡¿æ³•æ¯”ä¸€èˆ¬çš„æ¢¯åº¦ä¸‹é™æ³•æ”¶æ•›é€Ÿåº¦å¿«ï¼Œä½†æ˜¯åœ¨é«˜ç»´æƒ…å†µä¸‹ï¼Œè®¡ç®—ç›®æ ‡å‡½æ•°çš„äºŒé˜¶åå¯¼æ•°çš„å¤æ‚åº¦å¾ˆå¤§ï¼Œè€Œä¸”æœ‰æ—¶å€™ç›®æ ‡å‡½æ•°çš„æµ·æ£®çŸ©é˜µæ— æ³•ä¿æŒæ­£å®šï¼Œä¸å­˜åœ¨é€†çŸ©é˜µï¼Œæ­¤æ—¶ç‰›é¡¿æ³•å°†ä¸å†èƒ½ä½¿ç”¨ã€‚ å› æ­¤ï¼Œäººä»¬æå‡ºäº†æ‹Ÿç‰›é¡¿æ³•ã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯ï¼šä¸ç”¨äºŒé˜¶åå¯¼æ•°è€Œæž„é€ å‡ºå¯ä»¥è¿‘ä¼¼HessiançŸ©é˜µ(æˆ–HessiançŸ©é˜µçš„é€†çŸ©é˜µ)çš„æ­£å®šå¯¹ç§°çŸ©é˜µï¼Œè¿›è€Œå†é€æ­¥ä¼˜åŒ–ç›®æ ‡å‡½æ•°ã€‚ä¸åŒçš„æž„é€ æ–¹æ³•å°±äº§ç”Ÿäº†ä¸åŒçš„æ‹Ÿç‰›é¡¿æ³•ï¼ˆQuasi-Newton Methodsï¼‰ BFGSï¼LBFGSï¼Newton-CG æ­£åˆ™åŒ–çš„Logisticå›žå½’ è‹¥æŸå¤±å‡½æ•°å–logisticæŸå¤±ï¼Œåˆ™Logisticå›žå½’çš„ç›®æ ‡å‡½æ•°ä¸º$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$ åŒçº¿æ€§å›žå½’ç±»ä¼¼ï¼ŒLogisticå›žå½’äº¦å¯åŠ ä¸ŠL2æ­£åˆ™$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \Vert \mathbf w\Vert ^2_2$ æˆ–L1æ­£åˆ™$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \vert \mathbf w\vert $ L2æ­£åˆ™çš„Logisticå›žå½’æ±‚è§£ æ¢¯åº¦ä¸º: HessiançŸ©é˜µä¸ºï¼š ç±»ä¼¼ä¸å¸¦æ­£åˆ™çš„Logisticå›žå½’ï¼Œå¯é‡‡ç”¨ï¼ˆéšæœºï¼‰æ¢¯åº¦ä¸‹é™ã€ç‰›é¡¿æ³•æˆ–æ‹Ÿç‰›é¡¿æ³•æ±‚è§£ã€‚ L1æ­£åˆ™çš„Logisticå›žå½’æ±‚è§£ L1æ­£åˆ™é¡¹çš„åœ¨0å¤„ä¸å¯å¯¼ åœ¨æ­¤æˆ‘ä»¬L1æ­£åˆ™çš„Logisticå›žå½’çš„ç‰›é¡¿æ³•ï¼ˆIRLSï¼‰æ±‚è§£ éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰åœ¨CTRé¢„ä¼°éƒ¨åˆ†è®²è§£ Recallï¼šIRLS L1æ­£åˆ™çš„Logisticå›žå½’åœ¨æ¯æ¬¡è¿­ä»£ä¸­å¯è§†ä¸ºä¸€ä¸ªå†åŠ æƒçš„Lassoé—®é¢˜ï¼š å°ç»“ Logisticå›žå½’ï¼š æŸå¤±å‡½æ•°ï¼šè´Ÿlogä¼¼ç„¶æŸå¤± æ­£åˆ™ï¼šL2/L1æ­£åˆ™ ä¼˜åŒ–ï¼šæ¢¯åº¦ä¸‹é™ï¼ç‰›é¡¿æ³•ï¼æ‹Ÿç‰›é¡¿æ³• 2ã€Softmaxåˆ†ç±»å™¨3ã€Scikit learnä¸­çš„Logisticå›žå½’å®žçŽ°4ã€ä¸å¹³è¡¡æ•°æ®åˆ†ç±»å­¦ä¹ 5ã€åˆ†ç±»æ¨¡åž‹çš„è¯„ä»·6ã€Logisticå›žå½’ä¹‹æ¨¡åž‹é€‰æ‹©_å‚æ•°è°ƒä¼˜7ã€Logisticå›žå½’-Ottoå•†å“åˆ†ç±»ä»£ç 8ã€æ”¯æŒå‘é‡æœº9ã€å¸¦æ¾å¼›å› å­çš„C-SVM10ã€æ ¸æ–¹æ³•11ã€æ”¯æŒå‘é‡å›žå½’ï¼ˆSVRï¼‰12ã€sklearnä¸­çš„SVMå®žçŽ°13ã€SVM-Otto]]></content>
      <categories>
        <category>å­¦ä¹ ç¬”è®°</category>
      </categories>
      <tags>
        <tag>äººå·¥æ™ºèƒ½</tag>
        <tag>å­¦ä¹ ç¬”è®°</tag>
        <tag>Logisticå›žå½’</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + GitHub Pages + Nextåœ¨windowsä¸‹æ­å»ºä¸ªäººåšå®¢]]></title>
    <url>%2F2019%2F04%2F01%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[æ‰æ­å¥½åšå®¢ï¼Œå‘çŽ°åœ¨åšå®¢å‘å¸ƒæ–‡ç« ç¡®å®žæ¯”å¾®ä¿¡å…¬ä¼—å·æ–¹ä¾¿å¾ˆå¤šï¼Œè¿™é‡Œç®€ç•¥è¯´ä¸‹ç”¨ Hexo + GitHub Pages + Nextæ­å»ºä¸ªäººåšå®¢çš„è¯¾ç¨‹ï¼Œå¤§éƒ¨åˆ†ç»éªŒéƒ½æ˜¯æ¥è‡ªäºŽç½‘ç»œï¼Œæˆ‘ä¼šåœ¨æ•´ä¸ªè¿‡ç¨‹åŽé¢é™„ä¸Šå‚è€ƒçš„æ–‡ç« ï¼Œä¸€æ¥æ€»ç»“æ­å»ºåšå®¢çš„è¿‡ç¨‹ï¼ŒäºŒæ¥å‡å°‘åŽæ¥äººè¸©å‘ã€‚ æ•´ä¸ªè¿‡ç¨‹ï¼š 1ã€æ³¨å†ŒGithubè´¦å·åŠåˆ›å»ºä»“åº“ 2ã€å®‰è£…Git for Windows 3ã€é…ç½®Git 4ã€å®‰è£…node.js 5ã€å®‰è£…Hexo 6ã€ä½¿ç”¨nextè®¾è®¡ä¸ªæ€§åŒ–åšå®¢ 7ã€è¿žæŽ¥Hexoå’ŒGithub PagesåŠéƒ¨ç½²åšå®¢ 8ã€è´­ä¹°åŸŸåå¹¶è§£æžä»¥ä¸Šå°±æ˜¯å…¨éƒ¨çš„è¿‡ç¨‹ï¼Œå½“ç„¶å…·ä½“è¿˜æœ‰å¾ˆå¤šç»†èŠ‚ï¼Œæ¯”å¦‚æ›´æ¢é…ç½®ã€è®¾ç½®æ–‡ç« å­—æ•°çš„å•ä½ï¼Œé˜…è¯»æ—¶å¸¸çš„å•ä½ï¼Œè®¾ç½®è¯„è®ºåŒºï¼Œå…·ä½“çš„ä¸œè¥¿è¿˜æ˜¯è¦ä¾æ®ä¸ªäººçš„å–œå¥½è°ƒæ•´ï¼Œä½†æ˜¯nextä¸»é¢˜é‡Œé¢åŸºæœ¬éƒ½é›†æˆäº†è¿™äº›åŠŸèƒ½ï¼Œåªè¦ç¨å¾®è°ƒæ•´ä¸‹å°±è¡Œã€‚ å‚è€ƒçš„æ–‡ç« ï¼š å‚è€ƒçš„æ•´ä¸ªè¿‡ç¨‹ å„ç§ä¸ªæ€§åŒ–å°åŠŸèƒ½ ç»™ç»Ÿè®¡é‡æ·»åŠ å•ä½ å„ç§ä¸ªæ€§åŒ–è®¾ç½® æ–‡ç« å‘å¸ƒ GitHub/CodingåŒçº¿éƒ¨ç½² å°ä¹¦åŒ Markdownä½¿ç”¨æ‰‹å†Œ åœ¨Markdownä¸­è¾“å…¥æ•°å­¦å…¬å¼(MathJax) Hexo çš„ Next ä¸»é¢˜ä¸­æ¸²æŸ“ MathJax æ•°å­¦å…¬å¼ æŠ¥é”™ï¼šhexo fs.SyncWriteStream is deprecated Hexo Nextä¸»é¢˜åšå®¢åŠŸèƒ½å®Œå–„ MathJaxè¯­æ³• MathJaxä¸ŽLaTexä»‹ç» MathJax(Markdownä¸­çš„å…¬å¼)çš„åŸºæœ¬ä½¿ç”¨è¯­æ³•]]></content>
      <categories>
        <category>æ€»ç»“</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç¬¬ä¸€å‘¨ æœºå™¨å­¦ä¹ ç®€ä»‹ä¸Žçº¿æ€§å›žå½’]]></title>
    <url>%2F2019%2F03%2F31%2F%E7%AC%AC%E4%B8%80%E5%91%A8%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.1 ä¸€ä¸ªKaggleç«žèµ›ä¼˜èƒœè§£å†³æ–¹æ¡ˆ ä¸€ä¸ªKaggleç«žèµ›ä¼˜èƒœè§£å†³æ–¹æ¡ˆ ä»»åŠ¡ï¼šAvazuç‚¹å‡»çŽ‡é¢„ä¼°ç«žèµ› Rank 2nd Owen Zhangçš„è§£æ³• ä¼˜èƒœç®—æ³•çš„ç‰¹ç‚¹ ç‰¹å¾å·¥ç¨‹ èžåˆå¤§æ³• å¤šå±‚ å¤šç§ä¸åŒæ¨¡åž‹çš„ç»„åˆ æ‰€ä»¥ï¼š åŸºç¡€æ¨¡åž‹å¾ˆé‡è¦ï¼ˆçº¿æ€§æ¨¡åž‹ï¼‰ é›†æˆå­¦ä¹ æ¨¡åž‹å•æ¨¡åž‹æ€§èƒ½å¥½ï¼ˆGBDTï¼‰ ç‰¹å®šé—®é¢˜çš„æ¨¡åž‹è´¡çŒ®å¤§ï¼ˆFMï¼‰ æ¨¡åž‹èžåˆå¾ˆé‡è¦ è¯¾ç¨‹å†…å®¹å®‰æŽ’ åŸºæœ¬æ¨¡åž‹ çº¿æ€§æ¨¡åž‹ï¼š çº¿æ€§å›žå½’ï¼Œ logisticå›žå½’ï¼Œ SVM éžçº¿æ€§æ¨¡åž‹ï¼š ï¼ˆçº¿æ€§æ¨¡åž‹æ ¸åŒ–ï¼‰ã€åˆ†ç±»å›žå½’æ ‘ é›†æˆå­¦ä¹ æ¨¡åž‹ï¼ˆéšæœºæ£®æž—ã€GBDTï¼‰ æ•°æ®é¢„å¤„ç†ï¼šæ•°æ®æ¸…æ´—ï¼Œç‰¹å¾å·¥ç¨‹ï¼Œé™ç»´ï¼Œèšç±» æ¨¡åž‹èžåˆ æŽ¨èç³»ç»Ÿ/ç‚¹å‡»çŽ‡é¢„ä¼°é—®é¢˜ç‰¹å®šè§£å†³æ–¹æ¡ˆ 1.2 æœºå™¨å­¦ä¹ ä»»åŠ¡ç±»åž‹ å®šä¹‰ æ•°æ® æ•°æ®é€šå¸¸ä»¥äºŒç»´æ•°æ®è¡¨å½¢å¼ç»™å‡º æ¯ä¸€è¡Œï¼š ä¸€ä¸ªæ ·æœ¬ æ¯ä¸€åˆ—ï¼šä¸€ä¸ªå±žæ€§/ç‰¹å¾ ä¾‹ï¼šBostonæˆ¿ä»·é¢„æµ‹æ•°æ®ï¼Œæ ¹æ®æŸåœ°åŒºæˆ¿å±‹å±žæ€§ï¼Œé¢„æµ‹è¯¥åœ°åŒºé¢„æµ‹æˆ¿ä»· 506è¡Œï¼Œ 506ä¸ªæ ·æœ¬ 14åˆ— æœºå™¨å­¦ä¹ ä»»åŠ¡ç±»åž‹ ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰ åˆ†ç±»ï¼ˆclassficationï¼‰ å›žå½’ï¼ˆregressionï¼‰ æŽ’åºï¼ˆrankingï¼‰ éžç›‘ç£å­¦ä¹ ï¼ˆunsupervised learningï¼‰ èšç±»ï¼ˆclusteringï¼‰ é™ç»´ï¼ˆdimensionality reductionï¼‰ æ¦‚çŽ‡å¯†åº¦ä¼°è®¡ï¼ˆdensity estimationï¼‰ å¢žå¼ºå­¦ä¹ ï¼ˆreinforcement learningï¼‰ åŠç›‘ç£å­¦ä¹ ï¼ˆsemi-supervised learningï¼‰ è¿ç§»å­¦ä¹ ï¼ˆtransfer learningï¼‰ â€¦â€¦ ç›‘ç£å­¦ä¹  å­¦ä¹ ä¸€ä¸ªx-&gt;y çš„æ˜ å°„f, ä»Žè€Œå¯¹æ–°è¾“å…¥çš„xè¿›è¡Œé¢„æµ‹fï¼ˆxï¼‰D = \{X_i,y_i\}^N_{i=1} Dï¼šè®­ç»ƒæ•°æ®é›† Nï¼šè®­ç»ƒæ ·æœ¬æ•°ç›® $X_i$: ç¬¬iä¸ªæ ·æœ¬çš„è¾“å…¥ï¼Œäº¦è¢«ç§°ä¸ºç‰¹å¾ã€å±žæ€§æˆ–åå˜é‡ $y_i$: ç¬¬iä¸ªè®­ç»ƒæ ·æœ¬çš„è¾“å‡ºï¼Œäº¦è¢«ç§°ä¸ºå“åº”ï¼Œå¦‚ç±»åˆ«æ ‡ç­¾ã€åºå·æˆ–æ•°å€¼ ä¾‹ï¼šæ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹ å›žå½’ è‹¥è¾“å‡ºyâˆˆRä¸ºè¿žç»­å€¼ï¼Œåˆ™æˆ‘ä»¬ç§°ä¹‹ä¸ºä¸€ä¸ªå›žå½’ï¼ˆregressionï¼‰ä»»åŠ¡ä¾‹ï¼š æˆ¿ä»·é¢„æµ‹ï¼Œé¢„æµ‹äºŒæ‰‹è½¦çš„ä»·æ ¼ å‡è®¾å›žå½’æ¨¡åž‹ï¼š$y = f(\mathbf x|\theta)$ å¦‚åœ¨çº¿æ€§å›žå½’ä¸­ï¼Œ$f(\mathbf x|w) = \mathbf w^T \mathbf x$ è®­ç»ƒï¼šæ ¹æ®è®­ç»ƒæ•°æ® $D = \{\mathbf X_i,y_i\}^N_{i=1}$ å­¦ä¹ æ˜ å°„ é¢„æµ‹ï¼šå¯¹æ–°çš„æµ‹è¯•æ•°æ®xè¿›è¡Œé¢„æµ‹ï¼š$\hat f = f(x)$ yå¸¦å¸½è¡¨ç¤ºé¢„æµ‹ å­¦ä¹ ç›®æ ‡ï¼šè®­ç»ƒé›†ä¸Šé¢„æµ‹å€¼ä¸ŽçœŸå€¼ä¹‹é—´çš„å·®å¼‚æœ€å° æŸå¤±å‡½æ•°ï¼šåº¦é‡æ¨¡åž‹é¢„æµ‹å€¼ä¸ŽçœŸå€¼ä¹‹é—´çš„å·®å¼‚ï¼Œå¦‚L(f(\mathbf x),y) = \frac 12(f(x) - y)^2 ç›®æ ‡å‡½æ•°ä¸ºï¼š$J(\mathbf \theta) = \frac1N \sum_{i = 1}^N L(f(\mathbf x_i|\mathbf \theta), y_i)$ åˆ†ç±» è‹¥è¾“å‡ºyä¸ºç¦»æ•£å€¼ï¼Œåˆ™æˆ‘ä»¬ç§°ä¹‹ä¸ºä¸€ä¸ªåˆ†ç±»ï¼Œæ ‡ç­¾ç©ºé—´y = {1,2, â€¦ C} ä¾‹ï¼šä¿¡ç”¨è¯„åˆ† åˆ†ç±»ï¼š å­¦ä¹ ä»Žè¾“å…¥xåˆ°è¾“å‡ºyçš„æ˜ å°„f:æ¦‚çŽ‡é—®é¢˜$\hat y = f(\mathbf x) = \underset{c} {arg\ max} \ p(y = c\mid \mathbf x, D)$ å­¦ä¹ ç›®æ ‡ï¼š æŸå¤±å‡½æ•°ï¼š01æŸå¤± l_{0/1}(y, \hat y) = \begin {cases} 0 & y = \hat y \\ 1 & otherwise \end{cases} éœ€è¦é¢„æµ‹çš„æ¦‚çŽ‡ï¼š é¢„æµ‹ï¼šæœ€å¤§åŽéªŒä¼°è®¡ï¼ˆMaximum a Posteriori, MAPï¼‰$\hat y = f(\mathbf x) = \underset{c} {arg\ max}\ p(y = c\mid \mathbf x, D)$ æŽ’åºï¼ˆRankï¼‰ æŽ’åºå­¦ä¹ æ˜¯æŽ¨èã€æœç´ ã€å¹¿å‘Šçš„æ ¸å¿ƒæ–¹æ³• æŽ’åºå­¦ä¹ ä¸­éœ€è¦é¦–å…ˆæ ¹æ®æŸ¥è¯¢qåŠå…¶æ–‡æ¡£é›†åˆè¿›è¡Œæ ‡æ³¨ï¼ˆdata labelingï¼‰ å’Œæå–ç‰¹å¾ï¼ˆfeature extractionï¼‰ æ‰èƒ½å¾—åˆ°D = {â€¦.} éžç›‘ç£å­¦ä¹  å‘çŽ°æ•°æ®ä¸­çš„â€œæœ‰æ„ä¹‰çš„æ¨¡å¼â€ï¼Œ äº¦è¢«ç§°ä¸ºçŸ¥è¯†å‘çŽ° è®­ç»ƒæ•°æ®ä¸åŒ…å«æ ‡ç­¾ æ ‡ç­¾åœ¨è®­ç»ƒæ•°æ®ä¸­ä¸ºéšå«å˜é‡ $ D = \{ \bf X_i\}_{ i= 1}^ N $ èšç±»ä¾‹ï¼šäººçš„â€œç±»åž‹â€åˆ†å¤šå°‘ç±»ï¼Ÿ æ¨¡åž‹é€‰æ‹©$ K^* = arg\ max _K\ p(K \mid D)$æŸä¸ªæ ·æœ¬å±žäºŽå“ªä¸ªç±»ï¼Ÿ é™ç»´å¤šç»´ç‰¹å¾ï¼Œæœ‰äº›ç‰¹å¾ä¹‹é—´ä¼šç›¸å…³è€Œå­˜åœ¨å†—ä½™å¾ˆå¤šç®—æ³•ä¸­ï¼Œé™ç»´ç®—æ³•æˆä¸ºäº†æ•°æ®é¢„å¤„ç†çš„ä¸€éƒ¨åˆ†ï¼Œ å¦‚ä¸»æˆåˆ†åˆ†æžï¼ˆPrincipal Components Analysis, PCAï¼‰ åŠç›‘ç£å­¦ä¹  å½“æ ‡æ³¨æ•°æ®â€œæ˜‚è´µâ€æ—¶æœ‰ç”¨ ä¾‹ï¼šæ ‡æ³¨3Då§¿æ€ã€ è›‹ç™½è´¨åŠŸèƒ½ç­‰ç­‰ å¤šæ ‡ç­¾å­¦ä¹  æœ‰æ­§ä¹‰æ ‡ç­¾å­¦ä¹  å¤šå®žä¾‹å­¦ä¹  å¢žå¼ºå­¦ä¹ ä»Žè¡Œä¸ºçš„åé¦ˆ(å¥–åŠ±æˆ–æƒ©ç½š)ä¸­å­¦ä¹  è®¾è®¡ä¸€ä¸ªå›žæŠ¥å‡½æ•°ï¼ˆreward functionï¼‰ï¼Œ å¦‚æžœlearning agent(å¦‚æœºå™¨äººã€å›´æ£‹aiç¨‹åº)ï¼Œåœ¨å†³å®šä¸€æ­¥ä¹‹åŽï¼ŒèŽ·å¾—äº†è¾ƒå¥½çš„ç»“æžœï¼Œé‚£ä¹ˆæˆ‘ä»¬ç»™agentä¸€äº›å›žæŠ¥ï¼ˆæ¯”å¦‚å›žæŠ¥å‡½æ•°ç»“æžœä¸ºæ­£ï¼‰ï¼Œå¾—åˆ°è¾ƒå·®çš„ç»“æžœï¼Œé‚£ä¹ˆå›žæŠ¥å‡½æ•°ä¸ºè´Ÿ å¢žå¼ºå­¦ä¹ çš„ä»»åŠ¡ï¼šæ‰¾åˆ°ä¸€æ¡å›žæŠ¥å€¼æœ€å¤§çš„è·¯å¾„ 1.3 ä¸€ä¸ªå…¸åž‹çš„æœºå™¨å­¦ä¹ æ¡ˆä¾‹-å¯¹é±¼è¿›è¡Œåˆ†ç±» æ ¹æ®ä¸€äº›å…‰å­¦ä¼ æ„Ÿå™¨å¯¹ä¼ é€å¸¦ä¸Šçš„é±¼è¿›è¡Œåˆ†ç±» å½¢å¼åŒ–ä¸ºæœºå™¨å­¦ä¹ é—®é¢˜ è®­ç»ƒæ•°æ® æ¯æ¡é±¼çš„æµ‹é‡å‘é‡ æ¯æ¡é±¼çš„æ ‡ç­¾ æµ‹è¯• ç»™å®šä¸€ä¸ªæ–°çš„ç‰¹å¾å‘é‡x é¢„æµ‹å¯¹åº”çš„æ ‡ç­¾y å°†é•¿åº¦ä½œä¸ºç‰¹å¾è¿›è¡Œåˆ†ç±»ï¼ˆç›´æ–¹å›¾ï¼‰ éœ€è¦å…ˆåšä¸€ä¸ªå†³ç­–è¾¹ç•Œ æœ€å°åŒ–å¹³å‡æŸå¤± å°†äº®åº¦ä½œä¸ºç‰¹å¾è¿›è¡Œåˆ†ç±» ï¼ˆç›´æ–¹å›¾ï¼‰ å°†é•¿åº¦å’Œäº®åº¦ä¸€èµ·ä½œä¸ºç‰¹å¾ï¼ˆäºŒç»´æ•£ç‚¹å›¾ï¼‰ çº¿æ€§å†³ç­–å‡½æ•° äºŒæ¬¡å†³ç­–å‡½æ•° æ›´å¤æ‚çš„å†³ç­–è¾¹ç•Œè®­ç»ƒé›†ä¸Šçš„è¯¯å·® â‰  æµ‹è¯•é›†ä¸Šçš„è¯¯å·®æ•°æ®è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰æŽ¨å¹¿æ€§ï¼ˆgeneralizationï¼‰å·® å°ç»“ï¼šè®¾è®¡ä¸€ä¸ªé±¼çš„åˆ†ç±»å™¨ é€‰æ‹©ç‰¹å¾ å¯èƒ½æ˜¯æœ€é‡è¦çš„æ­¥éª¤ï¼ï¼ˆæ”¶é›†è®­ç»ƒæ•°æ®ï¼‰ é€‰æ‹©æ¨¡åž‹ï¼ˆå¦‚å†³ç­–è¾¹ç•Œçš„å½¢çŠ¶ï¼‰ æ ¹æ®è®­ç»ƒæ•°æ®ä¼°è®¡æ¨¡åž‹ åˆ©ç”¨æ¨¡åž‹å¯¹æ–°æ ·æœ¬è¿›è¡Œåˆ†ç±» 1.4 æœºå™¨å­¦ä¹ ç®—æ³•çš„ç»„æˆéƒ¨åˆ† æœºå™¨å­¦ä¹ ä»»åŠ¡çš„ä¸€èˆ¬æ­¥éª¤ ç¡®å®šç‰¹å¾ å¯èƒ½æ˜¯æœ€é‡è¦çš„æ­¥éª¤ï¼ï¼ˆæ”¶é›†è®­ç»ƒæ•°æ®ï¼‰ ç¡®å®šæ¨¡åž‹ ç›®æ ‡å‡½æ•°/å†³ç­–è¾¹ç•Œå½¢çŠ¶ æ¨¡åž‹è®­ç»ƒï¼šæ ¹æ®è®­ç»ƒæ•°æ®ä¼°è®¡æ¨¡åž‹å‚æ•° ä¼˜åŒ–è®¡ç®— æ¨¡åž‹è¯„ä¼°ï¼šåœ¨æ ¡éªŒé›†ä¸Šè¯„ä¼°æ¨¡åž‹é¢„æµ‹æ€§èƒ½ æ¨¡åž‹åº”ç”¨/é¢„æµ‹ æ¨¡åž‹ ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼š$D = \{X_i, y_i\} _{i = 1} ^ N $ æ¨¡åž‹ï¼šå¯¹ç»™å®šçš„è¾“å…¥x, å¦‚ä½•é¢„æµ‹å…¶æ ‡ç­¾$ \hat y$ ä¸åŒæ¨¡åž‹å¯¹æ•°æ®çš„å‡è®¾ä¸åŒ æœ€ç®€å•çš„æ¨¡åž‹ï¼šçº¿æ€§æ¨¡åž‹$ f(x) = \sum_j w_j x_j = \bf w^T \bf x$ ç¡®å®šæ¨¡åž‹ç±»åˆ«åŽï¼Œæ¨¡åž‹è®­ç»ƒè½¬åŒ–ä¸ºæ±‚è§£æ¨¡åž‹å‚æ•° å¦‚å¯¹çº¿æ€§æ¨¡åž‹å‚æ•°ä¸º$\theta = \{w_j \mid j = 1,â€¦, D\}$,å…¶ä¸­Dä¸ºç‰¹å¾ç»´æ•° æ±‚è§£æ¨¡åž‹å‚æ•°ï¼šç›®æ ‡å‡½æ•°æœ€å°åŒ– éžçº¿æ€§æ¨¡åž‹ åŸºå‡½æ•°ï¼š $x^2$, log, exp, æ ·æ¡å‡½æ•°ï¼Œå†³ç­–æ ‘â€¦. æ ¸åŒ–ï¼šå°†åŽŸé—®é¢˜è½¬åŒ–ä¸ºå¯¹å¶é—®é¢˜ï¼Œå°†å¯¹å¶é—®é¢˜ä¸­çš„å‘é‡ç§¯$\langle x_i, x_j\rangle$ æ¢æˆæ ¸å‡½æ•°$k(x_i,x_j)$ ç›®æ ‡å‡½æ•°ï¼šé€šå¸¸åŒ…å«ä¸¤é¡¹ï¼šæŸå¤±å‡½æ•°å’Œæ­£åˆ™é¡¹J(\theta) = \frac 1N \sum_{i=1}^N\ L(f(x_i; \theta), y_i) + R(\theta) æŸå¤±å‡½æ•° æŸå¤±å‡½æ•° - å›žå½’ æŸå¤±å‡½æ•°ï¼šåº¦é‡æ¨¡åž‹é¢„æµ‹å€¼ä¸ŽçœŸå€¼ä¹‹é—´çš„å·®å¼‚ å¯¹å›žå½’é—®é¢˜ï¼šä»¤æ®‹å·® $r = f(\bf x) - y$ L2æŸå¤±ï¼šè¿žç»­ï¼Œä½†å¯¹å™ªå£°æ•æ„ŸL_2 (r) = \frac 12 r ^2 L1æŸå¤±ï¼šä¸è¿žç»­ï¼Œå¯¹å™ªå£°ä¸æ•æ„ŸL_1(r) = |r| Huber æŸå¤±ï¼š è¿žç»­ï¼Œå¯¹å™ªå£°ä¸æ•æ„ŸL_\delta (r) = \begin{cases} \frac 12 r^2 & if|r| \le \delta\\ \delta |r| - \frac 12 \delta^2 & if|r| \ge \delta\end{cases} æŸå¤±å‡½æ•° - åˆ†ç±» æŸå¤±å‡½æ•°ï¼šåº¦é‡æ¨¡åž‹é¢„æµ‹å€¼ä¸ŽçœŸå€¼ä¹‹é—´çš„å·®å¼‚ å¯¹åˆ†ç±»é—®é¢˜ 0-1æŸå¤±ï¼š$l_{0/1}(y,f(x)) = \begin{cases} 1 &amp; yf(x) \lt 0 \\ 0 &amp; othereise\end{cases}$ logisticæŸå¤±ï¼šäº¦ç§°è´Ÿlogä¼¼ç„¶æŸå¤± $l_{log}(y,f(x)) = log(1 + exp(-yf(x)))$ æŒ‡æ•°æŸå¤±ï¼š$l_{exp}(y,f(x)) = exp(-yf(x))$ åˆé¡µæŸå¤±ï¼š$l_{hinge}(y,f(x)) = max(0, 1 - yf(x))$ æ­£åˆ™é¡¹ å¤æ‚æ¨¡åž‹ï¼ˆé¢„æµ‹ï¼‰ä¸ç¨³å®šï¼šæ–¹å·®å¤§ æ­£åˆ™é¡¹å¯¹å¤æ‚æ¨¡åž‹æ–½åŠ æƒ©ç½š æ­£åˆ™é¡¹çš„å¿…è¦æ€§ä¾‹ï¼šsinæ›²çº¿æ‹Ÿåˆ å¢žåŠ L2æ­£åˆ™å²­å›žå½’ï¼šæœ€å°åŒ–RSS æ¬ æ‹Ÿåˆï¼šæ¨¡åž‹å¤ªç®€å•/å¯¹å¤æ‚æ€§æƒ©ç½šå¤ªå¤š æ ·æœ¬æ•°ç›®å¢žå¤šæ—¶ï¼Œå¯ä»¥è€ƒè™‘æ›´å¤æ‚çš„æ¨¡åž‹ å¸¸è§æ­£åˆ™é¡¹ L2æ­£åˆ™: $R(\theta) = \lambda ||\theta||^2_2 = \lambda \sum^D_{j=1} \theta_j^2$ L1æ­£åˆ™: $R(\theta) = \lambda |\theta| = \lambda \sum ^D_{j=1}|\theta_j|$ L0æ­£åˆ™: $R(\theta) = \lambda||\theta||_ 0$ éž0å‚æ•°çš„æ•°ç›® ä¸å¥½ä¼˜åŒ–ï¼Œé€šå¸¸ç”¨L1æ­£åˆ™è¿‘ä¼¼ å¸¸è§çº¿æ€§æ¨¡åž‹çš„æŸå¤±å’Œæ­£åˆ™é¡¹ç»„åˆ L2æŸå¤± L1æŸå¤± HuberæŸå¤± LogisticæŸå¤± åˆé¡µæŸå¤± e-insensitiveæŸå¤± L2æ­£åˆ™ å²­å›žå½’ L2æ­£åˆ™ Logisticå›žå½’ SVM SVR L1æ­£åˆ™ LASSO L1æ­£åˆ™ Logisticå›žå½’ L2+L1æ­£åˆ™ Elastic æ¨¡åž‹è®­ç»ƒ åœ¨è®­ç»ƒæ•°æ®ä¸Šæ±‚ç›®æ ‡å‡½æ•°æžå°å€¼ï¼šä¼˜åŒ– ç®€å•ç›®æ ‡å‡½æ•°ç›´æŽ¥æ±‚è§£ å¦‚å°æ•°æ®é›†ä¸Šçš„çº¿æ€§å›žå½’ æ›´å¤æ‚é—®é¢˜ï¼šå‡¸ä¼˜åŒ– ï¼ˆéšæœºï¼‰æ¢¯åº¦ä¸‹é™ ç‰›é¡¿æ³•/æ‹Ÿç‰›é¡¿æ³• â€¦ æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰ç®—æ³• æ¢¯åº¦ä¸‹é™/æœ€é€Ÿä¸‹é™ç®—æ³•ï¼šå¿«é€Ÿå¯»æ‰¾å‡½æ•°å±€éƒ¨æžå°å€¼ æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼šæ±‚å‡½æ•°Jï¼ˆÎ¸ï¼‰çš„æœ€å°å€¼ ç»™å®šåˆå§‹å€¼$Î¸^0$ æ›´æ–°Î¸ï¼Œä½¿å¾—Jï¼ˆÎ¸ï¼‰è¶Šæ¥è¶Šå° $Î¸^t = Î¸^{t-1} - \eta\nabla J(Î¸)$ ( $\eta$ : å­¦ä¹ çŽ‡ ) ç›´åˆ°æ”¶æ•›åˆ° / è¾¾åˆ°é¢„å…ˆè®¾å®šçš„æœ€å¤§è¿­ä»£æ¬¡æ•° ä¸‹é™çš„æ­¥ä¼å¤ªå°ï¼ˆå­¦ä¹ çŽ‡ï¼‰éžå¸¸é‡è¦ï¼šå¦‚æžœå¤ªå°ï¼Œæ”¶æ•›é€Ÿåº¦æ…¢ï¼› å¦‚æžœå¤ªå¤§ï¼Œå¯èƒ½ä¼šå‡ºçŽ°overshoot the minimumçš„çŽ°è±¡ æ¢¯åº¦ä¸‹é™æ±‚å¾—çš„åªæ˜¯å±€éƒ¨æœ€å°å€¼ äºŒé˜¶å¯¼æ•° &gt; 0, åˆ™ç›®æ ‡å‡½æ•°ä¸ºå‡¸å‡½æ•°ï¼Œå±€éƒ¨æžå°å€¼å³ä¸ºå…¨å±€æœ€å°å€¼ éšæœºé€‰æ‹©å¤šä¸ªåˆå§‹å€¼ï¼Œå¾—åˆ°å‡½æ•°çš„å¤šä¸ªå±€éƒ¨æžå°å€¼ç‚¹ã€‚å¤šä¸ªå±€éƒ¨æžå°å€¼ç‚¹çš„æœ€å°å€¼ä¸ºå‡½æ•°çš„å…¨å±€æœ€å°å€¼ æ¢¯åº¦ä¸‹é™ç®—æ³•æ¯æ¬¡å­¦ä¹ éƒ½ä½¿ç”¨æ•´ä¸ªè®­ç»ƒé›†ï¼Œè¿™æ ·å¯¹å¤§çš„è®­ç»ƒæ•°æ®é›†åˆï¼Œæ¯æ¬¡å­¦ä¹ æ—¶é—´è¿‡é•¿ï¼Œå¯¹å¤§çš„è®­ç»ƒé›†éœ€è¦æ¶ˆè€—å¤§é‡çš„å†…å­˜ã€‚æ­¤æ—¶å¯é‡‡ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic gradient descent, SGD), æ¯æ¬¡ä»Žè®­ç»ƒé›†ä¸­éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†æ ·æœ¬è¿›è¡Œå­¦ä¹ ã€‚ æ›´å¤šï¼ˆéšæœºï¼‰æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ”¹è¿›ç‰ˆ åŠ¨é‡ï¼ˆMomentumï¼‰ Nesterov accelerated gradient (NAG) Adagrad RMSprop Adaptive Moment Estimation (Adam)â€¦ æ¨¡åž‹é€‰æ‹©ä¸Žæ¨¡åž‹è¯„ä¼° åŒä¸€ä¸ªé—®é¢˜æœ‰ä¸åŒçš„è§£å†³æ–¹æ¡ˆ å¦‚çº¿æ€§å›žå½’ vs. å†³ç­–æ ‘ å“ªä¸ªæ›´å¥½ï¼Ÿ æ¨¡åž‹è¯„ä¼°ä¸Žæ¨¡åž‹é€‰æ‹© åœ¨æ–°æ•°æ®ç‚¹çš„é¢„æµ‹è¯¯å·®æœ€å° æ¨¡åž‹è¯„ä¼°ï¼šå·²ç»é€‰å®šæœ€ç»ˆçš„æ¨¡åž‹ï¼Œä¼°è®¡å®ƒåœ¨æ–°æ•°æ®ä¸Šçš„é¢„æµ‹è¯¯å·® æ¨¡åž‹é€‰æ‹©ï¼šä¼°è®¡ä¸åŒæ¨¡åž‹çš„æ€§èƒ½ï¼Œé€‰å‡ºæœ€å¥½çš„æ¨¡åž‹ æ ·æœ¬è¶³å¤Ÿå¤šï¼šè®­ç»ƒé›†å’Œæ ¡éªŒé›† æ ·æœ¬ä¸å¤Ÿå¤šï¼šé‡é‡‡æ ·æŠ€æœ¯æ¥æ¨¡æ‹Ÿæ ¡éªŒé›†ï¼šäº¤å‰éªŒè¯å’Œbootstrap K-æŠ˜äº¤å‰éªŒè¯ äº¤å‰éªŒè¯ï¼ˆCross Validation, CVï¼‰ï¼š å°†è®­ç»ƒæ•°æ®åˆ†æˆå®¹é‡å¤§è‡´ç›¸ç­‰çš„Kä»½ï¼ˆé€šå¸¸K = 5/10ï¼‰ äº¤å‰éªŒè¯ä¼°è®¡çš„è¯¯å·®ä¸ºï¼šCV(M)= \frac1K \sum ^K_{k = 1} E_k(M) æ¨¡åž‹é€‰æ‹© å¯¹å¤šä¸ªä¸åŒæ¨¡åž‹ï¼Œè®¡ç®—å…¶å¯¹åº”çš„è¯¯å·®CVï¼ˆMï¼‰ï¼Œ æœ€ä½³æ¨¡åž‹ä¸ºCVï¼ˆMï¼‰æœ€å°çš„æ¨¡åž‹ æ¨¡åž‹å¤æ‚åº¦å’Œæ³›åŒ–è¯¯å·®çš„å…³ç³»é€šå¸¸æ˜¯Uå½¢æ›²çº¿ï¼š 1.5 å­¦ä¹ çŽ¯å¢ƒç®€ä»‹ ç¼–ç¨‹è¯­è¨€ Python æ•°æ®å¤„ç†å·¥å…·åŒ… Numpy SciPy pandas æ•°æ®å¯è§†åŒ–å·¥å…·åŒ… Matplotlib Seaborn æœºå™¨å­¦ä¹ å·¥å…·åŒ… scikit learn ç¤ºä¾‹ä»£ç ï¼šINotebook NumPy NumPy(Numeric Python)æ˜¯Pythonçš„å¼€æºæ•°å€¼è®¡ç®—æ‰©å±•ï¼Œå¯ç”¨æ¥å­˜å‚¨å’Œå¤„ç†å¤§åž‹çŸ©é˜µ NumpyåŒ…æ‹¬ï¼š Nç»´æ•°ç»„(ndarray) å®žç”¨çš„çº¿æ€§ä»£æ•°ã€å‚…é‡Œå¶å˜æ¢å’Œéšæœºæ•°ç”Ÿæˆå‡½æ•° Numpyå’Œç¨€ç–çŸ©é˜µè¿ç®—åŒ…SciPyé…åˆä½¿ç”¨æ›´åŠ æ–¹ä¾¿ SciPy SciPyæ˜¯å»ºç«‹åœ¨NumPyçš„åŸºç¡€ä¸Šã€æ˜¯ç§‘å­¦å’Œå·¥ç¨‹è®¾è®¡çš„Pythonå·¥å…·åŒ…ï¼Œæä¾›ç»Ÿè®¡ã€ä¼˜åŒ–å’Œæ•°å€¼å¾®ç§¯åˆ†è®¡ç®—ç­‰åŠŸèƒ½ NumPy å¤„ç†$10^6$çº§åˆ«çš„æ•°æ®é€šå¸¸æ²¡æœ‰å¤§é—®é¢˜ï¼Œä½†å½“æ•°æ®é‡è¾¾åˆ°$10^7$çº§åˆ«æ—¶é€Ÿåº¦å¼€å§‹å‘æ…¢ï¼Œå†…å­˜å—åˆ°é™åˆ¶ï¼ˆå…·ä½“æƒ…å†µå–å†³äºŽå®žé™…å†…å­˜çš„å¤§å°ï¼‰ å½“å¤„ç†è¶…å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¯”å¦‚$10^{10}$çº§åˆ«ï¼Œä¸”æ•°æ®ä¸­åŒ…å«å¤§é‡çš„0æ—¶ï¼Œå¯é‡‡ç”¨ç¨€ç–çŸ©é˜µå¯æ˜¾è‘—çš„æé«˜é€Ÿåº¦å’Œæ•ˆçŽ‡ Pandas(Pandel data structures) Pandasæ˜¯Pythonè¯­è¨€çš„â€œå…³ç³»åž‹æ•°æ®åº“â€æ•°æ®ç»“æž„å’Œæ•°æ®åˆ†æžå·¥å…·ï¼Œéžå¸¸é«˜æ•ˆä¸”æ˜“äºŽä½¿ç”¨ åŸºäºŽNumPyè¡¥å……äº†å¤§é‡æ•°æ®æ“ä½œåŠŸèƒ½ï¼Œèƒ½å®žçŽ°ç»Ÿè®¡ã€åˆ†ç»„ã€æŽ’åºã€é€è§†è¡¨(SQLè¯­å¥çš„å¤§éƒ¨åˆ†åŠŸèƒ½) Pandasä¸»è¦æœ‰2ç§é‡è¦çš„æ•°æ®ç±»åž‹ seriesï¼šä¸€ç»´åºåˆ— DataFrameï¼šäºŒç»´è¡¨(æœºå™¨å­¦ä¹ æ•°æ®çš„å¸¸ç”¨æ•°æ®ç»“æž„) Matplotlib Matplotlibæ˜¯Pythonè¯­è¨€çš„2Då›¾å½¢ç»˜åˆ¶å·¥å…· Seaborn Seabornæ˜¯ä¸€ä¸ªåŸºäºŽMatplotlibçš„Pythonå¯è§†åŒ–å·¥å…·åŒ…ï¼Œæä¾›æ›´é«˜å±‚æ¬¡çš„ç”¨æˆ·æŽ¥å£ï¼Œå¯ä»¥ç»™å‡ºæ¼‚äº®çš„æ•°æ®ç»Ÿè®¡ Scikit - Learn Machine Learning in Python Scikit-Learnæ˜¯åŸºäºŽPythonçš„å¼€æºæœºå™¨å­¦ä¹ æ¨¡å—ï¼Œæœ€æ—©äºŽ2007å¹´ç”±David Cournapeauå‘èµ· åŸºæœ¬åŠŸèƒ½æœ‰å…­éƒ¨åˆ†ï¼šåˆ†ç±»ï¼ˆClassificationï¼‰ï¼Œå›žå½’ï¼ˆRegressionï¼‰ï¼Œèšç±»ï¼ˆClusteringï¼‰ï¼Œæ•°æ®é™ç»´ï¼ˆDimensionality reductionï¼‰ï¼Œæ¨¡åž‹é€‰æ‹©ï¼ˆModel Selectionï¼‰ï¼Œæ•°æ®é¢„å¤„ç†ï¼ˆPreprocessingï¼‰ å¯¹äºŽå…·ä½“çš„æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œé€šå¸¸å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ æ•°æ®å‡†å¤‡ä¸Žé¢„å¤„ç†ï¼ˆPreprocessing, Dimensionality reductionï¼‰ æ¨¡åž‹é€‰æ‹©ä¸Žè®­ç»ƒï¼ˆClassification, Regression, Clusteringï¼‰ æ¨¡åž‹éªŒè¯ä¸Žå‚æ•°è°ƒä¼˜ï¼ˆModel Selectionï¼‰ å„ç§æœºå™¨å­¦ä¹ æ¨¡åž‹æœ‰ç»Ÿä¸€çš„æŽ¥å£ æ¨¡åž‹æ—¢æœ‰é»˜è®¤å‚æ•°ï¼Œä¹Ÿæä¾›å¤šç§å‚æ•°è°ƒä¼˜æ–¹æ³• å“è¶Šçš„æ–‡æ¡£ ä¸°å¯Œçš„éšé™„ä»»åŠ¡åŠŸèƒ½é›†åˆ æ´»è·ƒçš„ç¤¾åŒºæä¾›å¼€å‘å’Œæ”¯æŒ 1.6 çº¿æ€§å›žå½’æ¨¡åž‹ ç›®æ ‡å‡½æ•°é€šå¸¸åŒ…å«ä¸¤é¡¹ï¼šæŸå¤±å‡½æ•°å’Œæ­£åˆ™é¡¹J(\bf \theta) = \frac1N \sum_{i = 1}^N L(f(\bf x_i|\bf \theta), y_i) + \lambda R(\bf \theta) å¯¹å›žå½’é—®é¢˜ï¼ŒæŸå¤±å‡½æ•°å¯ä»¥é‡‡ç”¨L2æŸå¤±ï¼Œå¾—åˆ°\begin{eqnarray}J(\theta) &=&\sum_{i=1}^NL(y_i,\hat y_i) \\ &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\ &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} æ®‹å·®å¹³æ–¹å’Œï¼ˆresidual sum of squares, RSSï¼‰ ç”±äºŽçº¿æ€§æ¨¡åž‹æ¯”è¾ƒç®€å•ï¼Œå®žé™…åº”ç”¨ä¸­æœ‰æ—¶æ­£åˆ™é¡¹ä¸ºç©ºï¼Œå¾—åˆ°æœ€å°äºŒä¹˜çº¿æ€§å›žå½’ï¼ˆOrdinary Least Square, OLSï¼‰\begin{eqnarray}J(\theta) &=&\sum_{i=1}^NL(y_i,\hat y_i) &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\ &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} æ­£åˆ™é¡¹å¯ä»¥ä¸ºL2æ­£åˆ™ï¼Œå¾—åˆ°å²­å›žå½’ï¼ˆRidge Regressionï¼‰J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda ||w||^2_2 æ­£åˆ™é¡¹ä¹Ÿå¯ä»¥é€‰L1æ­£åˆ™ï¼Œå¾—åˆ°Lassoæ¨¡åž‹ï¼š J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda |w| å½“$\lambda$å–åˆé€‚å€¼æ—¶ï¼ŒLassoï¼ˆLeast absolute shrinkage and selection operatorï¼‰çš„ç»“æžœæ˜¯ç¨€ç–çš„ï¼ˆwçš„æŸäº›å…ƒç´ ç³»æ•°ä¸º0ï¼‰ï¼Œèµ·åˆ°ç‰¹å¾é€‰æ‹©ä½œç”¨ ä¸ºä»€ä¹ˆL1æ­£åˆ™çš„è§£æ˜¯ç¨€ç–çš„ï¼Ÿ çº¿æ€§å›žå½’æ¨¡åž‹çš„æ¦‚çŽ‡è§£é‡Š æœ€å°äºŒä¹˜ï¼ˆçº¿æ€§ï¼‰å›žå½’ç­‰ä»·äºŽæžå¤§ä¼¼ç„¶ä¼°è®¡ å‡è®¾ï¼š$ y = f(\bf x) + \epsilon = w^Tx + \epsilon $å…¶ä¸­$\epsilon$ä¸ºçº¿æ€§é¢„æµ‹å’ŒçœŸå€¼ä¹‹é—´çš„æ®‹å·®æˆ‘ä»¬é€šå¸¸å‡è®¾æ®‹å·®çš„åˆ†å¸ƒä¸º$\epsilon \sim N(0,\sigma ^2)$,å› æ­¤çº¿æ€§å›žå½’å¯å†™æˆï¼š$p(y|x,\theta) \sim N(y| \bf w^T \bf x, \sigma^2)$,å…¶ä¸­$ \bf \theta = (\bf w, \sigma ^2)$ æ­£åˆ™ï¼ˆçº¿æ€§ï¼‰å›žå½’ç­‰ä»·äºŽé«˜æ–¯å…ˆéªŒï¼ˆL2æ­£åˆ™ï¼‰æˆ–Laplaceå…ˆéªŒä¸‹ï¼ˆL1æ­£åˆ™ï¼‰çš„è´å¶æ–¯ä¼°è®¡ Recallï¼šæžå¤§ä¼¼ç„¶ä¼°è®¡ æžå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMaximize Likelihood Estimator, MLEï¼‰å®šä¹‰ä¸º\hat \theta = \underset {\theta} {arg\ max}\ log\ p(D\mid \theta) å…¶ä¸­ï¼ˆlogï¼‰ä¼¼ç„¶å‡½æ•°ä¸ºl(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) è¡¨ç¤ºåœ¨å‚æ•°ä¸º$\theta$çš„æƒ…å†µä¸‹ï¼Œæ•°æ®$D ={\bf x_i,y_i}^N_{i=1}$ æžå¤§ä¼¼ç„¶ï¼šé€‰æ‹©æ•°æ®å‡ºçŽ°æ¦‚çŽ‡æœ€å¤§çš„å‚æ•° çº¿æ€§å›žå½’çš„MLEp(y_i|x_i,\bf w,\sigma ^2) \sim N(y_i\mid \bf w^T \bf x_i, \sigma^2) = \frac 1{\sqrt{2\pi}\sigma} exp(-\frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2)) OLSçš„ä¼¼ç„¶å‡½æ•°ä¸ºl(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) æžå¤§ä¼¼ç„¶å¯ç­‰ä»·åœ°å†™æˆæžå°è´Ÿlogä¼¼ç„¶æŸå¤±ï¼ˆnegative log likelihood, NLLï¼‰ \begin{eqnarray}{NLL(\bf \theta)} &=& \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) \\ &=& - \sum_{i=1}^N log ((\frac 1{2 \pi \sigma^2})^ \frac 12 exp(- \frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))) \\ &=& \frac N2 log(2\pi \sigma ^2) + \frac 1{2 \sigma^2} \sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} æ­£åˆ™å›žå½’ç­‰ä»·äºŽè´å¶æ–¯ä¼°è®¡ å‡è®¾æ®‹å·®çš„åˆ†å¸ƒä¸º$\epsilon \sim N(0, \sigma ^2)$,çº¿æ€§å›žå½’å¯å†™æˆï¼š$p(y_i \mid \bf x_i, \theta) \sim N(y_i \mid \bf w^T \bf x_iï¼Œ\sigma ^2)$$p(y\mid \bf X, \bf w, \sigma ^2) = N(\bf y \mid \bf X \bf w, \sigma ^2 \bf I_N) \propto exp(- \frac 1{2\sigma ^2}((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w)))$ è‹¥å‡è®¾å‚æ•°ä¸ºwçš„å…ˆéªŒåˆ†å¸ƒä¸º $w_j \sim N(0, \tau ^2)$ åå‘è¾ƒå°çš„ç³»æ•°å€¼ï¼Œä»Žè€Œå¾—åˆ°çš„æ›²çº¿ä¹Ÿæ¯”è¾ƒå¹³æ»‘$p(\bf w) =\prod_{j=1}^{D} N(w_j \mid 0, \tau ^2) \propto exp(- \frac 1{2\tau^2} \sum_{j=1}^D \bf w_j^2 = exp(- \frac 1{2\tau^2} ( \bf w^T \bf w ) )) $ å…¶ä¸­$1/\tau ^2$æŽ§åˆ¶å…ˆéªŒçš„å¼ºåº¦ æ ¹æ®è´å¶æ–¯å…¬å¼ï¼Œå¾—åˆ°å‚æ•°çš„åŽéªŒåˆ†å¸ƒä¸º$p(y\mid \bf X, \bf w, \sigma ^2) = \propto exp(- \frac 1{2\sigma ^2} ((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w) ) - \frac 1{2 \tau^2} ( w^Tw ) )$ åˆ™æœ€å¤§åŽéªŒä¼°è®¡(MAP)ç­‰ä»·äºŽæœ€å°ç›®æ ‡å‡½æ•°$J(\bf w) = (\bf y - \bf X\bf w)^T(\bf y - \bf X\bf w) + \frac {\sigma ^2}{\tau^2} \bf w^T \bf w $ å¯¹æ¯”å²­å›žå½’çš„ç›®æ ‡å‡½æ•°$J(\bf w) = \sum_{i=1}^N(y_i -\bf w^T\bf x_i)^2 + \lambda \Vert \bf w\Vert ^2_2$ å°ç»“ çº¿æ€§å›žå½’æ¨¡åž‹å¯ä»¥æ”¾åˆ°æœºå™¨å­¦ä¹ ä¸€èˆ¬æ¡†æž¶ æŸå¤±å‡½æ•°ï¼šL2æŸå¤±ï¼Œâ€¦ æ­£åˆ™ï¼šæ— æ­£åˆ™ï¼Œ L2æ­£åˆ™ï¼ŒL1æ­£åˆ™â€¦ æ­£åˆ™å›žå½’æ¨¡åž‹å¯è§†ä¸ºå…ˆéªŒä¸ºæ­£åˆ™ã€ä¼¼ç„¶ä¸ºé«˜æ–¯åˆ†å¸ƒçš„è´å¶æ–¯ä¼°è®¡ L2æ­£åˆ™ï¼šå…ˆéªŒåˆ†å¸ƒä¸ºé«˜æ–¯åˆ†å¸ƒ L1æ­£åˆ™ï¼šå…ˆéªŒåˆ†å¸ƒä¸ºLaplaceåˆ†å¸ƒ 1.7 çº¿æ€§å›žå½’æ¨¡åž‹-ä¼˜åŒ–ç®—æ³• çº¿æ€§å›žå½’çš„ç›®æ ‡å‡½æ•° æ— æ­£åˆ™çš„æœ€å°äºŒä¹˜çº¿æ€§å›žå½’ï¼ˆOrdinary Least Square, OLSï¼‰ï¼šJ(w) = \sum_{i=1}^N(y_i - w^Tx_i)^2 L2æ­£åˆ™çš„å²­å›žå½’ï¼ˆRidge Regressionï¼‰æ¨¡åž‹ï¼šJ(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D w_j^2 L1æ­£åˆ™çš„Lassoæ¨¡åž‹ï¼šJ(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D |w_j| æ¨¡åž‹è®­ç»ƒï¼š æ ¹æ®è®­ç»ƒæ•°æ®æ±‚ç›®æ ‡å‡½æ•°å–æžå°å€¼çš„å‚æ•°ï¼š $\hat w = \underset {w} {arg\ min} J(\bf w)$ ç›®æ ‡å‡½æ•°çš„æœ€å°å€¼ï¼š ä¸€é˜¶çš„å¯¼æ•°ä¸º0ï¼š$\frac{\partial J(w)} {\partial w}$ äºŒé˜¶å¯¼æ•°&gt;0ï¼š$\frac{\partial J^2(w)} {\partial w^2}$ OLSçš„ä¼˜åŒ–æ±‚è§£ï¼š OLSçš„ä¼˜åŒ–æ±‚è§£ OLSçš„ç›®æ ‡å‡½æ•°å†™æˆçŸ©é˜µå½¢å¼ï¼š$J(w) = \sum ^N_{i=1}(y_i - w^Tx_i)^2 = (y - Xw)^T(y - Xw)$ åªå–ä¸Žwæœ‰å…³çš„é¡¹ï¼Œå¾—åˆ°$J(w) = w^T(X^TX)w - 2w^T(X^Ty)$ æ±‚å¯¼ $\frac{\partial J(w)} {\partial w} = 2X^TXw - 2X^Ty = 0 \Rightarrow X^TXw = X^Ty$`$\hat w_{OLS} = (X^TX)^{-1}X^Ty$ OLSçš„ä¼˜åŒ–æ±‚è§£ â€”â€”SVD OLSç›®æ ‡å‡½æ•°ï¼š$J(w) = \Vert y - Xw\Vert_2^2$ ç›¸å½“äºŽæ±‚ $y = Xw$ å¦‚æžœXä¸ºæ–¹é˜µï¼Œå¯æ±‚é€†ï¼š$w = X^{-1}y$ å¦‚æžœð—ä¸æ˜¯æ–¹é˜µï¼Œå¯æ±‚Moore-Penroseå¹¿ä¹‰é€†ï¼š$ð° = ð—^{\dagger }ð²$ã€‚ Moore-Penroseå¹¿ä¹‰é€†å¯é‡‡ç”¨å¥‡å¼‚å€¼åˆ†è§£(Singular Value Decomposition)å®žçŽ°ï¼šå¥‡å¼‚å€¼åˆ†è§£ï¼š$X = U \Sigma V^T$$X^{\dagger } = V \Sigma ^{\dagger} U^T$å…¶ä¸­ $\Sigma = \begin{pmatrix}{\sigma_1}&amp;{0}&amp;{\cdots}&amp;{0}\\{0}&amp;{\sigma_2}&amp;{\cdots}&amp;{0}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{0}&amp;{0}&amp;{\cdots}&amp;{0}\\\end{pmatrix}$,$\Sigma ^{\dagger} = \begin{pmatrix}{\frac {1}{\sigma_1}}&amp;{0}&amp;{\cdots}&amp;{0}\\{0}&amp;{\frac{1}{\sigma_2}}&amp;{\cdots}&amp;{0}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{0}&amp;{0}&amp;{\cdots}&amp;{0}\\\end{pmatrix}$ OLSçš„ä¼˜åŒ–æ±‚è§£â€”â€”æ¢¯åº¦ä¸‹é™ OLSç›®æ ‡å‡½æ•°ï¼š$J(w) = (y - Xw)^T(y - Xw)$æ¢¯åº¦ï¼š$\nabla_w = - 2X^T(y - Xw^t)$å‚æ•°æ›´æ–°ï¼š$w^{t+1} = w^t - \eta\nabla_w = w^t + 2\eta X^T(y - Xw^t)$ å²­å›žå½’çš„ä¼˜åŒ–æ±‚è§£ å²­å›žå½’çš„ç›®æ ‡å‡½æ•°ä¸ŽOLSåªç›¸å·®ä¸€ä¸ªæ­£åˆ™é¡¹ï¼ˆä¹Ÿæ˜¯wçš„äºŒæ¬¡å‡½æ•°ï¼‰ å²­å›žå½’çš„ä¼˜åŒ–æ±‚è§£â€”â€”SVD Lassoçš„ä¼˜åŒ–æ¡ä»¶ è½¯&amp; ç¡¬é˜ˆå€¼ Lassoçš„ä¼˜åŒ–æ±‚è§£â€”â€”åæ ‡è½´ä¸‹é™æ³• ä¸ºäº†æ‰¾åˆ°ä¸€ä¸ªå‡½æ•°çš„å±€éƒ¨æžå°å€¼ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­å¯ä»¥åœ¨å½“å‰ç‚¹å¤„æ²¿ä¸€ä¸ªåæ ‡æ–¹å‘è¿›è¡Œä¸€ç»´æœç´¢ã€‚ æ•´ä¸ªè¿‡ç¨‹ä¸­å¾ªçŽ¯ä½¿ç”¨ä¸åŒçš„åæ ‡æ–¹å‘ã€‚ä¸€ä¸ªå‘¨æœŸçš„ä¸€ç»´æœç´¢è¿­ä»£è¿‡ç¨‹ç›¸å½“äºŽä¸€ä¸ªæ¢¯åº¦è¿­ä»£ã€‚ æ³¨æ„ï¼š æ¢¯åº¦ä¸‹é™æ–¹æ³•æ˜¯åˆ©ç”¨ç›®æ ‡å‡½æ•°çš„å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰æ¥ç¡®å®šæœç´¢æ–¹å‘çš„ï¼Œè€Œè¯¥æ¢¯åº¦æ–¹å‘å¯èƒ½ä¸ä¸Žä»»ä½•åæ ‡è½´å¹³è¡Œã€‚ è€Œåæ ‡è½´ä¸‹é™æ³•æ˜¯åˆ©ç”¨å½“å‰åæ ‡ç³»ç»Ÿè¿›è¡Œæœç´¢ï¼Œä¸éœ€è¦æ±‚ç›®æ ‡å‡½æ•°çš„å¯¼æ•°ï¼ŒåªæŒ‰ç…§æŸä¸€åæ ‡æ–¹å‘è¿›è¡Œæœç´¢æœ€å°å€¼ã€‚ï¼ˆåœ¨ç¨€ç–çŸ©é˜µä¸Šçš„è®¡ç®—é€Ÿåº¦éžå¸¸å¿«ï¼ŒåŒæ—¶ä¹Ÿæ˜¯Lassoå›žå½’æœ€å¿«çš„è§£æ³•ï¼‰ å°ç»“ çº¿æ€§å›žå½’æ¨¡åž‹æ¯”è¾ƒç®€å• å½“æ•°æ®è§„æ¨¡æ¯”è¾ƒå°æ—¶ï¼Œå¯ç›´æŽ¥è§£æžæ±‚è§£ scikit learnä¸­çš„å®žçŽ°é‡‡ç”¨SVDåˆ†è§£å®žçŽ° å½“æ•°æ®è§„æ¨¡è¾ƒå¤§æ—¶ï¼Œå¯é‡‡ç”¨éšæœºæ¢¯åº¦ä¸‹é™ scikit learnæä¾›ä¸€ä¸ªSGDRegressionç±» å²­å›žå½’æ±‚è§£ç±»ä¼¼OLSï¼Œé‡‡ç”¨SVDåˆ†è§£å®žçŽ° Lassoä¼˜åŒ–æ±‚è§£é‡‡ç”¨åæ ‡è½´ä¸‹é™æ³• 1.8 çº¿æ€§å›žå½’æ¨¡åž‹-æ¨¡åž‹é€‰æ‹© æ¨¡åž‹è¯„ä¼°ä¸Žæ¨¡åž‹é€‰æ‹© æ¨¡åž‹è®­ç»ƒå¥½åŽï¼Œéœ€è¦åœ¨æ ¡éªŒé›†ä¸Šé‡‡ç”¨ä¸€äº›åº¦é‡å‡†åˆ™æ£€æŸ¥æ¨¡åž‹é¢„æµ‹çš„æ•ˆæžœ æ ¡éªŒé›†åˆ’åˆ†ï¼ˆtrain_test_splitã€äº¤å‰éªŒè¯ï¼‰ è¯„ä»·æŒ‡æ ‡ï¼ˆsklearn.metricsï¼‰ æ¨¡åž‹é€‰æ‹©ï¼š æ¨¡åž‹ä¸­é€šå¸¸æœ‰ä¸€äº›è¶…å‚æ•°ï¼Œéœ€è¦é€šè¿‡æ¨¡åž‹é€‰æ‹©æ¥ç¡®å®š çº¿æ€§å›žå½’æ¨¡åž‹ä¸­çš„æ­£åˆ™å‚æ•° OLSä¸­çš„ç‰¹å¾çš„æ•°ç›® å‚æ•°æœç´¢èŒƒå›´ï¼šç½‘æ ¼æœç´¢ï¼ˆGridSearchï¼‰ Scikit learnå°†äº¤å‰éªŒè¯ä¸Žç½‘æ ¼æœç´¢åˆå¹¶ä¸ºä¸€ä¸ªå‡½æ•° è¯„ä»·å‡†åˆ™ æ¨¡åž‹è®­ç»ƒå¥½åŽï¼Œå¯ç”¨ä¸€äº›åº¦é‡å‡†åˆ™æ£€æŸ¥æ¨¡åž‹æ‹Ÿåˆçš„æ•ˆæžœ å¼€æ–¹å‡æ–¹è¯¯å·®ï¼ˆrooted mean squared errorï¼ŒRMSEï¼‰:$RMSE = \sqrt{\frac 1N \sum_{i=1}^N(\hat y_i - y_i)^2}$` å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆmean absolute errorï¼ŒMAEï¼‰ï¼š$MAE = \frac 1N \sum_{i=1}^N|\hat y_i - y_i|$ R2 scoreï¼šæ—¢è€ƒè™‘äº†é¢„æµ‹å€¼ä¸ŽçœŸå€¼ä¹‹é—´çš„å·®å¼‚ï¼Œä¹Ÿè€ƒè™‘äº†é—®é¢˜æœ¬èº«çœŸå€¼ä¹‹é—´çš„å·®å¼‚ï¼ˆ scikit learn çº¿æ€§å›žå½’æ¨¡åž‹çš„ç¼ºçœè¯„ä»·å‡†åˆ™ï¼‰$SS_{res} = \sum_{i=1}^N(\hat y_i - y_i)^2, SStot = \sum_{i=1}^N(y_i - \bar{y})^2, R^2 = 1 - \frac {SS_{res}}{SS_{tot}})$ ä¹Ÿå¯ä»¥æ£€æŸ¥æ®‹å·®çš„åˆ†å¸ƒ è¿˜å¯ä»¥æ‰“å°é¢„æµ‹å€¼ä¸ŽçœŸå€¼çš„æ•£ç‚¹å›¾ çº¿æ€§å›žå½’ä¸­çš„æ¨¡åž‹é€‰æ‹©Scikit learnä¸­çš„model selectionæ¨¡å—æä¾›æ¨¡åž‹é€‰æ‹©åŠŸèƒ½ å¯¹äºŽçº¿æ€§æ¨¡åž‹ï¼Œç•™ä¸€äº¤å‰éªŒè¯ï¼ˆNæŠ˜äº¤å‰éªŒè¯ï¼Œäº¦ç§°ä¸ºleave-oneout cross-validationï¼ŒLOOCVï¼‰æœ‰æ›´ç®€ä¾¿çš„è®¡ç®—æ–¹å¼ï¼Œå› æ­¤Scikit learnæä¾›äº†RidgeCVç±»å’ŒLassoCVç±»å®žçŽ°äº†è¿™ç§æ–¹å¼ åŽç»­è¯¾ç¨‹å°†è®²è¿°ä¸€èˆ¬æ¨¡åž‹çš„äº¤å‰éªŒè¯å’Œå‚æ•°è°ƒä¼˜GridSearchCV RidgeCV RidgeCVä¸­è¶…å‚æ•°Î»ç”¨alphaè¡¨ç¤º RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False) LassoCV LassoCVçš„ä½¿ç”¨ä¸ŽRidgeCVç±»ä¼¼ Scikit learnè¿˜æä¾›ä¸€ä¸ªä¸ŽLassoç±»ä¼¼çš„LARSï¼ˆleast angle regressionï¼Œæœ€å°è§’å›žå½’ï¼‰ï¼ŒäºŒè€…ä»…ä»…æ˜¯ä¼˜åŒ–æ–¹æ³•ä¸åŒï¼Œç›®æ ‡å‡½æ•°ç›¸åŒã€‚ å½“æ•°æ®é›†ä¸­ç‰¹å¾ç»´æ•°å¾ˆå¤šä¸”å­˜åœ¨å…±çº¿æ€§æ—¶ï¼ŒLassoCVæ›´åˆé€‚ã€‚ å°ç»“ï¼šçº¿æ€§å›žå½’ä¹‹æ¨¡åž‹é€‰æ‹© é‡‡ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡åž‹é¢„æµ‹æ€§èƒ½ï¼Œä»Žè€Œé€‰æ‹©æœ€ä½³æ¨¡åž‹ å›žå½’æ€§èƒ½çš„è¯„ä»·æŒ‡æ ‡ çº¿æ€§æ¨¡åž‹çš„äº¤å‰éªŒè¯é€šå¸¸ç›´æŽ¥é‡‡ç”¨å¹¿ä¹‰çº¿æ€§æ¨¡åž‹çš„ç•™ä¸€äº¤å‰éªŒè¯è¿›è¡Œå¿«é€Ÿæ¨¡åž‹è¯„ä¼° Scikit learnä¸­å¯¹RidgeCVå’ŒLassoCVå®žçŽ°è¯¥åŠŸèƒ½ 1.9 æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹æ¡ˆä¾‹è¯¦è§£â€”â€”æ•°æ®æŽ¢ç´¢ ç¬¬ä¸€æ­¥ï¼šç†è§£ä»»åŠ¡ï¼Œå‡†å¤‡æ•°æ® æ•°æ®è¯»å– Pandasæ”¯æŒå¤šç§æ ¼å¼çš„æ•°æ® æ•°æ®æŽ¢ç´¢&amp;ç‰¹å¾å·¥ç¨‹ æ•°æ®è§„æ¨¡ ç¡®å®šæ•°æ®ç±»åž‹ï¼Œæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥ç¼–ç  ç‰¹å¾ç¼–ç  æ•°æ®æ˜¯å¦æœ‰ç¼ºå¤±å€¼ æ•°æ®å¡«è¡¥ æŸ¥çœ‹æ•°æ®åˆ†å¸ƒï¼Œæ˜¯å¦æœ‰å¼‚å¸¸æ•°æ®ç‚¹ ç¦»ç¾¤ç‚¹å¤„ç† æŸ¥çœ‹ä¸¤ä¸¤ç‰¹å¾ä¹‹é—´çš„å…³ç³»ï¼Œçœ‹æ•°æ®æ˜¯å¦æœ‰å†—ä½™/ç›¸å…³ é™ç»´ æ•°æ®æ¦‚è§ˆ pandas:DataFrame Head():æ•°æ®å‰5è¡Œï¼Œå¯æŸ¥çœ‹æ¯ä¸€åˆ—çš„åå­—åŠæ•°æ®ç±»åž‹ Info(): æ•°æ®è§„æ¨¡ï¼šè¡Œæ•°&amp;åˆ—æ•° æ¯åˆ—çš„æ•°æ®ç±»åž‹ã€æ˜¯å¦æœ‰ç©ºå€¼ å ç”¨å­˜å‚¨é‡ shape:è¡Œæ•°&amp;åˆ—æ•° å„å±žæ€§çš„ç»Ÿè®¡ç‰¹æ€§ ç›´æ–¹å›¾ æ¯ä¸ªå–å€¼åœ¨æ•°æ®é›†ä¸­å‡ºçŽ°çš„æ ·æœ¬æ•°ç›® ç¦»ç¾¤ç‚¹ ç¦»ç¾¤ç‚¹ï¼šå¥‡å¼‚ç‚¹ï¼ˆoutlierï¼‰,æŒ‡è¿œç¦»å¤§å¤šæ•°æ ·æœ¬çš„æ ·æœ¬ç‚¹ã€‚é€šå¸¸è®¤ä¸ºè¿™äº›ç‚¹æ˜¯å™ªå£°ï¼Œå¯¹æ¨¡åž‹æœ‰åå½±å“ ç›¸å…³æ€§ ç›¸å…³æ€§ï¼šç›¸å…³æ€§å¯ä»¥é€šè¿‡è®¡ç®—ç›¸å…³ç³»æ•°æˆ–æ‰“å°æ•£ç‚¹å›¾æ¥å‘çŽ° ç›¸å…³ç³»æ•°ï¼š æ•£ç‚¹å›¾ å¯ä»¥é€šè¿‡ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„æ•£ç‚¹å›¾ç›´è§‚æ„Ÿå—äºŒè€…çš„ç›¸å…³æ€§ æ•°æ®é¢„å¤„ç† æ•°æ®æ ‡å‡†åŒ–ï¼ˆ Standardization ï¼‰ æŸä¸ªç‰¹å¾çš„æ‰€æœ‰æ ·æœ¬å–å€¼ä¸º0å‡å€¼ã€1æ–¹å·® æ•°æ®å½’ä¸€åŒ–ï¼ˆ Scaling ï¼‰ æŸä¸ªç‰¹å¾çš„æ‰€æœ‰æ ·æœ¬å–å€¼åœ¨è§„å®šèŒƒå›´å†… æ•°æ®æ­£è§„åŒ–ï¼ˆ Normalization ï¼‰ æ¯ä¸ªæ ·æœ¬æ¨¡é•¿ä¸º1 æ•°æ®äºŒå€¼åŒ– æ ¹æ®ç‰¹å¾å€¼å–å€¼æ˜¯å¦å¤§äºŽé˜ˆå€¼å°†ç‰¹å¾å€¼å˜ä¸º0æˆ–1ï¼Œå¯ç”¨ç±»Binarizer å®žçŽ° æ•°æ®ç¼ºå¤± æ•°æ®ç±»åž‹å˜æ¢ æœ‰äº›æ¨¡åž‹åªèƒ½å¤„ç†æ•°å€¼åž‹æ•°æ®ã€‚å¦‚æžœç»™å®šçš„æ•°æ®æ˜¯ä¸åŒçš„ç±»åž‹ï¼Œå¿…é¡»å…ˆå°†æ•°æ®å˜æˆæ•°å€¼åž‹ã€‚ ç¬¬äºŒæ­¥ï¼šæ¨¡åž‹ç¡®å®šå’Œæ¨¡åž‹è®­ç»ƒ 1ã€ç¡®å®šæ¨¡åž‹ç±»åž‹ ç›®æ ‡å‡½æ•°ï¼ˆæŸå¤±å‡½æ•°ã€æ­£åˆ™ï¼‰ 2ã€æ¨¡åž‹è®­ç»ƒ ä¼˜åŒ–ç®—æ³•ï¼ˆè§£æžæ³•ï¼Œæ¢¯åº¦ä¸‹é™ã€éšæœºæ¢¯åº¦ä¸‹é™â€¦ï¼‰ ç¬¬ä¸‰æ­¥ï¼šæ¨¡åž‹è¯„ä¼°ä¸Žæ¨¡åž‹é€‰æ‹© æ¨¡åž‹è®­ç»ƒå¥½åŽï¼Œéœ€è¦åœ¨æ ¡éªŒé›†ä¸Šé‡‡ç”¨ä¸€äº›åº¦é‡å‡†åˆ™æ£€æŸ¥æ¨¡åž‹é¢„æµ‹çš„æ•ˆæžœ æ ¡éªŒé›†åˆ’åˆ†ï¼ˆtrain_test_splitã€äº¤å‰éªŒè¯ï¼‰ è¯„ä»·æŒ‡æ ‡ ï¼ˆsklearn.meticsï¼‰ ä¹Ÿå¯ä»¥æ£€æŸ¥æ®‹å·®çš„åˆ†å¸ƒ è¿˜å¯ä»¥æ‰“å°é¢„æµ‹å€¼ä¸ŽçœŸå€¼çš„æ•£ç‚¹å›¾ æ¨¡åž‹é€‰æ‹©ï¼šé€‰æ‹©é¢„æµ‹æ€§èƒ½æœ€å¥½çš„æ¨¡åž‹ æ¨¡åž‹ä¸­é€šå¸¸æœ‰ä¸€äº›è¶…å‚æ•°ï¼Œéœ€è¦é€šè¿‡æ¨¡åž‹é€‰æ‹©æ¥ç¡®å®š å‚æ•°æœç´¢èŒƒå›´ï¼šç½‘æ ¼æœç´¢ï¼ˆGridSearchï¼‰ 1.10 æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹-æ•°æ®æŽ¢ç´¢ä»£ç python 3.712345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns# è¯»å…¥æ•°æ®data = pd.read_csv("boston_housing.csv")# æ•°æ®æŽ¢ç´¢print(data.head())data.info()print(data.isnull().sum())print(data.describe())# ç›®æ ‡y(æˆ¿å±‹ä»·æ ¼)çš„ç›´æ–¹å›¾/åˆ†å¸ƒfig = plt.figure()sns.distplot(data.MEDV.values, bins=30, kde=True)plt.xlabel('Median value of owner_occupied homes', fontsize=12)plt.show()# å•ä¸ªç‰¹å¾æ•£ç‚¹å›¾plt.scatter(range(data.shape[0]), data["MEDV"].values, color='purple')plt.title("Distribution of Price")plt.show()# åˆ é™¤yå¤§äºŽ50çš„æ ·æœ¬data = data[data.MEDV &lt; 50]print(data.shape)# è¾“å…¥å±žæ€§çš„ç›´æ–¹å›¾ï¼åˆ†å¸ƒ# çŠ¯ç½ªçŽ‡ç‰¹å¾fig = plt.figure()sns.distplot(data.CRIM.values, bins=30, kde=False)plt.xlabel('crime rate', fontsize=12)plt.show()# æ˜¯å¦é è¿‘charles riversns.countplot(data.CHAS, order=[0, 1]);plt.xlabel('Charles River');plt.ylabel('Number of occurrences');plt.show()# é è¿‘é«˜é€Ÿsns.countplot(data.RAD)plt.xlabel('index of accessibility to radial highways')plt.show()# ä¸¤ä¸¤ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§# èŽ·å¾—æ‰€æœ‰åˆ—çš„åå­—cols = data.columns# è®¡ç®—ç›¸å…³æ€§data_corr = data.corr().abs()# ç›¸å…³æ€§çƒ­å›¾plt.subplots(figsize=(13, 9))sns.heatmap(data_corr, annot=True)sns.heatmap(data_corr, mask=data_corr &lt; 1, cbar=False)plt.savefig('house_coor.png')plt.show()# è¾“å‡ºå¼ºç›¸å…³å¯¹threshold = 0.5corr_list = []size = data_corr.shape[0]for i in range(0, size): for j in range(i+1, size): if (data_corr.iloc[i,j] &gt;= threshold and data_corr.iloc[i, j] &lt; 1) or (data_corr.iloc[i, j] &lt; 0 and data_corr.iloc &lt;= -threshold): corr_list.append([data_corr.iloc[i, j], i, j])s_corr_list = sorted(corr_list, key=lambda x: -abs(x[0]))for v, i, j in s_corr_list: print("%s and %s = %.2f" % (cols[i], cols[j], v))for v, i, j in s_corr_list: sns.pairplot(data, height=6, x_vars=cols[i], y_vars=cols[j]) plt.show() 1.11 æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹æ¡ˆä¾‹è¯¦è§£1.12 æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹æ¡ˆä¾‹è¯¦è§£-ä»£ç è®²è§£python 3.7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174# æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹æ¡ˆä¾‹â€”â€”çº¿æ€§å›žå½’åˆ†æžimport numpy as np # çŸ©é˜µæ“ä½œimport pandas as pd # SQLæ•°æ®å¤„ç†from sklearn.metrics import r2_score # è¯„ä»·å›žå½’é¢„æµ‹æ¨¡åž‹çš„æ€§èƒ½import matplotlib.pyplot as plt # ç”»å›¾import seaborn as sns# è¯»å…¥æ•°æ®data = pd.read_csv("boston_housing.csv")# 1ã€æ•°æ®å‡†å¤‡# ä»ŽåŽŸå§‹æ•°æ®ä¸­åˆ†ç¦»è¾“å…¥ç‰¹å¾xå’Œè¾“å‡ºyy = data['MEDV'].valuesX = data.drop('MEDV', axis=1)# ç”¨äºŽåŽç»­æ˜¾ç¤ºæƒé‡ç³»æ•°å¯¹åº”çš„ç‰¹å¾columns = X.columns# æ•°æ®è¾ƒå°‘ï¼Œå°†æ•°æ®åˆ†å‰²è®­ç»ƒæ•°æ®from sklearn.model_selection import train_test_split# éšæœºé‡‡æ ·20%çš„æ•°æ®æž„å»ºæµ‹è¯•æ ·æœ¬ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒæ ·æœ¬X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33,test_size=0.2)# print(X_train.shape)# 2ã€æ•°æ®é¢„å¤„ç†/ç‰¹å¾å·¥ç¨‹# æ•°æ®æ ‡å‡†åŒ–from sklearn.preprocessing import StandardScaler# åˆ†åˆ«åˆå§‹åŒ–å¯¹ç‰¹å¾å’Œç›®æ ‡å€¼çš„æ ‡å‡†åŒ–å™¨ss_X = StandardScaler()ss_y = StandardScaler()# åˆ†åˆ«å¯¹è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„ç‰¹å¾ä»¥åŠç›®æ ‡å€¼è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†X_train = ss_X.fit_transform(X_train)X_test = ss_X.transform(X_test)# å¯¹yæ ‡å‡†åŒ–ä¸æ˜¯å¿…é¡»# å¯¹yæ ‡å‡†åŒ–çš„å¥½å¤„æ˜¯ä¸åŒçš„é—®é¢˜çš„wå·®å¼‚ä¸å¤ªå¤§ï¼ŒåŒæ—¶æ­£åˆ™å‚æ•°çš„èŒƒå›´ä¹Ÿæœ‰é™y_train = ss_y.fit_transform(y_train.reshape(-1, 1))y_test = ss_y.transform(y_test.reshape(-1, 1))# 3ã€ç¡®å®šæ¨¡åž‹ç±»åž‹# 3.1 å°è¯•ç¼ºçœå‚æ•°çš„çº¿æ€§å›žå½’# çº¿æ€§å›žå½’from sklearn.linear_model import LinearRegression# ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–lr = LinearRegression()# è®­ç»ƒæ¨¡åž‹å‚æ•°lr.fit(X_train, y_train)# é¢„æµ‹y_test_pred_lr = lr.predict(X_test)y_train_pred_lr = lr.predict(X_train)# çœ‹çœ‹å„ç‰¹å¾çš„æƒé‡ç³»æ•°ï¼Œç³»æ•°çš„ç»å¯¹å€¼å¤§å°å¯è§†ä¸ºè¯¥ç‰¹å¾çš„é‡è¦æ€§fs = pd.DataFrame(&#123;"columns": list(columns), "coef": list((lr.coef_.T))&#125;)fs.sort_values(by=['coef'], ascending=False)print(fs)# æ¨¡åž‹è¯„ä»·# æµ‹è¯•é›†print('The r2 score of LinearRegression on test is', r2_score(y_test, y_test_pred_lr))# è®­ç»ƒé›†print('The r2 score of LinearRegression on train is', r2_score(y_train, y_train_pred_lr))# åœ¨è®­ç»ƒé›†ä¸Šè§‚å¯Ÿæ®‹å·®çš„åˆ†å¸ƒï¼Œçœ‹æ˜¯å¦ç¬¦åˆæ¨¡åž‹å‡è®¾ï¼šå™ªå£°ä¸º0å‡å€¼çš„é«˜æ–¯å™ªå£°f, ax = plt.subplots(figsize=(7, 5))f.tight_layout()ax.hist(y_train - y_train_pred_lr, bins=40, label='Residuals Linear', color='b', alpha=.5)ax.set_title("Histogram of Residuals")ax.legend(loc='best')plt.show()# è¿˜å¯ä»¥è§‚å¯Ÿé¢„æµ‹å€¼ä¸ŽçœŸå€¼çš„æ•£ç‚¹å›¾plt.figure(figsize=(4, 3))plt.scatter(y_train, y_train_pred_lr)plt.plot([-3, 3],[-3, 3], '--k')plt.axis('tight')plt.xlabel('True price')plt.ylabel('Predicted price')plt.tight_layout()plt.show()# çº¿æ€§æ¨¡åž‹ï¼Œéšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ¨¡åž‹å‚æ•°# éšæœºæ¢¯åº¦ä¸‹é™ä¸€èˆ¬åœ¨å¤§æ•°æ®é›†ä¸Šåº”ç”¨ï¼Œå…¶å®žæœ¬é¡¹ç›®ä¸é€‚åˆç”¨from sklearn.linear_model import SGDRegressor# ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–çº¿sgdr = SGDRegressor(max_iter=1000)# è®­ç»ƒï¼šå‚æ•°ä¼°è®¡sgdr.fit(X_train, y_train)# é¢„æµ‹sgdr.coef_print('The value of default measurement of SGDRegressor on test is', sgdr.score(X_test, y_test))print('The value of default measurement of SGDRegressor on train is', sgdr.score(X_train, y_train))# 3.2 æ­£åˆ™åŒ–çš„çº¿æ€§å›žå½’ï¼ˆL2æ­£åˆ™--&gt;å²­å›žå½’ï¼‰from sklearn.linear_model import RidgeCV# è®¾ç½®è¶…å‚æ•°ï¼ˆæ­£åˆ™å‚æ•°ï¼‰èŒƒå›´alphas = [0.01, 0.1, 1, 10, 100]# ç”Ÿæˆä¸€ä¸ªRidgeCVridge = RidgeCV(alphas=alphas, store_cv_values=True)# æ¨¡åž‹è®­ç»ƒridge.fit(X_train, y_train)# é¢„æµ‹y_test_pred_ridge = ridge.predict(X_test)y_train_pred_ridge = ridge.predict(X_train)# è¯„ä¼°ï¼Œä½¿ç”¨r2_scoreè¯„ä»·æ¨¡åž‹åœ¨æµ‹è¯•é›†å’Œè®­ç»ƒé›†ä¸Šçš„æ€§èƒ½print('The r2 score of RidgeCV on test is', r2_score(y_test, y_test_pred_ridge))print('The r2 score of RidgeCV on test is', r2_score(y_train, y_train_pred_ridge))# å¯è§†åŒ–mse_mean = np.mean(ridge.cv_values_, axis=0)plt.plot(np.log10(alphas), mse_mean.reshape(len(alphas), 1))# plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])plt.xlabel('log(alpha)')plt.ylabel('mse')plt.show()print('alpha is:', ridge.alpha_)# çœ‹çœ‹å„ç‰¹å¾çš„æƒé‡ç³»æ•°ï¼Œç³»æ•°çš„ç»å¯¹å€¼å¤§å°å¯è§†ä¸ºè¯¥ç‰¹åˆ¶çš„é‡è¦æ€§fs = pd.DataFrame(&#123;"columns": list(columns), "coef_lr": list(lr.coef_.T), "coef_ridge": list(ridge.coef_.T)&#125;)fs.sort_values(by=['coef_lr'], ascending=False)print(fs)# 3.3 æ­£åˆ™åŒ–çš„çº¿æ€§å›žå½’ï¼ˆL1æ­£åˆ™--&gt;Lassoï¼‰from sklearn.linear_model import LassoCV# ç”Ÿæˆä¸€ä¸ªLassoCVå®žä¾‹lasso = LassoCV()# è®­ç»ƒï¼ˆå†…å«CVï¼‰lasso.fit(X_train, y_train)# æµ‹è¯•y_test_pred_lasso = lasso.predict(X_test)y_train_pred_lasso = lasso.predict(X_train)# è¯„ä¼°ï¼Œ ä½¿ç”¨r2_scoreè¯„ä»·æ¨¡åž‹åœ¨æµ‹è¯•é›†å’Œè®­ç»ƒé›†ä¸Šçš„æ€§èƒ½print('The r2 score of LassoCV on test is', r2_score(y_test, y_test_pred_lasso))print('The r2 score of LassoCV on train is', r2_score(y_train, y_train_pred_lasso))# å¯è§†åŒ–mses = np.mean(lasso.mse_path_, axis=1)plt.plot(np.log10(lasso.alphas_), mses)# plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])plt.xlabel('log(alpha)')plt.ylabel('mse')plt.show()print('alpha is:', lasso.alpha_)# çœ‹çœ‹å„ç‰¹å¾çš„æƒé‡ç³»æ•°ï¼Œç³»æ•°çš„ç»å¯¹å€¼å¤§å°å¯è§†ä¸ºè¯¥ç‰¹åˆ¶çš„é‡è¦æ€§fs = pd.DataFrame(&#123;"columns": list(columns), "coef_lr": list(lr.coef_.T), "coef_ridge": list(lasso.coef_.T)&#125;)fs.sort_values(by=['coef_lr'], ascending=False)print(fs)]]></content>
      <categories>
        <category>å­¦ä¹ ç¬”è®°</category>
      </categories>
      <tags>
        <tag>äººå·¥æ™ºèƒ½</tag>
        <tag>å­¦ä¹ ç¬”è®°</tag>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>çº¿æ€§å›žå½’</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è®¡ç®—æœºè§†è§‰åŸºç¡€å…¥é—¨ å­¦ä¹ ç¬”è®°]]></title>
    <url>%2F2019%2F03%2F30%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[ä¸€ã€ è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æ¦‚è¿° è®¡ç®—æœºè§†è§‰å›žé¡¾ è®¡ç®—æœºè§†è§‰ï¼ˆcomputer visionï¼‰å®šä¹‰ æ•°æ®ï¼ˆé™æ€å›¾ç‰‡ï¼Œè§†é¢‘ï¼‰ ç®—æ³•ï¼ˆæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œç¥žç»ç½‘ç»œï¼‰æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå›žå½’+åˆ†ç±» è®¡ç®—æœºè§†è§‰çš„é‡è¦æ€§ ä¸‰å¤§ä»»åŠ¡ï¼šå›¾åƒè¯†åˆ«ï¼ˆimage classificationï¼‰è½¦ç‰Œè¯†åˆ«ï¼Œäººè„¸è¯†åˆ« ä¸‰å¤§ä»»åŠ¡ï¼šç›®æ ‡æ£€æµ‹ï¼ˆobject detection = classification + localizationï¼‰è¡Œäººæ£€æµ‹å’Œè½¦è¾†æ£€æµ‹ ä¸‰å¤§ä»»åŠ¡ï¼šå›¾åƒåˆ†å‰²å›¾åƒè¯­ä¹‰åˆ†å‰²ä¸ªä½“åˆ†å‰² = æ£€æµ‹ + åˆ†å‰² è§†è§‰ç›®æ ‡è·Ÿè¸ªï¼ˆtrackingï¼‰ è§†é¢‘åˆ†å‰² å›¾åƒé£Žæ ¼è¿ç§» ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ è§†é¢‘ç”Ÿæˆ æ·±åº¦å­¦ä¹ ä»‹ç» 2006 Hinton bp(åå‘ä¼ æ’­) 2012 Krizhevsky A æ·±åº¦å­¦ä¹  æ·±åº¦å·ç§¯ RNN LSTM æŒç»­ä¿¡æ¯ è§†è§‰è¯†åˆ«ï¼Œè¯­éŸ³è¯†åˆ«ï¼ŒDeepMind, AlphaGo äººè„¸è¯†åˆ«ï¼šLFW é”™è¯¯çŽ‡5% -&gt; 0.5% å›¾åƒåˆ†å‰² VGGNet, GoogleNet, ResNet, DenseNet å¸¸è§çš„æ·±åº¦å­¦ä¹ å¼€å‘å¹³å° Torch, TensorFlow, MatConvNetTheano, Caffe è¯¾ç¨‹ä»‹ç» å›¾åƒè¯†åˆ«ï¼šAlexnet, VGGnet, GoogleNet, ResNet, DenseNet ç›®æ ‡æ£€æµ‹Fast-rcnn, faster-rcnn, Yolo, Retina-Net å›¾åƒåˆ†å‰²FCN, Mask-Rcnn ç›®æ ‡è·Ÿè¸ªGORURNï¼Œ ECO å›¾åƒç”ŸæˆGANï¼Œ WGAN å…‰æµFlowNet è§†é¢‘åˆ†å‰²Segnet äºŒã€ å›¾åƒåˆ†ç±»ä¸Žæ·±åº¦å·ç§¯ç½‘ç»œçš„æ¨¡åž‹ å›¾åƒåˆ†ç±» å›¾åƒåˆ†ç±»çš„æŒ‘æˆ˜å…‰ç…§å˜åŒ–å½¢å˜ç±»å†…å˜åŒ– å›¾åƒåˆ†ç±»å®šä¹‰ ç›®æ ‡åˆ†ç±»æ¡†æž¶ æ³›åŒ–èƒ½åŠ›å¦‚ä½•æé«˜æ³›åŒ–èƒ½åŠ›ï¼Ÿ éœ€è¦ç”¨å›¾åƒç‰¹å¾æ¥æè¿°å›¾åƒ è®­ç»ƒå’Œæµ‹è¯•çš„æµç¨‹ å›¾åƒç‰¹å¾ color: Qutantize RGB values global shape: PCA space local shape: shape context texture: Filter banks SIFT, Hog, LBP, Harr æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ è¶…å¹³é¢ä¸Žæ”¯æŒå‘é‡ æœ€å¤§åŒ–é—´éš” svmåˆ†ç±»ï¼ˆpythonï¼‰ä»¥lriså…°èŠ±åˆ†ç±»ä¸ºä¾‹ ç¨‹åºå®žçŽ° æ›´å¥½çš„ç‰¹å¾ CNNç‰¹å¾ å­¦ä¹ å‡ºæ¥çš„ å¦‚ä½•å­¦ä¹ ï¼Ÿ æž„é€ ç¥žç»ç½‘ç»œ ç¥žç»ç½‘ç»œåŽŸç† ç¥žç»ç½‘ç»œåšå›¾åƒåˆ†ç±» ç¥žç»ç½‘ç»œæ­å»º ç¥žç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒï¼šç¥žç»å…ƒ æ¿€åŠ±å‡½æ•° Sigmoidã€tanhã€ReLUã€Leaky ReLUã€Maxoutã€ELU å·ç§¯å±‚ å·ç§¯æ»¤æ³¢çš„è®¡ç®— å·ç§¯å±‚å¯è§†åŒ– æ± åŒ–å±‚ï¼ˆpooling layerï¼‰ ç‰¹å¾è¡¨è¾¾æ›´åŠ ç´§å‡‘ï¼ŒåŒæ—¶å…·æœ‰ä½ç§»ä¸å˜æ€§ å…¨è¿žæŽ¥å±‚ æŸå¤±å‡½æ•° äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆSIGMOID_CROSS_ENTROPY_LOSS) åº”ç”¨äºŽäºŒåˆ†ç±»é—®é¢˜ Softmax æŸå¤±å‡½æ•°ï¼ˆSOFTMAX_LOSS) å¤šåˆ†ç±»é—®é¢˜ æ¬§å¼è·ç¦»æŸå¤±å‡½æ•°ï¼ˆEUCLIDEAN_LOSSï¼‰å›žå½’é—®é¢˜ å¯¹æ¯”æŸå¤±å‡½æ•°ï¼ˆContrastive lossï¼‰ç”¨æ¥è®¡ç®—ä¸¤ä¸ªå›¾åƒä¹‹é—´çš„ç›¸ä¼¼åº¦ Triplet loss è®­ç»ƒç½‘ç»œ ç½‘ç»œè®­ç»ƒå’Œæµ‹è¯• å·ç§¯ç¥žç»ç½‘ç»œä»‹ç» Alexnet, VGGnet, GoogleNet, ResNet, DenseNet è®­ç»ƒæŠ€å·§ï¼Œ é˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆæ³›åŒ–èƒ½åŠ›ä¸å¼ºï¼‰ æ•°æ®å¢žå¼ºï¼ˆData augmentationï¼‰ æ°´å¹³ç¿»è½¬ï¼Œ éšæœºè£å‰ªå’Œå¹³ç§»å˜æ¢ï¼Œé¢œè‰²ã€å…‰ç…§å˜æ¢ Dropout å…¶ä»–æœ‰åŠ©äºŽè®­ç»ƒçš„æ‰‹æ®µ L1ï¼Œ L2æ­£åˆ™åŒ– Batch Normalization åˆ©ç”¨caffeæ­å»ºæ·±åº¦ç½‘ç»œåšå›¾åƒåˆ†ç±»]]></content>
      <categories>
        <category>å­¦ä¹ ç¬”è®°</category>
      </categories>
      <tags>
        <tag>å­¦ä¹ ç¬”è®°</tag>
        <tag>è®¡ç®—æœºè§†è§‰</tag>
        <tag>åŸºç¡€</tag>
      </tags>
  </entry>
</search>
