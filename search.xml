<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo + GitHub Pages + Next在windows下搭建个人博客]]></title>
    <url>%2F2019%2F04%2F01%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[才搭好博客，发现在博客发布文章确实比微信公众号方便很多，这里简略说下用 Hexo + GitHub Pages + Next搭建个人博客的课程，大部分经验都是来自于网络，我会在整个过程后面附上参考的文章，一来总结搭建博客的过程，二来减少后来人踩坑。 整个过程： 1、注册Github账号及创建仓库 2、安装Git for Windows 3、配置Git 4、安装node.js 5、安装Hexo 6、使用next设计个性化博客 7、连接Hexo和Github Pages及部署博客 8、购买域名并解析以上就是全部的过程，当然具体还有很多细节，比如更换配置、设置文章字数的单位，阅读时常的单位，设置评论区，具体的东西还是要依据个人的喜好调整，但是next主题里面基本都集成了这些功能，只要稍微调整下就行。 参考的文章： 参考的整个过程 各种个性化小功能 给统计量添加单位 各种个性化设置 文章发布 GitHub/Coding双线部署 小书匠Markdown使用手册 在Markdown中输入数学公式(MathJax) Hexo 的 Next 主题中渲染 MathJax 数学公式 报错：hexo fs.SyncWriteStream is deprecated Hexo Next主题博客功能完善]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能工程师 学习笔记]]></title>
    <url>%2F2019%2F03%2F31%2F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%B8%88%2F</url>
    <content type="text"><![CDATA[一、机器学习原理及商品推荐系统实现1.1 引入 一个Kaggle竞赛优胜解决方案 任务：Avazu点击率预估竞赛 Rank 2nd Owen Zhang的解法 优胜算法的特点 特征工程 融合大法 多层 多种不同模型的组合 所以： 基础模型很重要（线性模型） 集成学习模型单模型性能好（GBDT） 特定问题的模型贡献大（FM） 模型融合很重要 课程内容安排 基本模型 线性模型： 线性回归， logistic回归， SVM 非线性模型： （线性模型核化）、分类回归树 集成学习模型（随机森林、GBDT） 数据预处理：数据清洗，特征工程，降维，聚类 模型融合 推荐系统/点击率预估问题特定解决方案1.2 机器学习简介 定义 数据 数据通常以二维数据表形式给出 每一行： 一个样本 每一列：一个属性/特征 例：Boston房价预测数据，根据某地区房屋属性，预测该地区预测房价 506行， 506个样本 14列 机器学习任务类型 监督学习（Supervised Learning） 分类（classfication） 回归（regression） 排序（ranking） 非监督学习（unsupervised learning） 聚类（clustering） 降维（dimensionality reduction） 概率密度估计（density estimation） 增强学习（reinforcement learning） 半监督学习（semi-supervised learning） 迁移学习（transfer learning） …… 监督学习 学习一个x-&gt;y 的映射f, 从而对新输入的x进行预测f（x） 例：波士顿房价预测 回归 若输出y∈R为连续值，则我们称之为一个回归（regression）任务例： 房价预测，预测二手车的价格 回归模型： 训练： 预测：y带帽表示预测 学习目标：训练集上预测值与真值之间的差异最小 损失函数：度量模型预测值与真值之间的差异 目标函数: 分类 若输出y为离散值，则我们称之为一个分类，标签空间y = {1,2, … C} 例：信用评分 分类： 概率问题 学习目标： 损失函数：01损失 需要预测的概率： 预测：最大后验估计（Maximum a Posteriori, MAP） 排序（Rank） 排序学习是推荐、搜素、广告的核心方法 排序学习中需要首先根据查询q及其文档集合进行标注（data labeling） 和提取特征（feature extraction） 才能得到D = {….} 非监督学习 发现数据中的“有意义的模式”， 亦被称为知识发现 训练数据不包含标签 标签在训练数据中为隐含变量 聚类例：人的“类型”分多少类？ 模型选择某个样本属于哪个类？ 降维多维特征，有些特征之间会相关而存在冗余很多算法中，降维算法成为了数据预处理的一部分， 如主成分分析（Principal Components Analysis, PCA） 半监督学习 当标注数据“昂贵”时有用 例：标注3D姿态、 蛋白质功能等等 多标签学习 有歧义标签学习 多实例学习 增强学习从行为的反馈(奖励或惩罚)中学习 设计一个回报函数（reward function）， 如果learning agent(如机器人、围棋ai程序)，在决定一步之后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负 增强学习的任务：找到一条回报值最大的路径1.3 一个典型的机器学习案例-对鱼进行分类 根据一些光学传感器对传送带上的鱼进行分类 形式化为机器学习问题 训练数据 每条鱼的测量向量 每条鱼的标签 测试 给定一个新的特征向量x 预测对应的标签y 将长度作为特征进行分类（直方图） 需要先做一个决策边界 最小化平均损失 将亮度作为特征进行分类 （直方图） 将长度和亮度一起作为特征（二维散点图） 线性决策函数 二次决策函数 更复杂的决策边界训练集上的误差 ≠ 测试集上的误差数据过拟合（overfitting）推广性（generalization）差 小结：设计一个鱼的分类器 选择特征 可能是最重要的步骤！（收集训练数据） 选择模型（如决策边界的形状） 根据训练数据估计模型 利用模型对新样本进行分类1.4 机器学习算法的组成部分 机器学习任务的一般步骤 确定特征 可能是最重要的步骤！（收集训练数据） 确定模型 目标函数/决策边界形状 模型训练：根据训练数据估计模型参数 优化计算 模型评估：在校验集上评估模型预测性能 模型应用/预测 模型 监督学习任务： 确定模型类别后，模型训练转化为求解模型参数 求解模型参数：目标函数最小化 非线性模型 基函数： x^2, log, exp, 样条函数，决策树…. 核化：将原问题转化为对偶问题，将对偶问题中的向量积 换成核函数k(xi,xj) 目标函数：通常包含两项：损失函数和正则项 损失函数 损失函数 - 回归 损失函数：度量模型预测值与真值之间的差异 对回归问题：令残差 r = f(x) - y L2损失：连续，但对噪声敏感 L1损失：不连续，对噪声不敏感 Huber 损失： 连续，对噪声不敏感 损失函数 - 分类 损失函数：度量模型预测值与真值之间的差异 对分类问题 0-1损失： logistic损失：亦称负log似然损失 指数损失： 合页损失： 正则项 复杂模型（预测）不稳定：方差大 正则项对复杂模型施加惩罚 正则项的必要性例：sin曲线拟合 增加L2正则岭回归：最小化RSS 欠拟合：模型太简单/对复杂性惩罚太多 样本数目增多时，可以考虑更复杂的模型 常见正则项 L2正则 L1正则 L0正则 非0参数的数目 不好优化，通常用L1正则近似 常见线性模型的损失和正则项组合 L2损失 L1损失 Huber损失 Logistic损失 合页损失 e-insensitive损失 L2正则 岭回归 L2正则 Logistic回归 SVM SVR L1正则 LASSO L1正则 Logistic回归 L2+L1正则 Elastic 模型训练 在训练数据上求目标函数极小值：优化 简单目标函数直接求解 如小数据集上的线性回归 更复杂问题：凸优化 （随机）梯度下降 牛顿法/拟牛顿法 … 梯度下降（Gradient Descent）算法 梯度下降/最速下降算法：快速寻找函数局部极小值 梯度下降算法：求函数J（θ）的最小值 给定初始值θ0 跟新θ，使得J（θ）越来越小 θt = θ t-1 - n▽θ J（θ） ( n : 学习率 ) 直到收敛到 / 达到预先设定的最大迭代次数 下降的步伐太小（学习率）非常重要：如果太小，收敛速度慢； 如果太大，可能会出现overshoot the minimum的现象 梯度下降求得的只是局部最小值 二阶导数 &gt; 0, 则目标函数为凸函数，局部极小值即为全局最小值 随机选择多个初始值，得到函数的多个局部极小值点。多个局部极小值点的最小值为函数的全局最小值 梯度下降算法每次学习都使用整个训练集，这样对大的训练数据集合，每次学习时间过长，对大的训练集需要消耗大量的内存。此时可采用随机梯度下降（Stochastic gradient descent, SGD), 每次从训练集中随机选择一部分样本进行学习。 更多（随机）梯度下降算法的改进版 动量（Momentum） Nesterov accelerated gradient (NAG) Adagrad RMSprop Adaptive Moment Estimation (Adam)… 模型选择与模型评估 同一个问题有不同的解决方案 如线性回归 vs. 决策树 哪个更好？ 模型评估与模型选择 在新数据点的预测误差最小 模型评估：已经选定最终的模型，估计它在新数据上的预测误差 模型选择：估计不同模型的性能，选出最好的模型 样本足够多：训练集和校验集 样本不够多：重采样技术来模拟校验集：交叉验证和bootstrap K-折交叉验证 交叉验证（Cross Validation, CV）： 将训练数据分成容量大致相等的K份（通常K = 5/10） 交叉验证估计的误差为：CV（M）= 1 / K… 模型选择 对多个不同模型，计算其对应的误差CV（M）， 最佳模型为CV（M）最小的模型 模型复杂度和泛化误差的关系通常是U形曲线：1.5 学习环境简介 编程语言 Python 数据处理工具包 Numpy SciPy pandas 数据可视化工具包 Matplotlib Seaborn 机器学习工具包 scikit learn 示例代码：INotebook]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>人工智能,</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉基础入门 学习笔记]]></title>
    <url>%2F2019%2F03%2F30%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[一、 计算机视觉和深度学习概述 计算机视觉回顾 计算机视觉（computer vision）定义 数据（静态图片，视频） 算法（机器学习算法，神经网络）本质上是一个回归+分类 计算机视觉的重要性 三大任务：图像识别（image classification）车牌识别，人脸识别 三大任务：目标检测（object detection = classification + localization）行人检测和车辆检测 三大任务：图像分割图像语义分割个体分割 = 检测 + 分割 视觉目标跟踪（tracking） 视频分割 图像风格迁移 生成对抗网络（GAN） 视频生成 深度学习介绍 2006 Hinton bp(反向传播) 2012 Krizhevsky A 深度学习 深度卷积 RNN LSTM 持续信息 视觉识别，语音识别，DeepMind, AlphaGo 人脸识别：LFW 错误率5% -&gt; 0.5% 图像分割 VGGNet, GoogleNet, ResNet, DenseNet 常见的深度学习开发平台 Torch, TensorFlow, MatConvNetTheano, Caffe 课程介绍 图像识别：Alexnet, VGGnet, GoogleNet, ResNet, DenseNet 目标检测Fast-rcnn, faster-rcnn, Yolo, Retina-Net 图像分割FCN, Mask-Rcnn 目标跟踪GORURN， ECO 图像生成GAN， WGAN 光流FlowNet 视频分割Segnet 二、 图像分类与深度卷积网络的模型 图像分类 图像分类的挑战光照变化形变类内变化 图像分类定义 目标分类框架 泛化能力如何提高泛化能力？ 需要用图像特征来描述图像 训练和测试的流程 图像特征 color: Qutantize RGB values global shape: PCA space local shape: shape context texture: Filter banks SIFT, Hog, LBP, Harr 支持向量机（SVM） 超平面与支持向量 最大化间隔 svm分类（python）以lris兰花分类为例 程序实现 更好的特征 CNN特征 学习出来的 如何学习？ 构造神经网络 神经网络原理 神经网络做图像分类 神经网络搭建 神经网络的基本单元：神经元 激励函数 Sigmoid、tanh、ReLU、Leaky ReLU、Maxout、ELU 卷积层 卷积滤波的计算 卷积层可视化 池化层（pooling layer） 特征表达更加紧凑，同时具有位移不变性 全连接层 损失函数 交叉熵损失函数（SIGMOID_CROSS_ENTROPY_LOSS) 应用于二分类问题 Softmax 损失函数（SOFTMAX_LOSS) 多分类问题 欧式距离损失函数（EUCLIDEAN_LOSS）回归问题 对比损失函数（Contrastive loss）用来计算两个图像之间的相似度 Triplet loss 训练网络 网络训练和测试 卷积神经网络介绍 Alexnet, VGGnet, GoogleNet, ResNet, DenseNet 训练技巧， 防止过拟合（泛化能力不强） 数据增强（Data augmentation） 水平翻转， 随机裁剪和平移变换，颜色、光照变换 Dropout 其他有助于训练的手段 L1， L2正则化 Batch Normalization 利用caffe搭建深度网络做图像分类]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>基础</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
</search>
