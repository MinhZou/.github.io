<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo + GitHub Pages + Next在windows下搭建个人博客]]></title>
    <url>%2F2019%2F04%2F01%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[才搭好博客，发现在博客发布文章确实比微信公众号方便很多，这里简略说下用 Hexo + GitHub Pages + Next搭建个人博客的课程，大部分经验都是来自于网络，我会在整个过程后面附上参考的文章，一来总结搭建博客的过程，二来减少后来人踩坑。 整个过程： 1、注册Github账号及创建仓库 2、安装Git for Windows 3、配置Git 4、安装node.js 5、安装Hexo 6、使用next设计个性化博客 7、连接Hexo和Github Pages及部署博客 8、购买域名并解析以上就是全部的过程，当然具体还有很多细节，比如更换配置、设置文章字数的单位，阅读时常的单位，设置评论区，具体的东西还是要依据个人的喜好调整，但是next主题里面基本都集成了这些功能，只要稍微调整下就行。 参考的文章： 参考的整个过程 各种个性化小功能 给统计量添加单位 各种个性化设置 文章发布 GitHub/Coding双线部署 小书匠Markdown使用手册 在Markdown中输入数学公式(MathJax) Hexo 的 Next 主题中渲染 MathJax 数学公式 报错：hexo fs.SyncWriteStream is deprecated Hexo Next主题博客功能完善 MathJax语法 MathJax与LaTex介绍 MathJax(Markdown中的公式)的基本使用语法]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能工程师 学习笔记]]></title>
    <url>%2F2019%2F03%2F31%2F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%B8%88%2F</url>
    <content type="text"><![CDATA[一、机器学习原理及商品推荐系统实现1.1 引入 一个Kaggle竞赛优胜解决方案 任务：Avazu点击率预估竞赛 Rank 2nd Owen Zhang的解法 优胜算法的特点 特征工程 融合大法 多层 多种不同模型的组合 所以： 基础模型很重要（线性模型） 集成学习模型单模型性能好（GBDT） 特定问题的模型贡献大（FM） 模型融合很重要 课程内容安排 基本模型 线性模型： 线性回归， logistic回归， SVM 非线性模型： （线性模型核化）、分类回归树 集成学习模型（随机森林、GBDT） 数据预处理：数据清洗，特征工程，降维，聚类 模型融合 推荐系统/点击率预估问题特定解决方案 1.2 机器学习简介 定义 数据 数据通常以二维数据表形式给出 每一行： 一个样本 每一列：一个属性/特征 例：Boston房价预测数据，根据某地区房屋属性，预测该地区预测房价 506行， 506个样本 14列 机器学习任务类型 监督学习（Supervised Learning） 分类（classfication） 回归（regression） 排序（ranking） 非监督学习（unsupervised learning） 聚类（clustering） 降维（dimensionality reduction） 概率密度估计（density estimation） 增强学习（reinforcement learning） 半监督学习（semi-supervised learning） 迁移学习（transfer learning） …… 监督学习 学习一个x-&gt;y 的映射f, 从而对新输入的x进行预测f（x）D = \{X_i,y_i\}^N_{i=1} D：训练数据集 N：训练样本数目 $X_i$: 第i个样本的输入，亦被称为特征、属性或协变量 $y_i$: 第i个训练样本的输出，亦被称为响应，如类别标签、序号或数值 例：波士顿房价预测 回归 若输出y∈R为连续值，则我们称之为一个回归（regression）任务例： 房价预测，预测二手车的价格 假设回归模型：$y = f(\mathbf x|\theta)$ 如在线性回归中，$f(\mathbf x|w) = \mathbf w^T \mathbf x$ 训练：根据训练数据 $D = \{\mathbf X_i,y_i\}^N_{i=1}$ 学习映射 预测：对新的测试数据x进行预测：$\hat f = f(x)$ y带帽表示预测 学习目标：训练集上预测值与真值之间的差异最小 损失函数：度量模型预测值与真值之间的差异，如L(f(\mathbf x),y) = \frac 12(f(x) - y)^2 目标函数为：$J(\mathbf \theta) = \frac1N \sum_{i = 1}^N L(f(\mathbf x_i|\mathbf \theta), y_i)$ 分类 若输出y为离散值，则我们称之为一个分类，标签空间y = {1,2, … C} 例：信用评分 分类： 学习从输入x到输出y的映射f:概率问题$\hat y = f(\mathbf x) = \underset{c} {arg\ max} \ p(y = c\mid \mathbf x, D)$ 学习目标： 损失函数：01损失 l_{0/1}(y, \hat y) = \begin {cases} 0 & y = \hat y \\ 1 & otherwise \end{cases} 需要预测的概率： 预测：最大后验估计（Maximum a Posteriori, MAP）$\hat y = f(\mathbf x) = \underset{c} {arg\ max}\ p(y = c\mid \mathbf x, D)$ 排序（Rank） 排序学习是推荐、搜素、广告的核心方法 排序学习中需要首先根据查询q及其文档集合进行标注（data labeling） 和提取特征（feature extraction） 才能得到D = {….} 非监督学习 发现数据中的“有意义的模式”， 亦被称为知识发现 训练数据不包含标签 标签在训练数据中为隐含变量 $ D = \{ \bf X_i\}_{ i= 1}^ N $ 聚类例：人的“类型”分多少类？ 模型选择$ K^* = arg\ max _K\ p(K \mid D)$某个样本属于哪个类？ 降维多维特征，有些特征之间会相关而存在冗余很多算法中，降维算法成为了数据预处理的一部分， 如主成分分析（Principal Components Analysis, PCA） 半监督学习 当标注数据“昂贵”时有用 例：标注3D姿态、 蛋白质功能等等 多标签学习 有歧义标签学习 多实例学习 增强学习从行为的反馈(奖励或惩罚)中学习 设计一个回报函数（reward function）， 如果learning agent(如机器人、围棋ai程序)，在决定一步之后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负 增强学习的任务：找到一条回报值最大的路径 1.3 一个典型的机器学习案例-对鱼进行分类 根据一些光学传感器对传送带上的鱼进行分类 形式化为机器学习问题 训练数据 每条鱼的测量向量 每条鱼的标签 测试 给定一个新的特征向量x 预测对应的标签y 将长度作为特征进行分类（直方图） 需要先做一个决策边界 最小化平均损失 将亮度作为特征进行分类 （直方图） 将长度和亮度一起作为特征（二维散点图） 线性决策函数 二次决策函数 更复杂的决策边界训练集上的误差 ≠ 测试集上的误差数据过拟合（overfitting）推广性（generalization）差 小结：设计一个鱼的分类器 选择特征 可能是最重要的步骤！（收集训练数据） 选择模型（如决策边界的形状） 根据训练数据估计模型 利用模型对新样本进行分类 1.4 机器学习算法的组成部分 机器学习任务的一般步骤 确定特征 可能是最重要的步骤！（收集训练数据） 确定模型 目标函数/决策边界形状 模型训练：根据训练数据估计模型参数 优化计算 模型评估：在校验集上评估模型预测性能 模型应用/预测 模型 监督学习任务：$D = \{X_i, y_i\} _{i = 1} ^ N $ 模型：对给定的输入x, 如何预测其标签$ \hat y$ 不同模型对数据的假设不同 最简单的模型：线性模型$ f(x) = \sum_j w_j x_j = \bf w^T \bf x$ 确定模型类别后，模型训练转化为求解模型参数 如对线性模型参数为!$\theta = \{w_j \mid j = 1,…, D\}$,其中D为特征维数 求解模型参数：目标函数最小化 非线性模型 基函数： $x^2$, log, exp, 样条函数，决策树…. 核化：将原问题转化为对偶问题，将对偶问题中的向量积$\langle x_i, x_j\rangle$ 换成核函数$k(x_i,x_j)$ 目标函数：通常包含两项：损失函数和正则项J(\theta) = \frac 1N \sum_{i=1}^N\ L(f(x_i; \theta), y_i) + R(\theta) 损失函数 损失函数 - 回归 损失函数：度量模型预测值与真值之间的差异 对回归问题：令残差 $r = f(\bf x) - y$ L2损失：连续，但对噪声敏感L_2 (r) = \frac 12 r ^2 L1损失：不连续，对噪声不敏感L_1(r) = |r| Huber 损失： 连续，对噪声不敏感L_\delta (r) = \begin{cases} \frac 12 r^2 & if|r| \le \delta\\ \delta |r| - \frac 12 \delta^2 & if|r| \ge \delta\end{cases} 损失函数 - 分类 损失函数：度量模型预测值与真值之间的差异 对分类问题 0-1损失：$l_{0/1}(y,f(x)) = \begin{cases} 1 &amp; yf(x) \lt 0 \\ 0 &amp; othereise\end{cases}$ logistic损失：亦称负log似然损失 $l_{log}(y,f(x)) = log(1 + exp(-yf(x)))$ 指数损失：$l_{exp}(y,f(x)) = exp(-yf(x))$ 合页损失：$l_{hinge}(y,f(x)) = max(0, 1 - yf(x))$ 正则项 复杂模型（预测）不稳定：方差大 正则项对复杂模型施加惩罚 正则项的必要性例：sin曲线拟合 增加L2正则岭回归：最小化RSS 欠拟合：模型太简单/对复杂性惩罚太多 样本数目增多时，可以考虑更复杂的模型 常见正则项 L2正则: $R(\theta) = \lambda ||\theta||^2_2 = \lambda \sum^D_{j=1} \theta_j^2$ L1正则: $R(\theta) = \lambda |\theta| = \lambda \sum ^D_{j=1}|\theta_j|$ L0正则: $R(\theta) = \lambda||\theta||_0$ 非0参数的数目 不好优化，通常用L1正则近似 常见线性模型的损失和正则项组合 L2损失 L1损失 Huber损失 Logistic损失 合页损失 e-insensitive损失 L2正则 岭回归 L2正则 Logistic回归 SVM SVR L1正则 LASSO L1正则 Logistic回归 L2+L1正则 Elastic 模型训练 在训练数据上求目标函数极小值：优化 简单目标函数直接求解 如小数据集上的线性回归 更复杂问题：凸优化 （随机）梯度下降 牛顿法/拟牛顿法 … 梯度下降（Gradient Descent）算法 梯度下降/最速下降算法：快速寻找函数局部极小值 梯度下降算法：求函数J（θ）的最小值 给定初始值θ0 跟新θ，使得J（θ）越来越小 $θ_t = θ ^{t-1} - \eta\nabla J(θ)$ ( $\eta$ : 学习率 ) 直到收敛到 / 达到预先设定的最大迭代次数 下降的步伐太小（学习率）非常重要：如果太小，收敛速度慢； 如果太大，可能会出现overshoot the minimum的现象 梯度下降求得的只是局部最小值 二阶导数 &gt; 0, 则目标函数为凸函数，局部极小值即为全局最小值 随机选择多个初始值，得到函数的多个局部极小值点。多个局部极小值点的最小值为函数的全局最小值 梯度下降算法每次学习都使用整个训练集，这样对大的训练数据集合，每次学习时间过长，对大的训练集需要消耗大量的内存。此时可采用随机梯度下降（Stochastic gradient descent, SGD), 每次从训练集中随机选择一部分样本进行学习。 更多（随机）梯度下降算法的改进版 动量（Momentum） Nesterov accelerated gradient (NAG) Adagrad RMSprop Adaptive Moment Estimation (Adam)… 模型选择与模型评估 同一个问题有不同的解决方案 如线性回归 vs. 决策树 哪个更好？ 模型评估与模型选择 在新数据点的预测误差最小 模型评估：已经选定最终的模型，估计它在新数据上的预测误差 模型选择：估计不同模型的性能，选出最好的模型 样本足够多：训练集和校验集 样本不够多：重采样技术来模拟校验集：交叉验证和bootstrap K-折交叉验证 交叉验证（Cross Validation, CV）： 将训练数据分成容量大致相等的K份（通常K = 5/10） 交叉验证估计的误差为：CV(M)= \frac1K \sum ^K_{k = 1} E_k(M) 模型选择 对多个不同模型，计算其对应的误差CV（M）， 最佳模型为CV（M）最小的模型 模型复杂度和泛化误差的关系通常是U形曲线：1.5 学习环境简介 编程语言 Python 数据处理工具包 Numpy SciPy pandas 数据可视化工具包 Matplotlib Seaborn 机器学习工具包 scikit learn 示例代码：INotebook NumPy NumPy(Numeric Python)是Python的开源数值计算扩展，可用来存储和处理大型矩阵 Numpy包括： N维数组(ndarray) 实用的线性代数、傅里叶变换和随机数生成函数 Numpy和稀疏矩阵运算包SciPy配合使用更加方便 SciPy SciPy是建立在NumPy的基础上、是科学和工程设计的Python工具包，提供统计、优化和数值微积分计算等功能 NumPy 处理$10^6$级别的数据通常没有大问题，但当数据量达到$10^7$级别时速度开始发慢，内存受到限制（具体情况取决于实际内存的大小） 当处理超大规模数据集，比如$10^10$级别，且数据中包含大量的0时，可采用稀疏矩阵可显著的提高速度和效率 Pandas(Pandel data structures) Pandas是Python语言的“关系型数据库”数据结构和数据分析工具，非常高效且易于使用 基于NumPy补充了大量数据操作功能，能实现统计、分组、排序、透视表(SQL语句的大部分功能) Pandas主要有2种重要的数据类型 series：一维序列 DataFrame：二维表(机器学习数据的常用数据结构) Matplotlib Matplotlib是Python语言的2D图形绘制工具 Seaborn Seaborn是一个基于Matplotlib的Python可视化工具包，提供更高层次的用户接口，可以给出漂亮的数据统计 Scikit - Learn Machine Learning in Python Scikit-Learn是基于Python的开源机器学习模块，最早于2007年由David Cournapeau发起 基本功能有六部分：分类（Classification），回归（Regression），聚类（Clustering），数据降维（Dimensionality reduction），模型选择（Model Selection），数据预处理（Preprocessing） 对于具体的机器学习问题，通常可以分为三个步骤 数据准备与预处理（Preprocessing, Dimensionality reduction） 模型选择与训练（Classification, Regression, Clustering） 模型验证与参数调优（Model Selection） 各种机器学习模型有统一的接口 模型既有默认参数，也提供多种参数调优方法 卓越的文档 丰富的随附任务功能集合 活跃的社区提供开发和支持 1.6 线性回归 目标函数通常包含两项：损失函数和正则项J(\bf \theta) = \frac1N \sum_{i = 1}^N L(f(\bf x_i|\bf \theta), y_i) + \lambda R(\bf \theta) 对回归问题，损失函数可以采用L2损失，得到\begin{eqnarray}J(\theta) &=&\sum_{i=1}^NL(y_i,\hat y_i) \\ &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\ &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 残差平方和（residual sum of squares, RSS） 由于线性模型比较简单，实际应用中有时正则项为空，得到最小二乘线性回归（Ordinary Least Square, OLS\begin{eqnarray}J(\theta) &=&\sum_{i=1}^NL(y_i,\hat y_i) &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\ &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 正则项可以为L2正则，得到岭回归（Ridge Regression）J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda ||w||^2_2 正则项也可以选L1正则，得到Lasso模型： J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda |w| 当$\lambda$取合适值时，Lasso（Least absolute shrinkage and selection operator）的结果是稀疏的（w的某些元素系数为0），起到特征选择作用 为什么L1正则的解是稀疏的？ 线性回归模型的概率解释 最小二乘（线性）回归等价于极大似然估计 假设：$ y = f(\bf x) + \epsilon = w^T + \epsilon $其中$\epsilon$为线性预测和真值之间的残差我们通常假设残差的分布为$\epsilon \sim N(0,\sigma ^2)$,因此线性回归可写成：$p(y|x,\theta) \sim N(y| \bf w^T \bf x, \sigma^2)$,其中$ \bf \theta = (\bf w, \sigma ^2)$ 正则（线性）回归等价于高斯先验（L2正则）或Laplace先验下（L1正则）的贝叶斯估计 Recall：极大似然估计 极大似然估计（Maximize Likelihood Estimator, MLE）定义为\hat \theta = \underset {\theta} {arg\ max}\ log p(D\mid \theta) 其中（log）似然函数为l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) 表示在参数为$\theta$的情况下，数据$D ={\bf x_i,y_i}^N_{i=1}$ 极大似然：选择数据出现概率最大的参数 线性回归的MLEp(y_i|x_i,\bf w,\sigma ^2) \sim N(y_i\mid \bf w^T \bf x_i, \sigma^2) = \frac 1{\sqrt{2\pi}\sigma} exp(-\frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2)) OLS的似然函数为l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) 极大似然可等价地写成极小负log似然损失（negative log likelihood, NLL） \begin{eqnarray}{NLL(\bf \theta)} &=& \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) \\ &=& - \sum_{i=1}^N log ((\frac 1{2 \pi \sigma^2})^ \frac 12 exp(- \frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))) \\ &=& \frac N2 log(2\pi \sigma ^2) + \frac 1{2 \sigma^2} \sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 正则回归等价于贝叶斯估计 假设残差的分布为$\epsilon \sim N(0, \sigma ^2)$,线性回归可写成：$p(y_i \mid \bf x_i, \theta) \sim N(y_i \mid \bf w^T \bf x_i，\sigma ^2)$$p(y\mid \bf X, \bf w, \sigma ^2) = N(\bf y \mid \bf X \bf w, \sigma ^2 \bf I_N) \propto exp(- \frac 1{2\sigma ^2}((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w)))$ 若假设参数为w的先验分布为 $w_j \sim N(0, \tau ^2)$ 偏向较小的系数值，从而得到的曲线也比较平滑$p(\bf w) =\prod_{j=1}^{D} N(w_j \mid 0, \tau ^2) \propto exp(- \frac 1{2\tau^2} \sum_{j=1}^D \bf w_j^2 = exp(- \frac 1{2\tau^2} ( \bf w^T \bf w ) )) $ 其中$1/\tau ^2$控制先验的强度 根据贝叶斯公式，得到参数的后验分布为$p(y\mid \bf X, \bf w, \sigma ^2) = \propto exp(- \frac 1{2\sigma ^2} ((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w) ) - \frac 1{2 \tau^2} ( w^Tw ) )$ 则最大后验估计(MAP)等价于最小目标函数$J(\bf w) = (\bf y - \bf X\bf w)^T(\bf y - \bf X\bf w) + \frac {\sigma ^2}{\tau^2} \bf w^T \bf w $ 对比岭回归的目标函数$J(\bf w) = \sum_{i=1}^N(y_i -\bf w^T\bf x_i)^2 + \lambda \Vert \bf w\Vert ^2_2$ 小结 线性回归模型可以放到机器学习一般框架 损失函数：L2损失，… 正则：无正则， L2正则，L1正则… 正则回归模型可视为先验为正则、似然为高斯分布的贝叶斯估计 L2正则：先验分布为高斯分布 L1正则：先验分布为Laplace分布```]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉基础入门 学习笔记]]></title>
    <url>%2F2019%2F03%2F30%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[一、 计算机视觉和深度学习概述 计算机视觉回顾 计算机视觉（computer vision）定义 数据（静态图片，视频） 算法（机器学习算法，神经网络）本质上是一个回归+分类 计算机视觉的重要性 三大任务：图像识别（image classification）车牌识别，人脸识别 三大任务：目标检测（object detection = classification + localization）行人检测和车辆检测 三大任务：图像分割图像语义分割个体分割 = 检测 + 分割 视觉目标跟踪（tracking） 视频分割 图像风格迁移 生成对抗网络（GAN） 视频生成 深度学习介绍 2006 Hinton bp(反向传播) 2012 Krizhevsky A 深度学习 深度卷积 RNN LSTM 持续信息 视觉识别，语音识别，DeepMind, AlphaGo 人脸识别：LFW 错误率5% -&gt; 0.5% 图像分割 VGGNet, GoogleNet, ResNet, DenseNet 常见的深度学习开发平台 Torch, TensorFlow, MatConvNetTheano, Caffe 课程介绍 图像识别：Alexnet, VGGnet, GoogleNet, ResNet, DenseNet 目标检测Fast-rcnn, faster-rcnn, Yolo, Retina-Net 图像分割FCN, Mask-Rcnn 目标跟踪GORURN， ECO 图像生成GAN， WGAN 光流FlowNet 视频分割Segnet 二、 图像分类与深度卷积网络的模型 图像分类 图像分类的挑战光照变化形变类内变化 图像分类定义 目标分类框架 泛化能力如何提高泛化能力？ 需要用图像特征来描述图像 训练和测试的流程 图像特征 color: Qutantize RGB values global shape: PCA space local shape: shape context texture: Filter banks SIFT, Hog, LBP, Harr 支持向量机（SVM） 超平面与支持向量 最大化间隔 svm分类（python）以lris兰花分类为例 程序实现 更好的特征 CNN特征 学习出来的 如何学习？ 构造神经网络 神经网络原理 神经网络做图像分类 神经网络搭建 神经网络的基本单元：神经元 激励函数 Sigmoid、tanh、ReLU、Leaky ReLU、Maxout、ELU 卷积层 卷积滤波的计算 卷积层可视化 池化层（pooling layer） 特征表达更加紧凑，同时具有位移不变性 全连接层 损失函数 交叉熵损失函数（SIGMOID_CROSS_ENTROPY_LOSS) 应用于二分类问题 Softmax 损失函数（SOFTMAX_LOSS) 多分类问题 欧式距离损失函数（EUCLIDEAN_LOSS）回归问题 对比损失函数（Contrastive loss）用来计算两个图像之间的相似度 Triplet loss 训练网络 网络训练和测试 卷积神经网络介绍 Alexnet, VGGnet, GoogleNet, ResNet, DenseNet 训练技巧， 防止过拟合（泛化能力不强） 数据增强（Data augmentation） 水平翻转， 随机裁剪和平移变换，颜色、光照变换 Dropout 其他有助于训练的手段 L1， L2正则化 Batch Normalization 利用caffe搭建深度网络做图像分类]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>基础</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
</search>
