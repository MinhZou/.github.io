<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python爬虫基础知识整理]]></title>
    <url>%2FPython%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86.html</url>
    <content type="text"><![CDATA[整理下Python涉及的基本知识，巩固和方便查阅。 Python爬虫的流程 获取网页request、urllib和selenium（模拟登陆），多进程多线程抓取、登陆抓取、突破IP封禁和服务器抓取 解析网页html文档，json格式文本可用re正则表达式、BeautifulSoup和lxml解析，二进制数据（如图片视频）保存 存储数据保存特定格式的文件如：txt, xls, csv等，存入MySQL数据库和存入MongoDB数据库 HTML+CSSHTMLHTML就是说明一个网页中包含哪些元素，HTML标签指由成对尖括号包围的关键词，解析网页常常需要根据这些标签进行定位。举几个例子：12345678&lt;a&gt; 定义锚&lt;dd&gt; 定义定义列表中项目的描述。&lt;div&gt; 定义文档中的节&lt;h1&gt; to &lt;h6&gt; 定义 HTML 标题&lt;li&gt; 定义列表的项目&lt;p&gt; 定义段落&lt;span&gt; 定义文档中的节&lt;ul&gt; 定义无序列表。 CSSCSS指层叠样式表 (Cascading Style Sheets)，就是用来让网页中的元素显示的更加美观。CSS 规则由两个主要的部分构成：选择器，以及一条或多条声明，每条声明由一个属性和一个值组成，例如：12345p&#123;color:red;text-align:center;&#125; id 和 class 选择器： 12345678910111213141516171819202122&lt;style&gt;#center&#123; text-align:center; color:red;&#125; &lt;/style&gt;&lt;body&gt; &lt;p id="center"&gt;Hello World!&lt;/p&gt;&lt;/body&gt;&lt;style&gt;p.center&#123; text-align:center;&#125;&lt;/style&gt;&lt;body&gt; &lt;p class="center"&gt;Hello World!&lt;/p&gt;&lt;/body&gt; id选择器和class选择器的区别就是class可以在多个元素中使用。id 选择器以 “#” 来定义class选择器以一个点”.”号显示 获取网页request库 请求方式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758requests.get('http://www.minhzou.top/') # 请求指定的页面信息，并返回实体主体requests.post('url') # 从请求服务器接受所指定的文档作为对所标识的URI的新的从属实体requests.put('url') # 从客户端向服务器传送的数据取代指定的文档的内容requests.head('url') # 只请求页面的首部response = requests.get('http://www.minhzou.top/')response = response.text# 添加headersheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'&#125;response = requests.get('http://www.minhzou.top/',headers = headers)# 使用params带参数请求data = &#123;'key1': 'value1','key2': 'value2'&#125;response = requests.get("http://www.minhzou.top/", params=data)# 解析jsonresponse = requests.get('http://www.minhzou.top/')print(response.json())print(json.loads(response.text))# .content获取二进制文件print（response.content）# post请求data = &#123;'key1': 'value1','key2': 'value2'&#125;response = requests.post("http://www.minhzou.top/", data=data)# 响应response = requests.get('http://www.minhzou.top/')response.status_coderesponse.headersresponse.cookiesresponse.url# 代理设置proxies = &#123; "http": "http://127.0.0.1:1234", "https": "https://127.0.0.1:1234",&#125;response = requests.get("http://www.minhzou.top/", proxies=proxies) # 超时设置import requestsfrom requests.exceptions import ReadTimeouttry: response = requests.get("http://www.minhzou.top/", timeout = 0.5) print(response.status_code)except ReadTimeout: print('Timeout') urllib库1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556urllib.request # 请求模块urllib.error # 异常处理模块urllib.parse # url解析模块urllib.robotparser # robots.txt解析模块# get请求import urllib.requestresponse = urllib.request.urlopen('http://www.baidu.com')print(response.read().decode('utf-8')) # response.read() 获取到网页的内容# urlopen常用参数urllib.requeset.urlopen(url,data,timeout)# 使用data参数 post请求data = bytes(urllib.parse.urlencode(&#123;'word': 'hello'&#125;), encoding='utf8')print(data)response = urllib.request.urlopen('http://www.minhzou.top/', data=data)print(response.read())# request添加头部信息from urllib import request, parseurl = 'https://www.lagou.com/'headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36', 'Host': 'www.lagou.com'&#125;dict = &#123; 'first': 'true', 'pn': '1', 'kd': 'python'&#125;data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, headers=headers, method='POST')response = request.urlopen(req)print(response.read().decode('utf-8'))# 代理，ProxyHandlerimport urllib.requestproxy_handler = urllib.request.ProxyHandler(&#123; 'http': 'http://127.0.0.1:1234', 'https': 'https://127.0.0.1:1234'&#125;)opener = urllib.request.build_opener(proxy_handler)response = opener.open('http://www.minhzou.top/')print(response.read())# cookie,HTTPCookiProcessorimport http.cookiejar, urllib.requestcookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')for item in cookie: print(item.name+"="+item.value) selenium库12345678910111213141516171819202122232425262728293031323334353637383940414243from selenium import webdriverchrome_driver = 'E:\chromedriver_win32\chromedriver.exe' # 需要下载适合浏览器版本的driverbrowser = webdriver.Chrome(executable_path=chrome_driver)input = browser.find_element_by_id("q")browser.get(url)browser.close()# 8种查找元素的方法 / element改成elements 多元素查找find_element_by_namefind_element_by_idfind_element_by_xpathfind_element_by_link_textfind_element_by_partial_link_textfind_element_by_tag_namefind_element_by_class_namefind_element_by_css_selectorfrom selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get("http://www.baidu.com")input = browser.find_element(By.ID,"q")print(input)browser.close()# 元素交互操作# 交互动作# 获取元素属性input.get_attribute('class')# 获取文本值, ID，位置，标签名textidlocationtag_namesize# 浏览器的前进和后退browser.back()browser.forward()# 选项卡管理browser.execute_script('window.open()') # 新开选项卡browser.switch_to_window(browser.window_handles[1]) 解析网页re正则表达式1234567891011121314151. 原子普通字符：abc非打印字符：\n, \t通用字符： \w, \W, \d, \D, \s, \S原子表：[abc],[^abc]2. 元字符任意匹配元字符：. 边界限制元字符： ^ ,$限定符：* , ?, +, &#123;n&#125;, &#123;n,&#125;, &#123;n,m&#125;模式选择符：|模式单元符：() 3. 模式修正re.I, re.M, re.L, re.U, re.S4. 贪婪模式与懒惰模式贪婪模式.* 加问号变成懒惰模式 .* ? XPath选择器 基本语法 123456nodename # 选取此节点的所有子节点/ # 从根节点选取// # 从匹配选择的当前节点选择文档中的节点，而不考虑他们的位置. # 选取当前节点.. # 选取当前节点的父节点@ # 选取属性 核心思想：写XPath就是写地址 123456//标签1[@属性1=“属性值1”]/标签2[@属性=“属性2”]/.../test()//标签1[@属性1=“属性值1”]/标签2[@属性=“属性2”]/.../@属性n/artical/div[1]/artical/div[position()&lt;3] 前两个元素//div[price&gt;3.5]//div[@class="center"]/ul/li/text() CSS选择器 基本语法 1234567891011121314151617 * # 选取所有节点#title # 选取 id 为 title 的元素.col-md # 选取所有 class 包含 col-md 的元素li a # 选取所有 li 下的 a 元素ul + p # 选取 ul 后面的第一个 p 元素div#title &gt; ul # 选取 id 为 title 的 div 的第一个 ul 子元素ul ~ p # 选取 与 url 相邻的所有 p 元素span#title ::text # 选取 id 为 title 的 span 元素的文本值a.link::attr(href) # 选取 class 为 link 的 a 元素的 href 属性值div:not(#content-container) # 选取所有id为非content-container 的diva:nth-child(2) # 选取下面第二个标签，如果是a的话则选取，不是则不取a:nth-child(2n) # 选取第偶数个a元素a:nth-child(2n+1) # 选取第奇数个a元素a[title] # 选取所有拥有title属性的a元素a[href=”https://www.lagou.com/”] # 选取所有href属性为https://www.lagou.com/的a元素a[href*=”www.lagou.com”] # 选取所有href属性值中包含www.lagou.com的a元素a[href^=”http”] # 选取所有href属性值中以http开头的a元素 例子 1234# 选取文字item.css('::text').extract()[1]# div a 选取所有div下所有a元素response.css('div a') BeautifulSoup提取数据的方式：find选择器，css选择器 12345678# find选择器soup.find_all(attrs=&#123;'name': 'elements'&#125;title = house.find('div', class_='house-title').a.text.strip()rooms = house.find('div', class_='details-item').span.textsoup.find_all(re.compile(), html)# css选择器soup.select("header h3"）ranking = school.select('td:nth-of-type(1)')[0].text lxml提取数据的方式：XPath选择器，css选择器 数据保存123456789101112131415# 写入xlsworkbook = xlwt.Workbook(encoding='utf-8')worksheet = workbook.add_sheet('house_price', cell_overwrite_ok=True)for k, row in enumerate(information): for j, col in enumerate(row): worksheet.write(k, j, col)workbook.save('house_price.xls')# 写入csvwith open('ranking.csv', 'a+', newline='') as f: writer = csv.writer(f) writer.writerow(data)# 写入txtfilepath = "E:/data.txt" with open(filepath, "a+") as fp: fp.write(data) Scrapy框架Scrapy框架：分布式，异步处理请求，自动去重包含部分：Scrapy Engine:控制数据流，触发事件Scheduler: 调度器Downloader:获取页面数据Spiders:获取Item和跟进的urlItem Pipeline:处理被Spide提取的Item Downloader middlewares: 更换代理IP，cookies, User-Agent，自动重试等Spider middleware：输出、处理异常等 只简单的在Scrapy框架上写过爬虫，因为要求不高所以很多功能自然也没用上，但是先了解了整个Scrapy，方便如果以后用得到拾起来。 App爬虫基本原理就是抓包分析python还可以使用uiautomato操作Android手机模拟人为操作 其他 异步更新技术AJAX(Asynchronous Javascript And XML，异步JavaScript和XML) 动态网页抓取的两种技术：通过浏览器审查元素解析真实网页地址和使用selenium模拟浏览器 Python中使用多线程的两种方法：① 函数式：调用_thread模块中的start_new_thread()函数产生新线程。 ② 调用Threading库创建线程，从threading.Thread继承。 小结整个python爬虫学习过程大概花了十多天，只爬过7-8个网站，但也遇到了很多情况，有了一定基础，正如有前辈说过的：如果你爬过80%的主流网站，下次遇到需求你就都能很快的响应。想起《曾国藩家书》的箴言：为学譬如熬肉，先须用猛火煮，然后用慢火温。到此，Python爬虫学习告一段落。 ps: 希望传上之后，这篇博客排版还能看吧。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卡特兰大数（Catalan大数）]]></title>
    <url>%2F%E5%8D%A1%E7%89%B9%E5%85%B0%E5%A4%A7%E6%95%B0%EF%BC%88Catalan%E5%A4%A7%E6%95%B0%EF%BC%89.html</url>
    <content type="text"><![CDATA[卡特兰数（Catalan数）因为有很多应用，很重要也很常见，之前做某机试题就是以Catalan大数为背景，当然用java可以方便求解，但用C/C++公式求解对于大数很容易超过long long 型，所以要运用大整数技巧。 Catalan数的典型应用： 一个有n个结点的二叉树总共有多少种形态？n个非叶节点的满二叉树的形态数? 在一个圆上，有2*K个不同的结点，我们以这些点为端点，连K条线段，使得每个结点都恰好用一次。在满足这些线段将圆分成最少部分的前提下，请计算有多少种连线的方法（类似：将多边行划分为三角形问题。将一个凸多边形区域分成三角形区域(划分线不交叉)的方法数?） 出栈次序问题。一个栈(无穷大)的进栈序列为1,2,3,..n,有多少个不同的出栈序列? n*n的方格地图中，从一个角到另外一个角，不跨越对角线的路径数？ 括号化问题。一个合法的表达式由()包围，()可以嵌套和连接，如(())()也是合法 表达式，它们可以组成的合法表达式的个数为?…… 等等很多应用 Catalan数的公式递推关系：h(n)= h(0)h(n-1)+h(1)h(n-2) + … + h(n-1)h(0) (n&gt;=2)递推关系的解：h(n)=C(2n,n)/(n+1) (n=0,1,2,…)另类递推式：h(n)=h(n-1)(4n-2)/(n+1)递推关系的另类解：h(n)=c(2n,n)-c(2n,n-1)(n=0,1,2,…) java版Catalan大数123456789101112131415161718192021import java.io.BufferedInputStream;import java.math.BigInteger;import java.util.*;;public class test &#123; final static int maxn = 5000; static BigInteger[] C = new BigInteger[maxn]; public static void main(String[] args) &#123; Scanner input = new Scanner(new BufferedInputStream(System.in)); int n, k; while(input.hasNextInt()) &#123; n = input.nextInt(); C[0] = BigInteger.valueOf(1); for(k = 1; k &lt;= n; k++) &#123; C[k] = (C[k-1].multiply(BigInteger.valueOf(4*k - 2))).divide(BigInteger.valueOf(k+1)); &#125; System.out.println(C[n]); &#125; &#125;&#125; C语言版Catalan大数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//大卡特兰数// 递推公式 h(n) = ((4*n -2) / (n+1)) *h(n-1)#include &lt;stdio.h&gt;#define ll long longll a[100000000];int main()&#123; int n; while(scanf("%d", &amp;n) != EOF)&#123; a[0] = 1; //h(0) = 1; int d = 1; // 位数 for(int i = 1; i &lt;= n; i++)&#123; //处理乘法 ll num = 0; for(int j = 0; j &lt; d; j++)&#123; num = a[j]*(4*i -2) + num; a[j] = num % 10; num = num / 10; &#125; while(num &gt; 0)&#123; //最后计算完,处理进位 a[d] = num % 10; num = num / 10; d++; &#125; //处理除法 num = 0; for(int j = d - 1; j &gt;= 0; j--)&#123; num = a[j] + num*10; // if( num &lt; (i + 1)) a[j] = 0; else &#123; a[j] = num / (i + 1); num = num % (i + 1); &#125; &#125; while(a[d - 1] == 0 &amp;&amp; d &gt;= 2)&#123; //最后计算完，去除前面的0 d--; &#125; &#125; for(int i = d - 1; i &gt;= 0; i--)&#123; //倒序输出 printf("%lld", a[i]); &#125; printf("\n"); &#125; return 0;&#125;]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫实战——爬取特定微信公众号文章]]></title>
    <url>%2FPython%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E7%88%AC%E5%8F%96%E7%89%B9%E5%AE%9A%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0.html</url>
    <content type="text"><![CDATA[前言学机器学习，学着学着就学偏了，就学着写爬虫了，目的主要是一熟悉下python，二解决自己某需求，这篇文章主要是解决需求写的代码中的主要部分——针对某一微信公众号的爬虫，当然除了学会了简单的爬虫，也运用了之前学习的很多知识，学有所用真的是一件非常开心的事情。 本篇目标 根据微信公众号在Sogou微信搜索，提取微信公众号链接 进入微信公众号界面，获取最近十篇微信文章链接 根据微信文章链接，解析网页，爬取所需文章内容 因为微信的反爬虫措施很多，每篇文章以及微信公众号的链接都是临时链接（从链接中包含timestamp字段可知）所以要另想办法，其中一种方法就是从Sogou微信搜索界面开始进行。 1. 确定URL直接在Sogou微信界面输入要搜索的微信公众号的具体名称，展示的第一条应该就是需要寻找的微信公众号，选取此时的url，例如：（已经输入搜索关键词）1https://weixin.sogou.com/weixin?type=1&amp;s_from=input&amp;query=%E5%B8%96%E6%9C%A8%E8%AE%B0&amp;ie=utf8&amp;_sug_=n&amp;_sug_type_= 2. 提取微信公众号的链接搜狗微信搜索出来的文章链接均为微信的临时链接，通过客户端查看的文章链接均为永久链接，所以只能从搜狗微信搜索界面进入。1234567891011121314151617# 网页有反爬虫机制，这里需要换上浏览器的headersheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'&#125;begin_url = 'https://weixin.sogou.com/weixin?type=1&amp;s_from=input&amp;query=%E5%B8%96%E6%9C%A8%E8%AE%B0&amp;ie=utf8&amp;_sug_=n&amp;_sug_type_='chrome_driver = 'E:\chromedriver_win32\chromedriver.exe'# 需要下载浏览器合适版本的driverbrowser = webdriver.Chrome(executable_path=chrome_driver)browser.get(begin_url)# 通过selenium用chrome模拟点击链接element = browser.find_element_by_link_text("帖木记")element.click()# 获取当前窗口的url，即微信公众号的链接browser.switch_to.window(browser.window_handles[1])now_url = browser.current_url 3. 获取微信文章的链接123456789101112131415161718192021# 获取文章id, 比如最近一天发的文章id为WXAPPMSG1000000013# 这里网页动态加载需要用selenium获取链接信息# 需要获取十篇只需要加个for循环对id_inc递减即可id_inc = 0start_id = 1000000013 + id_inctrue_id = "WXAPPMSG" + str(start_id)element = browser.find_element_by_id(true_id)content = element.get_attribute('innerHTML')# 无法直接获取链接，通过字符串切片获取文章的临时链接# 这种切片方式只获取了每天的第一条图文消息的链接str1 = 'hrefs="'right_url = content.partition(str1)[2]str2 = '"&gt;&lt;/span&gt;'right_url = right_url.partition(str2)[0]right_url = right_url.replace('amp;', '') # 去除多余字符# 前后合并形成一个完整的urlleft_url = 'https://mp.weixin.qq.com'url = left_url + right_urlprint(url) 4. 爬取微信文章的内容临时链接直接在浏览器中浏览不显示阅读数以及点赞数，所以只能获取文章的内容，数据需要想其他办法。 123456789101112131415# 用requests获取网页req = requests.get(url=url, headers=headers)# 用BeautifulSoup解析网页soup = BeautifulSoup(req.text, "html.parser")# 获取文章标题title = soup.h2.string.strip()# 获取文章内容contents = soup.find_all('p') # 等价于soup('p')with open('data.txt', "w+", encoding='utf-8') as fp: fp.write(title) for content in contents: fp.write(content.text + '\n') 5. 小结显然上面的代码显然可以优化的，主要是通过爬取微信文章用到了很多python库及技巧，比如：requests库，bs4库，selenium库和re正则表达式，字符串的处理，也包括验证码的识别，因为pytesseract识别率太低，自己又懒得去训练，也没必要花费选择现成的打码平台，所以就选择绕过验证码识别。当然最开心的是自己的小需求目前可以正常运行。 6. 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from bs4 import BeautifulSoupfrom selenium import webdriverfrom time import sleepimport requests# 网页有反爬虫机制，这里需要换上浏览器的headersheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'&#125;begin_url = 'https://weixin.sogou.com/weixin?type=1&amp;s_from=input&amp;query=%E5%B8%96%E6%9C%A8%E8%AE%B0&amp;ie=utf8&amp;_sug_=n&amp;_sug_type_='chrome_driver = 'E:\chromedriver_win32\chromedriver.exe' # 需要下载浏览器合适版本的driverbrowser = webdriver.Chrome(executable_path=chrome_driver)browser.get(begin_url)# 用selenium用chrome模拟点击链接element = browser.find_element_by_link_text("帖木记")element.click()# 获取当前窗口的url，即微信公众号的临时链接browser.switch_to.window(browser.window_handles[1])now_url = browser.current_urlsleep(1) # 停一秒# 获取文章id, 比如最近一天发的文章id为WXAPPMSG1000000013# 这里网页动态加载需要用selenium获取链接信息id_inc = 0start_id = 1000000013 + id_inctrue_id = "WXAPPMSG" + str(start_id)element = browser.find_element_by_id(true_id)content = element.get_attribute('innerHTML')# 无法直接获取链接，通过字符串切片获取文章的临时链接# 这种切片方式只获取了每天的第一条图文消息的链接str1 = 'hrefs="'right_url = content.partition(str1)[2]str2 = '"&gt;&lt;/span&gt;'right_url = right_url.partition(str2)[0]right_url = right_url.replace('amp;', '') # 去除多余字符# 前后合并形成一个完整的urlleft_url = 'https://mp.weixin.qq.com'url = left_url + right_url# 用requests获取网页req = requests.get(url=url, headers=headers)# 用BeautifulSoup解析网页soup = BeautifulSoup(req.text, "html.parser")# 文章标题获取title = soup.h2.string.strip()# 文章内容获取contents = soup.find_all('p') # 等价于soup('p')with open('data.txt', "w+", encoding='utf-8') as fp: fp.write(title) for content in contents: fp.write(content.text + '\n')]]></content>
      <categories>
        <category>实战</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向量，矩阵求导]]></title>
    <url>%2F%E5%90%91%E9%87%8F%EF%BC%8C%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC.html</url>
    <content type="text"><![CDATA[向量求导有两种布局方式，分子布局和分母布局。分子布局和分母布局的操作结果可以通过转置来切换。分母布局：标量对向量求导，得到的结果跟分母上的向量保持一致。即，如果标量是对列向量求导，得到的导数也是列向量。分子布局：标量对向量求导，得到的结果是分母上的向量的转置。即，如果标量对列向量求导，得到的导数将是行向量。以下以分母布局为例： A=\left[ \begin{array}{cccc}{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\ {a_{21}} & {a_{22}} & {\cdots} & {a_{2 n}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots}\\ {a_{m 1}} & {a_{m 2}} & {\cdots} & {a_{m n}}\end{array}\right]，x=\left[ \begin{array}{c}{x_{1}} \\ {x_{2}} \\ {\vdots} \\ {x_{n}}\end{array}\right] A x=\left[ \begin{array}{c}{a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}} \\ {a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}} \\ {\vdots} \\ {a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}}\end{array}\right]_{m×1} \frac{\partial A x}{\partial x}=\left[ \begin{array}{cccc}{a_{11}} & {a_{21}} & {\ldots} & {a_{m 1}} \\ {a_{12}} & {a_{22}} & {\ldots} & {a_{m 2}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {a_{1 n}} & {a_{2 n}} & {\ldots} & {a_{m n}}\end{array}\right]=A^{T}以分母作为主序，x是列向量，Ax关于x求导数，那么对Ax的分量(每行)分别对x的分量求偏导数，然后整理排成一列，对每个分量都如此操作。 可以得到如下结论： \frac{\partial A x}{x}=A^{T} \frac{\partial A x}{x^{T}}=A \frac{\partial x^{T} A x}{x}=\left(A^{T}+A\right) x \frac{\partial x^{T} y}{x}=y元素对向量，矩阵求导： 元素对行向量求导设y是元素，$\mathbf{x}^{T}=\left[ \begin{array}{lll}{x_{1}} &amp; {\cdots} &amp; {x_{q}}\end{array}\right]$是q维行向量，则$\frac{\partial y}{\partial \mathbf{x}^{r}}=\left[\frac{\partial y}{\partial x_{1}} \ldots \frac{\partial y}{\partial x_{q}}\right]$ 元素对列向量求导设y是元素，$\mathbf{x}=\left[ \begin{array}{c}{x_{1}} \\ {\vdots} \\ {x_{p}}\end{array}\right]$是p维列向量，则$\frac{\partial y}{\partial \mathbf{x}}=\left[ \begin{array}{c}{\frac{\partial y}{\partial x_{1}}} \\ {\vdots} \\ {\frac{\dot{\partial} y}{\partial x_{p}}}\end{array}\right]$ 元素对矩阵求导设y是元素，$X=\left[ \begin{array}{ccc}{x_{11}} &amp; {\cdots} &amp; {x_{1 q}} \\ {\vdots} &amp; &amp; {\vdots} \\ {x_{p 1}} &amp; {\cdots} &amp; {y_{p q}}\end{array}\right]$是p×q矩阵，则$\frac{\partial y}{\partial X}=\left[ \begin{array}{ccc}{\frac{\partial y}{\partial x_{11}}} &amp; {\cdots} &amp; {\frac{\partial y}{\partial x_{1 q}}} \\ {\vdots} \\ {\frac{\partial y}{\partial x_{p 1}}} &amp; {\cdots} &amp; {\frac{\partial y}{\partial x_{p q}}}\end{array}\right]$```]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++梳理补缺]]></title>
    <url>%2FC%26C%2B%2B%E6%A2%B3%E7%90%86%E8%A1%A5%E7%BC%BA.html</url>
    <content type="text"><![CDATA[1. 标准名字空间 &amp; C++标准输入输出流 标准名字空间 cout是标准名字空间std的一个名字。必须加上名字空间限定 std:: std::cout 例如不同班级都可能有同名的学生，如“张伟”计科::张伟 机械::张伟 也可以用：using std::cout using namespace std C++标准输入输出流 cout 是一个标准输出流变量（对象），代表控制台窗口 &lt;&lt;是一个运算符，假如o是一个输出流对象，x是一个数据， o&lt;&lt; x 123456789101112131415#include &lt;iostream&gt;#include &lt;fstream&gt; //文件输入输出 #include &lt;string&gt;using namespace std;int main()&#123; ofstream oF("1.txt"); oF &lt;&lt; 3.14 &lt;&lt;" "&lt;&lt; "hello world\n"; oF.close(); ifstream iF("1.txt"); double d; string str; iF &gt;&gt; d &gt;&gt; str; cout&lt;&lt;d&lt;&lt;" "&lt;&lt; str&lt;&lt;endl; return 0; &#125; 2. 引用变量和引用参数 引用变量 引用变量是其他变量的别名。如同一个人的外号或小名 既然是引用，定义引用变量时就必须指明其引用的是哪个变量int a = 3;int &amp;r = a; 引用变量“从一而终”， 一旦定义就不能再引用其他变量int &amp;r=a;int &amp;r = b; //错 引用变量和被引用的变量类型必须匹配double d;int &amp;r = d; 对引用变量的操作就是对它引用的变量的操作 12345678910#include &lt;iostream&gt;using namespace std;int main()&#123; int a = 3, &amp;r = a; cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; r&lt;&lt; endl; r = 5; cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; r &lt;&lt;endl; return 0;&#125; 函数的值形参 C函数的形参都是值参数，形参作为函数的局部变量有自己单独的内存块，当函数调用时，实参将值拷贝（赋值给）形参。对形参的修改不会影响实参 1234567891011121314151617#include &lt;iostream&gt;using namespace std;void swap(int x, int y)&#123; cout &lt;&lt; x &lt;&lt; '\t' &lt;&lt; y&lt;&lt; endl; int t = x; x = y; y = t; cout &lt;&lt; x &lt;&lt; '\t' &lt;&lt; y&lt;&lt; endl;&#125;int main()&#123; int a = 3, b = 4; cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b&lt;&lt; endl; swap(a,b); cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b &lt;&lt;endl; return 0; &#125; 函数的值形参：传递指针 12345678910111213141516#include &lt;iostream&gt;using namespace std;void swap(int *x, int *y)&#123; //指针 x, y cout &lt;&lt; x &lt;&lt; '\t' &lt;&lt; y&lt;&lt; endl; //输出的是a，b的地址 int t = *x; // 取x指针指向的值赋值给t *x = *y; // 同理 *y = t; // 同理 &#125;int main()&#123; int a = 3, b = 4; cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b&lt;&lt; endl; swap(&amp;a,&amp;b); // &amp;取地址 a, b地址 传给x，y cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b &lt;&lt;endl; return 0; &#125; 函数的引用形参：引用实参 12345678910111213141516#include &lt;iostream&gt;using namespace std;void swap(int &amp;x, int &amp;y)&#123; // &amp;引用变量 cout &lt;&lt; x &lt;&lt; '\t' &lt;&lt; y&lt;&lt; endl; int t = x; x = y; y = t; &#125;int main()&#123; int a = 3, b = 4; cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b&lt;&lt; endl; swap(a,b); cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b &lt;&lt;endl; return 0; &#125; 3. 函数的默认形参、函数重载 函数的形参可以有默认值void print(char ch, int n = 1) 默认形参必须在非默认形参右边，即一律靠右add(x = 1, y, z = 3); // 错add(y, x = 1, z = 3); // OK 1234567891011#include &lt;iostream&gt;using namespace std;void print(char ch, int n = 1) &#123; for (int i = 0; i &lt; n; i++) cout &lt;&lt; ch;&#125;int main() &#123; print('*'); cout &lt;&lt; endl; print('*',3); cout &lt;&lt; endl; print('*',5); cout &lt;&lt; endl;&#125; 12345678910#include &lt;iostream&gt;using namespace std;int add(int x,int y=2,int z=3) &#123; return x + y + z;&#125;int main() &#123; cout &lt;&lt; add(5)&lt;&lt;endl; cout &lt;&lt; add(5,7) &lt;&lt; endl; cout &lt;&lt; add(5,7,9) &lt;&lt; endl;&#125; 函数重载 C++允许同一作用域里有同名的函数，只要他们的形参不同。如：int add(int x, int y);double add(double x, double y); 函数名和形参列表构造了函数的签名 函数重载不能根据返回类型区分函数。如int add(int x, int y);double add(int x, int y); 12345678910111213#include &lt;iostream&gt;using namespace std;int add(int x, int y = 2) &#123; return x + y ;&#125;double add(double x, double y = 2.0) &#123; return x + y;&#125;int main() &#123; cout &lt;&lt; add(5,3) &lt;&lt; endl; cout &lt;&lt; add(5.3, 7.8) &lt;&lt; endl; cout &lt;&lt; add((double)5, 7.8) &lt;&lt; endl;//歧义性加double强制转换 &#125; 4. 函数模板 函数模板 通用算法：函数模板。也称为泛型算法 用template关键字增加一个模板头，将数据类型变成类型模板参数 1234template&lt;typename T&gt; T add(T x, T y) &#123; return x + y; &#125; 给模板参数传递实际的模板参数 12cout &lt;&lt; add&lt;int&gt;(5, 3) &lt;&lt; endl;cout &lt;&lt; add&lt;double&gt;(5.3, 7.8) &lt;&lt; endl; 函数模板实例化 123456789101112131415161718#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;template&lt;typename T&gt; T add(T x, T y) &#123; return x + y;&#125;int main() &#123;#if 1 cout &lt;&lt; add&lt;int&gt;(5, 3) &lt;&lt; endl; cout &lt;&lt; add&lt;double&gt;(5.3, 7.8) &lt;&lt; endl; cout &lt;&lt; add&lt;int&gt;(4, 6) &lt;&lt; endl; cout &lt;&lt; add&lt;string&gt;("hello", "world") &lt;&lt; endl;#else cout &lt;&lt; add(5, 3) &lt;&lt; endl; cout &lt;&lt; add(5.3, 7.8) &lt;&lt; endl;#endif&#125; 模板参数自动推断 5. 用户自定义类型string和vector string 是一个用户定义类型，表示的是符串。string s = “hello”, s2(“world”); 用成员访问运算符.访问string类的成员12cout &lt;&lt; s.size() &lt;&lt; endl;string s3 = s.substr(1, 3); 内在的数组（静态数组） 123456789#include &lt;iostream&gt;using std::cout;int main() &#123;int arr[] = &#123; 10,20,30,40 &#125;; //大小固定，以后不能添加更多int值for (int i = 0; i &lt; 4; i++) &#123; cout &lt;&lt; arr[i] &lt;&lt; '\t'; cout &lt;&lt; '\n';&#125;#endif vector 向量，类似于数组，但可以动态增长。头文件 &lt;vector&gt; 是一个类模板，实例化产生一个类，如vector&lt;int&gt; 产生一个数据元素是int的vector&lt;int&gt; 类（向量）。 同样，可以通过vector&lt;int&gt; 类对象去访问其他成员，如成员函数。 同样可以用运算符进行一些运算。 6. 指针和动态内存分配 指针 指针就是地址，变量的指针就是变量的地址。可以用 取值地址符&amp; 获得一个变量的地址。如： &amp;var 指针变量就是存储指针（地址）的变量。如：T *p; //p是存储 “T类型变量的地址”的变量 通过取内容运算符 *可以得到一个指针变量指向的变量。*p就是p指向的那个变量 1234567891011121314151617181920/*指针就是地址，变量的指针就是变量的地址指针变量就是存储指针（地址）的变量*/#include &lt;iostream&gt;using namespace std;int main() &#123; int a=3; int *p = &amp;a; //取地址运算符&amp;用于获得a的地址：&amp;a cout &lt;&lt; p &lt;&lt; '\t' &lt;&lt; &amp;a &lt;&lt; endl; //取内容运算符*用于获得指针指向的变量(内存块) cout &lt;&lt; *p &lt;&lt; '\t' &lt;&lt; a &lt;&lt; endl; //*p就是a *p = 5; //即a = 5; cout &lt;&lt; *p &lt;&lt; '\t' &lt;&lt; a &lt;&lt; endl;#if 0 int *q = p; //q和p值相同，都是a的地址(指针) cout &lt;&lt; *p &lt;&lt; '\t' &lt;&lt; *q &lt;&lt; '\t' &lt;&lt; a &lt;&lt; endl; char *s = &amp;a; //int *#endif&#125; 1234567891011121314/*用指针访问数组元素*/#include &lt;iostream&gt;using namespace std;int main() &#123; int arr[] = &#123; 10,20,30,40 &#125;; int *p = arr;//数组名就是数组第一个元素的地址，即arr等于&amp;(arr[0]) // p[i]就是*(p+i) cout &lt;&lt; *(p + 2) &lt;&lt; '\t' &lt;&lt; p[2] &lt;&lt; '\t' &lt;&lt; arr[2] &lt;&lt; endl; for (int *q = p + 4; p &lt; q; p++) cout &lt;&lt; *p &lt;&lt; '\t'; cout &lt;&lt; '\n';&#125; 动态内存分配 123456789#include &lt;cstdio&gt;#include &lt;malloc.h&gt;int main()&#123; char *s = (char *)malloc(10*sizeof(s)); // char *s 指针， （char *）指针类型， sizeof s用内存大小 s = "hello"; puts(s); return 0; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344/*malloc free realloc 动态内存分配：new用于申请内存块、delete用于释放内存块 T *p = new T; delete p; T *q = new T[5]; delete[] q;*/#if 1 // 堆存储区#include &lt;iostream&gt;using namespace std;int main() &#123; int *p = new int; //malloc *p = 3; cout &lt;&lt; p &lt;&lt; '\t' &lt;&lt; *p &lt;&lt; endl; delete p; //如果没有这行，内存泄漏 p = new int; *p = 5; cout &lt;&lt; p &lt;&lt; '\t' &lt;&lt; *p &lt;&lt; endl; delete p;&#125;#else#include &lt;iostream&gt;using namespace std;int main() &#123; int n = 4; int *p = new int[n]; for (int i = 0; i &lt; n; i++) p[i] = 2 * i + 1; for (int *q = p + n; p &lt; q; p++) cout &lt;&lt; *p &lt;&lt; '\t'; cout &lt;&lt; '\n'; char *s = (char *)p; char ch = 'A'; int n2 = n * sizeof(int) / sizeof(char); for (int i = 0; i &lt; n2; i++) s[i] = ch + i; for (char *r = s+n2; s &lt; r; s++) cout &lt;&lt; *s; cout &lt;&lt; '\n'; delete[] p;&#125;#endif 7. 类和对象 面向对象编程 传统的过程式编程：变量（对象）就是一些存储数据的内存块，而过程（函数）对这些数据进行处理。 面向对象编程：程序是由不同种类的许多对象相互协作完成的。对象之间通过发送/接收消息来协作完成各种任务。由这些对象构成的程序也称为“对象式系统”。 面向对象设计 人驾驶车：设计对象“人”和“车” 从具有共同特征的许多抽象出某种概念，如“人”，“车” 某些概念之间可能存在某种关系 组合（包含）关系 继承和派生关系 不同思考方式：面向对象编程 vs 过程式编程 过程式编程：用内在类型（概念）如int、double表示数据，用面向这些机器类型的概念去解决复杂问题，不易于思考问题 面向对象编程：用现实世界中的概念（人、车、地图）来思考问题。更自然、更易于理解、易于查错、易于组装（组件式开发） C++的面向对象特性：用户定义类型 程序员定义自己的“用户定义类型” 如类类型，来表示各种应用问题中的各种概念 C++ 标准库已经提供了很多实用的“用户定义类型”， 是C++标准库的程序员实现的 cout是一个ostream类的对象（变量），cin是一个istream的对象（变量）。可以向它们发送消息：cout &lt;&lt; “hello world”; string是一个表示字符串的类。向一个string对象发送一个size()消息，查询该对象包含的字符数目string str = “hello world”; cout &lt;&lt; str.size(); 一个用户定义类型包括： 有哪些属性？ 有哪些操作（运算）？ 不同属性或操作的访问权限？哪些是（类）外部可以访问？哪些是仅仅内部才能访问的？ 面向对象设计要考虑多个用户定义类型的关系 不同类型的对象之间是继承还是包含关系？ 程序：哪些具体对象如何进行交互协作 类和对象 用struct或class关键字定义一个类。定义的类就是一个数据类型。struct student{ string name; double score;} ; 类类型的变量通常称为对象。如：student stu; 对象就是类的一个实例 成员访问运算符 访问类对象的成员stu.name = “LiPing”;stu.score = 78.5; 对象数组 和内在一样，可以定义类类型的数组。存储一组类对象。 类类型的指针变量 T是一个类类型，则T *就是T指针类型。如int *是int指针类型。 T *变量可以指向一个类对象。student stu;student *p = &stu;student students[3];p = students + 2; 间接访问运算符-&gt;、取内容运算符*student stu;student p = &stu;(\p).name = “LiPing”; //*p就是p指向的变量stup-&gt;score = 78; // p指向的类对象的成员score 指向可以指向动态分配的对象 类的成员函数 类体外定义成员函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344/*输入一组学生成绩(姓名和分数)，输出：平均成绩、最高分和最低分。当然，也要能输出所有学生信息*/#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;vector&gt;using namespace std;struct student&#123; string name; double score; void print();&#125;;void student::print() &#123; cout &lt;&lt; name &lt;&lt; " " &lt;&lt; score &lt;&lt; endl;&#125;int main() &#123;#if 0 student stu; stu.name = "Li Ping"; stu.score = 78.5; stu.print();#endif vector&lt;student&gt; students; while (1) &#123; student stu; cout &lt;&lt; "请输入姓名 分数:\n"; cin &gt;&gt; stu.name &gt;&gt; stu.score; if (stu.score &lt; 0) break; students.push_back(stu); &#125; for (int i = 0; i &lt; students.size(); i++) students[i].print(); double min = 100, max=0, average = 0; for (int i = 0; i &lt; students.size(); i++) &#123; if (students[i].score &lt; min) min = students[i].score; if (students[i].score &gt; max) max = students[i].score; average += students[i].score; &#125; average /= students.size(); cout &lt;&lt; "平均分、最高分、最低分："&lt;&lt; average &lt;&lt; " " &lt;&lt; max &lt;&lt; " " &lt;&lt; min &lt;&lt; endl;&#125; 8. this指针， 访问控制，构造函数 this指针 12345678910111213141516171819/*this指针: 成员函数实际上隐含一个this指针。*/#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;struct student &#123; string name; double score; void print() &#123; cout &lt;&lt; this-&gt;name &lt;&lt; " " &lt;&lt; this-&gt;score &lt;&lt; endl; &#125;&#125;;int main() &#123; student stu; stu.name = "Li Ping"; stu.score = 78.5; stu.print(); // print(&amp;stu);&#125; 访问控制 123456789101112131415161718192021222324252627282930313233/*struct和class区别：struct里的成员默认是public(公开的)class里的成员默认是private(私有的)*/#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class student&#123;public: //接口 void print() &#123; cout &lt;&lt; this-&gt;name &lt;&lt; " " &lt;&lt; this-&gt;score &lt;&lt; endl;&#125; string get_name() &#123; return name; &#125; double get_score() &#123; return score; &#125; void set_name(string n) &#123; name = n; &#125; void set_score(double s) &#123; score = s; &#125;private: string name; double score;&#125;;int main() &#123; student stu;// stu.name = "Li Ping";// stu.score = 78.5; stu.set_name("Li Ping"); stu.set_score(78.5); stu.print(); // print(&amp;stu); cout &lt;&lt; stu.get_name() &lt;&lt; " " &lt;&lt; stu.get_score() &lt;&lt; endl;&#125; 构造函数 123456789101112131415161718192021222324252627282930/*构造函数： 函数名和类名相同且无返回类型的成员函数。*/#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class student&#123; string name; double score;public: // student()&#123;&#125; // 默认构造函数 student()&#123; name = "NO"; score = 0; cout &lt;&lt; "构造函数\n"; &#125; student(string n,double s)&#123; //不是默认构造函数 name = n; score = s; cout &lt;&lt; "构造函数\n"; &#125; void print() &#123; cout &lt;&lt; this-&gt;name &lt;&lt; " " &lt;&lt; this-&gt;score &lt;&lt; endl; &#125;&#125;;int main() &#123; //student stu;//在创建一个类对象时会自动调用称为“构造函数”的成员函数 student stu("LiPing",80.5); stu.print(); //student students[3]; 数组必须有默认构造函数 &#125; 9. 运算重载符12345678910111213141516171819202122232425262728293031/*运算符重载：针对用户定义类型重新定义运算符函数*/#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class student &#123; string name; double score;public: student(string n, double s) &#123; name = n; score = s; &#125; //友元函数 friend ostream&amp; operator&lt;&lt;(ostream &amp;o, student s); friend istream&amp; operator&gt;&gt;(istream &amp;in, student &amp;s);&#125;;ostream&amp; operator&lt;&lt;(ostream &amp;o, student s) &#123; cout &lt;&lt; s.name &lt;&lt; "," &lt;&lt; s.score &lt;&lt; endl; return o; &#125;istream&amp; operator&gt;&gt;(istream &amp;in, student &amp;s) &#123; in &gt;&gt; s.name &gt;&gt; s.score; return in;&#125;int main() &#123; student stu("LiPing", 80.5); cin &gt;&gt; stu; //operator&gt;&gt;(cin,stu) cout &lt;&lt; stu; //operator&lt;&lt;(cout,stu)&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class Point&#123; double x, y;public: double operator[](int i) const&#123; //const函数 if (i == 0) return x; else if (i == 1) return y; else throw "下标非法!"; //抛出异常 &#125; double&amp; operator[](int i) &#123; if (i == 0) return x; else if (i == 1) return y; else throw "下标非法!"; //抛出异常 &#125; Point(double x_,double y_) &#123; x = x_; y = y_; &#125; Point operator+(const Point q) &#123; return Point(this-&gt;x+q[0],this-&gt;y + q[1]); &#125; //友元函数 friend ostream &amp; operator&lt;&lt;(ostream &amp;o, Point p); friend istream &amp; operator&gt;&gt;(istream &amp;i, Point &amp;p);&#125;;ostream &amp; operator&lt;&lt;(ostream &amp;o, Point p) &#123; o &lt;&lt;p.x &lt;&lt; " " &lt;&lt; p.y&lt;&lt; endl; return o;&#125;istream &amp; operator&gt;&gt;(istream &amp;i, Point &amp;p) &#123; i &gt;&gt; p.x &gt;&gt; p.y; return i;&#125;#if 0Point operator+(const Point p,const Point q) &#123; return Point(p[0] + q[0], p[1] + q[1]);&#125;#endifint main() &#123; Point p(3.5, 4.8),q(2.0,3.0);#if 0// cin &gt;&gt; p; cout &lt;&lt; p; cout &lt;&lt; p[0] &lt;&lt; "-" &lt;&lt; p[1] &lt;&lt; endl; //p.operator[](0) p[0] = 3.45; p[1] = 5.67; cout &lt;&lt; p;#endif cout &lt;&lt; p&lt;&lt;q; Point s = p + q; //p.operator+(q) vs operator+(p,q) cout &lt;&lt; s;&#125; 10. String类、拷贝构造函数、析构函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include &lt;iostream&gt;using namespace std;class String &#123; char *data; //C风格的字符串 int n;public: ~String() &#123; //析构函数，析构的顺序与创建的相反顺序 cout &lt;&lt;n&lt;&lt; " 析构函数\n"; if(data) delete[] data; &#125;#if 1 String(const String &amp;s) &#123; //硬拷贝 cout &lt;&lt; "拷贝构造函数\n"; data = new char[s.n + 1]; n = s.n; for (int i = 0; i &lt; n; i++) data[i] = s.data[i]; data[n] = '\0'; &#125;#endif String(const char *s=0) &#123; //构造函数 cout &lt;&lt; "构造函数\n"; if (s == 0) &#123; cout &lt;&lt; "s==0\n"; data = 0; n = 0; return; &#125; const char *p = s; while (*p) p++; n = p - s; //String长 data = new char[n + 1]; //分配内存空间 for (int i = 0; i &lt; n; i++)//存储String data[i] = s[i]; data[n] = '\0'; &#125; int size() &#123; return n; &#125; char operator[](int i)const &#123; //下标运算符，不可以修改 if (i&lt;0 || i&gt;=n ) throw "下标非法"; return data[i]; &#125; char&amp; operator[](int i) &#123; //下标运算符，可以修改 if (i &lt; 0 || i &gt;= n) throw "下标非法"; return data[i]; &#125;&#125;;ostream &amp; operator&lt;&lt;(ostream &amp;o, String s) &#123; for (int i = 0; i &lt; s.size(); i++) cout &lt;&lt; s[i]; return o;&#125;void f() &#123; String str,str2("hello world"); str2[1] = 'E';// cout &lt;&lt; str2 &lt;&lt; endl;#if 1 String s3 = str2; //拷贝构造函数 cout &lt;&lt; s3 &lt;&lt; endl; s3[3] = 'L'; cout &lt;&lt; s3 &lt;&lt; endl; cout &lt;&lt; str2 &lt;&lt; endl;#endif&#125;int main() &#123; f();&#125; 11. 类模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/*类模拟vector&lt;int&gt;的类Vector*/#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class student &#123; string name; double score;public: student(string n="no", double s=0) &#123; //申请了空间必须要默认 name = n; score = s; &#125; friend ostream&amp; operator&lt;&lt;(ostream &amp;o, student s);&#125;;ostream&amp; operator&lt;&lt;(ostream &amp;o, student s) &#123; cout &lt;&lt; s.name &lt;&lt; "," &lt;&lt; s.score &lt;&lt; endl; return o;&#125;//类模板template&lt;typename T&gt; //加这一句，改数据类型 class Vector &#123; T *data; int capacity; int n;public: Vector(int cap=3) &#123; data = new T[cap]; if (data == 0) &#123; cap = 0; n = 0; return; &#125; capacity = cap; n = 0; &#125; void push_back(T e) &#123; if (n == capacity) &#123;//空间已经满 cout &lt;&lt; "增加容量！\n"; T *p = new T[2 * capacity]; if (p) &#123; for (int i = 0; i &lt; n; i++) p[i] = data[i]; delete[] data; data = p; capacity = 2*capacity; &#125; else &#123; return; &#125; &#125; data[n] = e; n++; &#125; T operator[](int i) const&#123; if (i &lt; 0 || i &gt;= n) throw "下标非法!"; return data[i]; &#125; int size() &#123; return n; &#125;&#125;;int main() &#123; Vector&lt;student&gt; v; v.push_back(student("Li",45.7)); v.push_back(student("Wang", 45.7)); v.push_back(student("zhao", 45.7)); for (int i = 0; i &lt; v.size(); i++) cout &lt;&lt; v[i] ; cout &lt;&lt; endl; v.push_back(student("zhang", 45.7)); v.push_back(student("Liu", 45.7)); for (int i = 0; i &lt; v.size(); i++) cout &lt;&lt; v[i]; cout &lt;&lt; endl;#if 0#if 1 Vector&lt;int&gt; v; v.push_back(3); v.push_back(4); v.push_back(5); for(int i = 0 ; i&lt;v.size();i++) cout&lt;&lt;v[i]&lt;&lt;'\t'; cout &lt;&lt; endl; v.push_back(6); v.push_back(7); for (int i = 0; i &lt; v.size(); i++) cout &lt;&lt; v[i] &lt;&lt; '\t'; cout &lt;&lt; endl;#else Vector&lt;string&gt; v; v.push_back("hello"); v.push_back("world"); v.push_back("sdfasdf"); for (int i = 0; i &lt; v.size(); i++) cout &lt;&lt; v[i] &lt;&lt; '\t'; cout &lt;&lt; endl; v.push_back("ggg"); v.push_back("hhh"); for (int i = 0; i &lt; v.size(); i++) cout &lt;&lt; v[i] &lt;&lt; '\t'; cout &lt;&lt; endl;#endif#endif&#125; 学习来源：b站：从C到C++快速入门(2019版)]]></content>
      <tags>
        <tag>C/C++</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习的数学基础]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html</url>
    <content type="text"><![CDATA[预备了一下学习机器学习需要的数学基础知识，补充了一些不清楚的数学知识 概览 算法或理论 用到的数学知识 贝叶斯分类器 随机变量，贝叶斯公式，随机变量独立性，正太分布，最大似然估计 决策树 概率，熵，Gini系数 KNN算法 距离函数 主成分分析 协方差矩阵，散布矩阵，拉格朗日乘数法，特征值与特征向量 流形学习 流形，最优化，测地线，测地距离，图，特征值与特征向量 线性判别分析 散度矩阵，逆矩阵，拉格朗日乘数法，特征值与特征向量 支持向量机 点到平面的距离，Slater条件，强对偶，拉格朗日对偶，KKT条件，凸优化，核函数，Mercer条件 ]logistics回归 概率、随机变量，最大似然估计，梯度下降法，凸优化，牛顿法 随机森林 抽样，方差 AdaBoost算法 概率，随机变量，最大似然估计，梯度下降法，凸优化，牛顿法 隐马尔科夫链 概率，离散型随机变量，条件概率，随机变量独立性，拉格朗日乘数法，最大似然估计 条件随机场 条件概率，数学期望，最大似然估计 高斯混合模型 正态分布，最大似然估计，Jensen不等式 人工神经网络 梯度下降法，链式法则 卷积神经网络 梯度下降法，链式法则 循环神经网络 梯度下降法，链式法则 生成对抗网络 梯度下降法，链式法则，极值定理，Kullback-Leibler散度，Jensen-Shannon散度，测地距离，条件分布，互信息 K-means算法 距离函数 强化学习 数学期望，贝尔曼方程 贝叶斯网络 条件概率，贝叶斯公式，图 VC维 Hoeffding不等式 微积分 导数与求导公式 一阶导数与函数的单调性 一元函数极值判定法则 高阶导数 二阶导数与函数的凹凸性 一元函数泰勒展开 偏导数 梯度$\nabla f(\mathrm{x})=\left(\frac{\partial f}{\partial x_{1}}, \ldots, \frac{\partial f}{\partial x_{n}}\right)^{\mathrm{T}}$ 雅可比矩阵多元函数的一阶偏导数组成$\left[ \begin{array}{cccc}{\frac{\partial y_{1}}{\partial x_{1}}} &amp; {\frac{\partial y_{1}}{\partial x_{2}}} &amp; {\dots} &amp; {\frac{\partial y_{1}}{\partial x_{n}}} \\ {\frac{\partial y_{2}}{\partial x_{1}}} &amp; {\frac{\partial y_{2}}{\partial x_{2}}} &amp; {\dots} &amp; {\frac{\partial y_{2}}{\partial x_{n}}} \\ {\cdots} &amp; {\cdots} &amp; {\cdots} &amp; {\cdots} \\ {\frac{\partial y_{m}}{\partial x_{1}}} &amp; {\frac{\partial y_{m}}{\partial x_{2}}} &amp; {\ldots} &amp; {\frac{\partial y_{m}}{\partial x_{n}}}\end{array}\right]$ Hessian矩阵多元函数的二阶导数组成$\left[ \begin{array}{cccc}{\frac{\partial^{2} f}{\partial x_{1}^{2}}} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} &amp; {\dots} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{2}^{2}}} &amp; {\dots} &amp; {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}}\\ {\cdots} &amp; {\cdots} &amp; {\cdots} &amp; {\cdots}\\ {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{n} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n}^{2}}}\end{array}\right]$ 多元函数泰勒展开$f(\mathrm{x})=f\left(\mathrm{x}_{0}\right)+\left(\nabla f\left(\mathrm{x}_{0}\right)\right)^{\mathrm{T}}\left(\mathrm{x}-\mathrm{x}_{0}\right)+\frac{1}{2}\left(\mathrm{x}-\mathrm{x}_{0}\right)^{\mathrm{T}} \mathrm{H}\left(\mathrm{x}-\mathrm{x}_{0}\right)+o\left(\left\Vert \mathrm{x}-\mathrm{x}_{0}\right\Vert^{2}\right)$ 多元函数极值判断法则 如果Hessian矩阵正定，函数在该点有极小值 如果Hessian矩阵负定，函数在该点有极大值 如果Hessian矩阵不定，还需要看更高阶的导数 线性代数 向量及其运算 向量的范数$\Vert\mathrm{x}\Vert_{p}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}$$\Vert\mathrm{x}\Vert_{1}=\sum_{i=1}^{n}\left|x_{i}\right|$$\Vert\mathrm{x}\Vert_{2}=\sqrt{\sum_{i=1}^{n}\left(x_{i}\right)^{2}}$ 矩阵及其运算 张量 行列式 $|\mathrm{A}|=\sum_{\sigma \in S_{n}} \operatorname{sgn}(\sigma) \prod_{i=1}^{n} a_{i, \sigma(i)}$ 逆序数 二次型 特征值与特征向量 奇异值分解（SVD）$\mathrm{A}=\mathrm{U} \Sigma \mathrm{V}^{\mathrm{T}}$U：$AA^{T}$ 正交矩阵 mxmV：$A^{T}A$ 正交矩阵 nxn$\Sigma$ 对角阵 mxn 常用的矩阵与向量求导公式$\nabla \mathrm{w}^{\mathrm{T}} \mathrm{x}=\mathrm{w}$$\nabla \mathrm{x}^{\mathrm{T}} \mathrm{Ax}=\left(\mathrm{A}+\mathrm{A}^{\mathrm{T}}\right) \mathrm{x}$$\nabla^{2} \mathrm{x}^{\mathrm{T}} \mathrm{Ax}=\mathrm{A}+\mathrm{A}^{\mathrm{T}}$ （$\nabla^{2}$ 相当于H ） 概率论 随机事件与概率 条件概率与贝叶斯公式 随机变量 数学期望与方差 常用概率分布 随机向量 协方差与协方差矩阵协方差反应随机变量线性相关的程度 $\operatorname{cov}\left(x_{1}, x_{2}\right)=\mathrm{E}\left(\left(x_{1}-\mathrm{E}\left(x_{1}\right)\right)\left(x_{2}-\mathrm{E}\left(x_{2}\right)\right)\right)$ $\operatorname{cov}\left(x_{1}, x_{2}\right)=\mathrm{E}\left(x_{1} x_{2}\right)-\mathrm{E}\left(x_{1}\right) \mathrm{E}\left(x_{2}\right)$ 最大似然估计（MLE）用来估计概率密度的参数 最优化方法 最优化的基本概念 最优化问题 目标函数 优化变量 可行域 等式约束 不等式约束 局部最小值 全局最小值 迭代法 $\lim _{k \rightarrow+\infty} \nabla f\left(\mathrm{x}_{k}\right)=0$ $\mathrm{x}_{k+1}=h\left(\mathrm{x}_{k}\right)$ 梯度下降法$\mathrm{x}_{k+1}=\mathrm{x}_{k}-\gamma \nabla f\left(\mathrm{x}_{k}\right)$ 牛顿法$\mathrm{x}_{k+1}=\mathrm{x}_{k}-\mathrm{H}_{k}^{-1} \mathrm{g}_{k}$ 坐标下降法$\min f(x), x=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$$\min _{x_{i}} f(x)$ 优化算法面临的问题 局部极值点问题 鞍点问题（H不定） 拉格朗日乘数法 凸优化简介 凸集$\theta \mathrm{x}+(1-\theta) \mathrm{y} \in C$ 凸函数 凸函数定义$f(\theta \mathrm{x}+(1-\theta) \mathrm{y})&lt;\theta f(\mathrm{x})+(1-\theta) f(\mathrm{y})$ 一阶判别法 二阶判别法（H半正定，正定严格凸函数） 凸优化的性质 局部最优解一定是全局最优解$\mathrm{z}=\theta \mathrm{y}+(1-\theta) \mathrm{x} \quad \theta=\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}}$$\begin{aligned}\Vert\mathrm{x}-\mathrm{z}\Vert_{2} &amp;=\Vert \mathrm{x}-\left(\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}} \mathrm{y}+\left(1-\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}}\right) \mathrm{x}\left\Vert_{2}\right.\right.\\ &amp;=\left\Vert\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}}(\mathrm{x}-\mathrm{y})\right\Vert_{2} \\ &amp;=\frac{\delta}{2} \leq \delta \end{aligned}$$f(z)=f(\theta y+(1-\theta) x) \leq \theta f(y)+(1-\theta) f(x)&lt;f(x)$ 拉格朗日对偶 $\min f(\mathrm{x})$$\mathrm{g}_{i}(\mathrm{x}) \leq 0 \quad \mathrm{i}=1, \ldots, m$$h_{i}(\mathrm{x})=0 \quad \mathrm{i}=1, \ldots, p$ $L(\mathrm{x}, \lambda, v)=f(\mathrm{x})+\sum_{i=1}^{m} \lambda_{1} g_{i}(\mathrm{x})+\sum_{i=1}^{p} v_{i} h_{i}(\mathrm{x})$ 原问题$\begin{aligned} p^{ \ast } &amp;=\min _{x} \max _{\lambda, v, \lambda_{i} \geq 0} L(\mathrm{x}, \lambda, v) &amp; \min _{\mathrm{x}} \theta_{P}(\mathrm{x})=\min _{\mathrm{x}} \max _{\lambda, v, \lambda_{i} \geq 0} L(\mathrm{x}, \lambda, v) \\ &amp;=\min _{\mathrm{x}} \theta_{P}(x) \end{aligned}$ 对偶问题$d^{ \ast }=\max _{\lambda, v, \lambda_{i} \geq 0} \min _{x} L(\mathrm{x}, \lambda, v)=\max _{\lambda, v, \lambda_{i} \geq 0} \theta_{D}(\lambda, v)$ 弱对偶问题$d^{ \ast }=\max _{\lambda, v, \lambda_{i} \geq 0} \min _{x} L(\mathrm{x}, \lambda, v) \leq \min _{x} \max _{\lambda, v, \lambda_{i} \geq 0} L(\mathrm{x}, \lambda, v)=p^{ \ast }$ 强对偶Slater条件 凸函数 不等式严格成立（不取等号） KKT条件$\min f(\mathrm{x})$$g_{i}(\mathrm{x}) \leq 0 \quad \mathrm{i}=1, \ldots, q$$h_{i}(\mathrm{x})=0 \quad \mathrm{i}=1, \ldots, p$ $L(\mathrm{x}, \lambda, \mu)=f(\mathrm{x})+\sum_{j=1}^{p} \lambda_{i} h_{j}(\mathrm{x})+\sum_{k=1}^{q} \mu_{i} g_{k}(\mathrm{x})$ $\nabla_{\mathrm{x}} L\left(\mathrm{x}^{\ast}\right)=0$$\mu_{k} \geq 0$$\mu_{k} g_{k}\left(\mathrm{x}^{\ast}\right)=0$$h_{j}\left(\mathrm{x}^{\ast}\right)=0$$g_{k}\left(\mathrm{x}^{\ast}\right) \leq 0$]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>基础</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周 Logistic回归、SVM]]></title>
    <url>%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%20Logistic%E5%9B%9E%E5%BD%92%E3%80%81SVM.html</url>
    <content type="text"><![CDATA[1、Logistic回归基本原理 分类 给定训练数据$D =\{\mathbf x_i, y_i\}^N_{i=1}$，分类任务学习一个从输入x到输出y的映射f ：$\hat y = f(\mathbf x) = \underset{c}{arg\ max}\ p(y = c \mid \mathbf x, D)$ 其中y为离散值，其取值范围称为标签空间:$Y =\{1,2,…,C\}$ 当C=2时，为两类分类问题，计算出$p(y = 1 \mid \mathbf x)$即可。此时分布为Bernoulli分布: p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))其中$\mu (\mathbf x) = \mathbb{E}(y \mid \mathbf x) = p(y = 1 \mid \mathbf x)$ Recall:Bernouili分布 Bernoulli分布又名两点分布或者0-1分布。若Bernoulli试验成功，则Bernoulli随机变量X取值为1，否则X为0。记试验成功概率为θ， 我们称X服从参数为θ的Bernoulli分布，记为: 𝑋~𝐵𝑒𝑟(θ), 概率函数（pmf）为：p(x) = \theta ^x(1- \theta)^{1-x} = \begin{cases} \theta & if\ x = 1\\ 1 - \theta & if\ x = 0 \end{cases} Bernoulli分布的均值：$\mu = \theta $ 方差：$\sigma^2 = \theta \times (1-\theta)$ Logistic回归模型 Logistic回归模型同线性回归模型类似，也是一个线性模型，只是条件概率𝑝(𝑦|𝐱)的形式不同：p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))\mu (\mathbf x) = \sigma(\mathbf w^T\mathbf x) 其中sigmoid函数（S形函数）定义为\sigma(a) = \frac{1}{1+exp(-a)} = \frac{exp(-a)}{exp(-a)+1} 亦被称为logistic函数或logit函数，将实数a变换到[0,1]区间。 因为概率取值在[0,1]区间 Logistic回归亦被称为logit回归 为什么用logistic函数？ 在神经科学中 神经元对其输入进行加权和：$f(x) = w^Tx$ 如果该和大于某阈值 $f(x) &gt; \tau $，神经元发放脉冲 在Logistic回归，定义Log Odds Ratio:\begin{eqnarray} LOR(x) &=& log \frac {p(y=1 \mid \mathbf x, \mathbf f w)}{p(y = 0 \mid \mathbf x, \mathbf w)} &=& log [\frac{1}{1+exp(-\mathbf w^T\mathbf x)} \frac {1+exp(-\mathbf w^T\mathbf x)}{exp(-\mathbf w^T\mathbf x)}] \\ &=& log [exp(\mathbf w^T \mathbf x)]\\ &=& \mathbf w^T\mathbf x \end{eqnarray} 因此，$iff LOR(\mathbf x) = \mathbf w^T \mathbf x &gt; 0$神经元发放脉冲，即$p(y=1 \mid \mathbf x, \mathbf w) &gt; p(y=0 \mid \mathbf x, \mathbf w)$ 线性决策函数 在Logistic回归中 $LOR(\bf x) = w^Tx &gt; 0, \hat y = 1$ $LOR(\bf x) = w^Tx &lt;0, \hat y = 0$ $\bf w^T \bf x = 0$:决策面 因此$a(\bf x) = w^Tx$分类决策面 因此Logistic回归是一个线性分类器 极大似然估计 Logistic回归：$p(y \mid \mathbf {x,w}) = Ber(y \mid \mu (x)), \mu (\mathbf x) = \sigma (\mathbf w^T\mathbf x)$ 令$\mu_i = \mu(\mathbf x_i)$，则负log似然为$\begin{eqnarray} J(w) = NLL(\mathbf w) &amp;=&amp; - \sum_{i=1}^N log \left[(\mu_i)^{y_i} \times (1-\mu_i)^{1-y_i}\right]\\ &amp;=&amp; \sum_{i=1}^N- \left[y_i log(\mu_i)+(1-y_i)log(1-u_i) \right] \end{eqnarray}$ 极大似然估计等价于最小Logistic损失 优化求解：梯度下降／牛顿法 梯度 目标函数为$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$ 梯度为$\begin{eqnarray} g(\bf w) &amp;=&amp; \frac{\partial J(\bf w)}{\partial \bf w} = \frac{\partial}{\partial \bf w}[\sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]]\\&amp;=&amp; \sum_{i=1}^N[\mu(\mathbf x_i) - y_i] \mathbf x_i \\&amp;=&amp; \bf X^T(\mu - y)\end{eqnarray}$ 二阶Hessian矩阵为$\begin{eqnarray} H(w) &amp;=&amp; \frac{\partial}{\partial \mathbf w}[\mathbf g( \mathbf w)^T] = \sum_{i=1}^N(\frac {\partial}{\partial \mathbf w}\mu_i) \mathbf x_i^T\\&amp;=&amp; \sum_{i=1}{N}\mu_i(1-\mu_i)\bf x_ix_i^T = X^T\underset{S}{ \underbrace{diag(\mu_i(1-\mu_i)}}X = X^TSX\end{eqnarray}$ 牛顿法 亦称牛顿-拉夫逊（ Newton-Raphson ）方法 牛顿在17世纪提出的一种近似求解方程的方法 使用函数f(x)的泰勒级数的前面几项来寻找方程f(x) = 0的根 在求极值问题中，求$g(\mathbf w) = \frac {\partial J(\mathbf w)}{\partial w} = 0$的根 对应$J(\mathbf w)$处取极值 将导数$g(\mathbf w)$在 $w^t$处进行Taylor展开：$0 = \bf g(\hat w) = g(w^t)+(\hat w - w^t)H(w^t) + Op(\hat w - w^t)$ 去掉高阶无穷小$Op(\bf \hat w - w^t)$，从而得到$g(\bf w^t)+(\hat w - w^t)H(w^t) = 0 \Rightarrow \hat w = w^t - H^{-1}(w^t)g(w^t)$ 因此迭代机制为：$\bf w^{t+1} = w^t - H^{-1}(w^t)g(w^t)$ 也被称为二阶梯度下降法，移动方向:$\bf d = -(H(w^t))^{-1}g(w^t)$ Vs. 一阶梯度法，移动方向:$\bf d = -g(w^t)$移动 Iteratively Reweighted Least Squares（IRLS） 引入记号：$\bf g^t(w) = X^T(\mu^t - y), \mu_i^t = \sigma((w^t)^Tx_i)$$\bf H^t(w) = X^TS^tX$,$ S^t:diag(\mu_i^t(1-\mu_1^t),…,\mu_N^t(1-\mu_N^t))$ 根据牛顿法的结果：$w^{t+1} = w^t - (H^t)^{-1}g^t = (X^TS^tX)^{-1}X^TS^tz$ 回忆最小二乘问题： 目标函数：$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T(y - Xw)$ 解：$\hat w = (X^TX)^{-1}X^Ty$ 回忆加权最小二乘问题： 目标函数:$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T\Sigma^{-1}(y - Xw)$ 解：$\hat w = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}y$ IRLS中，$\bf w^{t+1} = (X^TS^tX)^{-1}X^TS^t[Xw^t + (S^t)^{-1}(y - \mu ^t)]$ 相当于权重矩阵为 $\Sigma^{-1} = \bf S^t$ 由于$S^t$是对角阵，$S^t$相当于给每个样本的权重$S_{ii}^t = \mu_i^t(1-\mu_i^t), \mathbf z_i^t = (\mathbf w^t)^T\mathbf x_i + \frac {y_i - \mu_i^t}{S_{ii}^t}$ 拟牛顿法 牛顿法比一般的梯度下降法收敛速度快，但是在高维情况下，计算目标函数的二阶偏导数的复杂度很大，而且有时候目标函数的海森矩阵无法保持正定，不存在逆矩阵，此时牛顿法将不再能使用。 因此，人们提出了拟牛顿法。其基本思想是：不用二阶偏导数而构造出可以近似Hessian矩阵(或Hessian矩阵的逆矩阵)的正定对称矩阵，进而再逐步优化目标函数。不同的构造方法就产生了不同的拟牛顿法（Quasi-Newton Methods） BFGS／LBFGS／Newton-CG 正则化的Logistic回归 若损失函数取logistic损失，则Logistic回归的目标函数为$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$ 同线性回归类似，Logistic回归亦可加上L2正则$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \Vert \mathbf w\Vert ^2_2$ 或L1正则$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \vert \mathbf w\vert $ L2正则的Logistic回归求解 梯度为:$g_{I 2}(\mathbf{w})=g(\mathbf{w})+\lambda \mathbf{w}=\sum_{i=1}^{N}\left(\mu\left(\mathbf{x}_{i}\right)-y_{i}\right) \mathbf{x}_{i}+\lambda \mathbf{w}=\mathbf{X}^{T}(\mathbf{\mu}-\mathbf{y})+\lambda \mathbf{w}$ Hessian矩阵为：$\mathbf{H}_{L 2}(\mathbf{w})=\mathbf{H}(\mathbf{w})+\lambda \mathbf{I}=\mathbf{X}^{T} \mathbf{S} \mathbf{X}+\lambda \mathbf{I}$ 类似不带正则的Logistic回归，可采用（随机）梯度下降、牛顿法或拟牛顿法求解。 L1正则的Logistic回归求解 L1正则项的在0处不可导 在此我们L1正则的Logistic回归的牛顿法（IRLS）求解 随机梯度下降（在线学习）在CTR预估部分讲解 Recall：IRLS\mathbf{w}^{t+1}=\left(\mathbf{X}^{T} \mathbf{S}^{t} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{S}^{t} \mathbf{z}=\underset{\mathbf{w}}{\arg \min }\left\|\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{X} \mathbf{w}-\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{z}\right\|_{2}^{2} L1正则的Logistic回归在每次迭代中可视为一个再加权的Lasso问题：\mathbf{w}^{t+1}=\underset{\mathbf{w}}{\arg \min }\left\|\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{X} \mathbf{w}-\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{z}\right\|_{2}^{2}, s . t .\|\mathbf{w}\|_{1}]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>人工智能</tag>
        <tag>Logistic回归</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + GitHub Pages + Next在windows下搭建个人博客]]></title>
    <url>%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2.html</url>
    <content type="text"><![CDATA[才搭好博客，发现在博客发布文章确实比微信公众号方便很多，这里简略说下用 Hexo + GitHub Pages + Next搭建个人博客的课程，大部分经验都是来自于网络，我会在整个过程后面附上参考的文章，一来总结搭建博客的过程，二来减少后来人踩坑。 整个过程： 1、注册Github账号及创建仓库 2、安装Git for Windows 3、配置Git 4、安装node.js 5、安装Hexo 6、使用next设计个性化博客 7、连接Hexo和Github Pages及部署博客 8、购买域名并解析以上就是全部的过程，当然具体还有很多细节，比如更换配置、设置文章字数的单位，阅读时常的单位，设置评论区，具体的东西还是要依据个人的喜好调整，但是next主题里面基本都集成了这些功能，只要稍微调整下就行。 参考的文章： 参考的整个过程 各种个性化小功能 给统计量添加单位 各种个性化设置 文章发布 GitHub/Coding双线部署 小书匠Markdown使用手册 在Markdown中输入数学公式(MathJax) Hexo 的 Next 主题中渲染 MathJax 数学公式 报错：hexo fs.SyncWriteStream is deprecated Hexo Next主题博客功能完善 MathJax语法 MathJax与LaTex介绍 MathJax(Markdown中的公式)的基本使用语法]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周 机器学习简介与线性回归]]></title>
    <url>%2F%E7%AC%AC%E4%B8%80%E5%91%A8%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    <content type="text"><![CDATA[1.1 一个Kaggle竞赛优胜解决方案 一个Kaggle竞赛优胜解决方案 任务：Avazu点击率预估竞赛 Rank 2nd Owen Zhang的解法 优胜算法的特点 特征工程 融合大法 多层 多种不同模型的组合 所以： 基础模型很重要（线性模型） 集成学习模型单模型性能好（GBDT） 特定问题的模型贡献大（FM） 模型融合很重要 课程内容安排 基本模型 线性模型： 线性回归， logistic回归， SVM 非线性模型： （线性模型核化）、分类回归树 集成学习模型（随机森林、GBDT） 数据预处理：数据清洗，特征工程，降维，聚类 模型融合 推荐系统/点击率预估问题特定解决方案 1.2 机器学习任务类型 定义 数据 数据通常以二维数据表形式给出 每一行： 一个样本 每一列：一个属性/特征 例：Boston房价预测数据，根据某地区房屋属性，预测该地区预测房价 506行， 506个样本 14列 机器学习任务类型 监督学习（Supervised Learning） 分类（classfication） 回归（regression） 排序（ranking） 非监督学习（unsupervised learning） 聚类（clustering） 降维（dimensionality reduction） 概率密度估计（density estimation） 增强学习（reinforcement learning） 半监督学习（semi-supervised learning） 迁移学习（transfer learning） …… 监督学习 学习一个x-&gt;y 的映射f, 从而对新输入的x进行预测f（x）D = \{X_i,y_i\}^N_{i=1} D：训练数据集 N：训练样本数目 $X_i$: 第i个样本的输入，亦被称为特征、属性或协变量 $y_i$: 第i个训练样本的输出，亦被称为响应，如类别标签、序号或数值 例：波士顿房价预测 回归 若输出y∈R为连续值，则我们称之为一个回归（regression）任务例： 房价预测，预测二手车的价格 假设回归模型：$y = f(\mathbf x|\theta)$ 如在线性回归中，$f(\mathbf x|w) = \mathbf w^T \mathbf x$ 训练：根据训练数据 $D = \{\mathbf X_i,y_i\}^N_{i=1}$ 学习映射 预测：对新的测试数据x进行预测：$\hat f = f(x)$ y带帽表示预测 学习目标：训练集上预测值与真值之间的差异最小 损失函数：度量模型预测值与真值之间的差异，如L(f(\mathbf x),y) = \frac 12(f(x) - y)^2 目标函数为：$J(\mathbf \theta) = \frac1N \sum_{i = 1}^N L(f(\mathbf x_i|\mathbf \theta), y_i)$ 分类 若输出y为离散值，则我们称之为一个分类，标签空间y = {1,2, … C} 例：信用评分 分类： 学习从输入x到输出y的映射f:概率问题$\hat y = f(\mathbf x) = \underset{c} {arg\ max} \ p(y = c\mid \mathbf x, D)$ 学习目标： 损失函数：01损失 l_{0/1}(y, \hat y) = \begin {cases} 0 & y = \hat y \\ 1 & otherwise \end{cases} 需要预测的概率： 预测：最大后验估计（Maximum a Posteriori, MAP）$\hat y = f(\mathbf x) = \underset{c} {arg\ max}\ p(y = c\mid \mathbf x, D)$ 排序（Rank） 排序学习是推荐、搜素、广告的核心方法 排序学习中需要首先根据查询q及其文档集合进行标注（data labeling） 和提取特征（feature extraction） 才能得到D = {….} 非监督学习 发现数据中的“有意义的模式”， 亦被称为知识发现 训练数据不包含标签 标签在训练数据中为隐含变量 $ D = \{ \bf X_i\}_{ i= 1}^ N $ 聚类例：人的“类型”分多少类？ 模型选择$ K^* = arg\ max _K\ p(K \mid D)$某个样本属于哪个类？ 降维多维特征，有些特征之间会相关而存在冗余很多算法中，降维算法成为了数据预处理的一部分， 如主成分分析（Principal Components Analysis, PCA） 半监督学习 当标注数据“昂贵”时有用 例：标注3D姿态、 蛋白质功能等等 多标签学习 有歧义标签学习 多实例学习 增强学习从行为的反馈(奖励或惩罚)中学习 设计一个回报函数（reward function）， 如果learning agent(如机器人、围棋ai程序)，在决定一步之后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负 增强学习的任务：找到一条回报值最大的路径 1.3 一个典型的机器学习案例-对鱼进行分类 根据一些光学传感器对传送带上的鱼进行分类 形式化为机器学习问题 训练数据 每条鱼的测量向量 每条鱼的标签 测试 给定一个新的特征向量x 预测对应的标签y 将长度作为特征进行分类（直方图） 需要先做一个决策边界 最小化平均损失 将亮度作为特征进行分类 （直方图） 将长度和亮度一起作为特征（二维散点图） 线性决策函数 二次决策函数 更复杂的决策边界训练集上的误差 ≠ 测试集上的误差数据过拟合（overfitting）推广性（generalization）差 小结：设计一个鱼的分类器 选择特征 可能是最重要的步骤！（收集训练数据） 选择模型（如决策边界的形状） 根据训练数据估计模型 利用模型对新样本进行分类 1.4 机器学习算法的组成部分 机器学习任务的一般步骤 确定特征 可能是最重要的步骤！（收集训练数据） 确定模型 目标函数/决策边界形状 模型训练：根据训练数据估计模型参数 优化计算 模型评估：在校验集上评估模型预测性能 模型应用/预测 模型 监督学习任务：$D = \{X_i, y_i\} _{i = 1} ^ N $ 模型：对给定的输入x, 如何预测其标签$ \hat y$ 不同模型对数据的假设不同 最简单的模型：线性模型$ f(x) = \sum_j w_j x_j = \bf w^T \bf x$ 确定模型类别后，模型训练转化为求解模型参数 如对线性模型参数为$\theta = \{w_j \mid j = 1,…, D\}$,其中D为特征维数 求解模型参数：目标函数最小化 非线性模型 基函数： $x^2$, log, exp, 样条函数，决策树…. 核化：将原问题转化为对偶问题，将对偶问题中的向量积$\langle x_i, x_j\rangle$ 换成核函数$k(x_i,x_j)$ 目标函数：通常包含两项：损失函数和正则项J(\theta) = \frac 1N \sum_{i=1}^N\ L(f(x_i; \theta), y_i) + R(\theta) 损失函数 损失函数 - 回归 损失函数：度量模型预测值与真值之间的差异 对回归问题：令残差 $r = f(\bf x) - y$ L2损失：连续，但对噪声敏感L_2 (r) = \frac 12 r ^2 L1损失：不连续，对噪声不敏感L_1(r) = |r| Huber 损失： 连续，对噪声不敏感L_\delta (r) = \begin{cases} \frac 12 r^2 & if|r| \le \delta\\ \delta |r| - \frac 12 \delta^2 & if|r| \ge \delta\end{cases} 损失函数 - 分类 损失函数：度量模型预测值与真值之间的差异 对分类问题 0-1损失：$l_{0/1}(y,f(x)) = \begin{cases} 1 &amp; yf(x) \lt 0 \\ 0 &amp; othereise\end{cases}$ logistic损失：亦称负log似然损失 $l_{log}(y,f(x)) = log(1 + exp(-yf(x)))$ 指数损失：$l_{exp}(y,f(x)) = exp(-yf(x))$ 合页损失：$l_{hinge}(y,f(x)) = max(0, 1 - yf(x))$ 正则项 复杂模型（预测）不稳定：方差大 正则项对复杂模型施加惩罚 正则项的必要性例：sin曲线拟合 增加L2正则岭回归：最小化RSS 欠拟合：模型太简单/对复杂性惩罚太多 样本数目增多时，可以考虑更复杂的模型 常见正则项 L2正则: $R(\theta) = \lambda ||\theta||^2_2 = \lambda \sum^D_{j=1} \theta_j^2$ L1正则: $R(\theta) = \lambda |\theta| = \lambda \sum ^D_{j=1}|\theta_j|$ L0正则: $R(\theta) = \lambda||\theta||_ 0$ 非0参数的数目 不好优化，通常用L1正则近似 常见线性模型的损失和正则项组合 L2损失 L1损失 Huber损失 Logistic损失 合页损失 e-insensitive损失 L2正则 岭回归 L2正则 Logistic回归 SVM SVR L1正则 LASSO L1正则 Logistic回归 L2+L1正则 Elastic 模型训练 在训练数据上求目标函数极小值：优化 简单目标函数直接求解 如小数据集上的线性回归 更复杂问题：凸优化 （随机）梯度下降 牛顿法/拟牛顿法 … 梯度下降（Gradient Descent）算法 梯度下降/最速下降算法：快速寻找函数局部极小值 梯度下降算法：求函数J（θ）的最小值 给定初始值$θ^0$ 更新θ，使得J（θ）越来越小 $θ^t = θ^{t-1} - \eta\nabla J(θ)$ ( $\eta$ : 学习率 ) 直到收敛到 / 达到预先设定的最大迭代次数 下降的步伐太小（学习率）非常重要：如果太小，收敛速度慢； 如果太大，可能会出现overshoot the minimum的现象 梯度下降求得的只是局部最小值 二阶导数 &gt; 0, 则目标函数为凸函数，局部极小值即为全局最小值 随机选择多个初始值，得到函数的多个局部极小值点。多个局部极小值点的最小值为函数的全局最小值 梯度下降算法每次学习都使用整个训练集，这样对大的训练数据集合，每次学习时间过长，对大的训练集需要消耗大量的内存。此时可采用随机梯度下降（Stochastic gradient descent, SGD), 每次从训练集中随机选择一部分样本进行学习。 更多（随机）梯度下降算法的改进版 动量（Momentum） Nesterov accelerated gradient (NAG) Adagrad RMSprop Adaptive Moment Estimation (Adam)… 模型选择与模型评估 同一个问题有不同的解决方案 如线性回归 vs. 决策树 哪个更好？ 模型评估与模型选择 在新数据点的预测误差最小 模型评估：已经选定最终的模型，估计它在新数据上的预测误差 模型选择：估计不同模型的性能，选出最好的模型 样本足够多：训练集和校验集 样本不够多：重采样技术来模拟校验集：交叉验证和bootstrap K-折交叉验证 交叉验证（Cross Validation, CV）： 将训练数据分成容量大致相等的K份（通常K = 5/10） 交叉验证估计的误差为：CV(M)= \frac1K \sum ^K_{k = 1} E_k(M) 模型选择 对多个不同模型，计算其对应的误差CV（M）， 最佳模型为CV（M）最小的模型 模型复杂度和泛化误差的关系通常是U形曲线： 1.5 学习环境简介 编程语言 Python 数据处理工具包 Numpy SciPy pandas 数据可视化工具包 Matplotlib Seaborn 机器学习工具包 scikit learn 示例代码：INotebook NumPy NumPy(Numeric Python)是Python的开源数值计算扩展，可用来存储和处理大型矩阵 Numpy包括： N维数组(ndarray) 实用的线性代数、傅里叶变换和随机数生成函数 Numpy和稀疏矩阵运算包SciPy配合使用更加方便 SciPy SciPy是建立在NumPy的基础上、是科学和工程设计的Python工具包，提供统计、优化和数值微积分计算等功能 NumPy 处理$10^6$级别的数据通常没有大问题，但当数据量达到$10^7$级别时速度开始发慢，内存受到限制（具体情况取决于实际内存的大小） 当处理超大规模数据集，比如$10^{10}$级别，且数据中包含大量的0时，可采用稀疏矩阵可显著的提高速度和效率 Pandas(Pandel data structures) Pandas是Python语言的“关系型数据库”数据结构和数据分析工具，非常高效且易于使用 基于NumPy补充了大量数据操作功能，能实现统计、分组、排序、透视表(SQL语句的大部分功能) Pandas主要有2种重要的数据类型 series：一维序列 DataFrame：二维表(机器学习数据的常用数据结构) Matplotlib Matplotlib是Python语言的2D图形绘制工具 Seaborn Seaborn是一个基于Matplotlib的Python可视化工具包，提供更高层次的用户接口，可以给出漂亮的数据统计 Scikit - Learn Machine Learning in Python Scikit-Learn是基于Python的开源机器学习模块，最早于2007年由David Cournapeau发起 基本功能有六部分：分类（Classification），回归（Regression），聚类（Clustering），数据降维（Dimensionality reduction），模型选择（Model Selection），数据预处理（Preprocessing） 对于具体的机器学习问题，通常可以分为三个步骤 数据准备与预处理（Preprocessing, Dimensionality reduction） 模型选择与训练（Classification, Regression, Clustering） 模型验证与参数调优（Model Selection） 各种机器学习模型有统一的接口 模型既有默认参数，也提供多种参数调优方法 卓越的文档 丰富的随附任务功能集合 活跃的社区提供开发和支持 1.6 线性回归模型 目标函数通常包含两项：损失函数和正则项J(\bf \theta) = \frac1N \sum_{i = 1}^N L(f(\bf x_i|\bf \theta), y_i) + \lambda R(\bf \theta) 对回归问题，损失函数可以采用L2损失，得到\begin{eqnarray}J(\theta) &=&\sum_{i=1}^NL(y_i,\hat y_i) \\ &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\ &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 残差平方和（residual sum of squares, RSS） 由于线性模型比较简单，实际应用中有时正则项为空，得到最小二乘线性回归（Ordinary Least Square, OLS）\begin{eqnarray}J(\theta) &=&\sum_{i=1}^NL(y_i,\hat y_i) &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\ &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 正则项可以为L2正则，得到岭回归（Ridge Regression）J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda ||w||^2_2 正则项也可以选L1正则，得到Lasso模型： J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda |w| 当$\lambda$取合适值时，Lasso（Least absolute shrinkage and selection operator）的结果是稀疏的（w的某些元素系数为0），起到特征选择作用 为什么L1正则的解是稀疏的？ 线性回归模型的概率解释 最小二乘（线性）回归等价于极大似然估计 假设：$ y = f(\bf x) + \epsilon = w^Tx + \epsilon $其中$\epsilon$为线性预测和真值之间的残差我们通常假设残差的分布为$\epsilon \sim N(0,\sigma ^2)$,因此线性回归可写成：$p(y|x,\theta) \sim N(y| \bf w^T \bf x, \sigma^2)$,其中$ \bf \theta = (\bf w, \sigma ^2)$ 正则（线性）回归等价于高斯先验（L2正则）或Laplace先验下（L1正则）的贝叶斯估计 Recall：极大似然估计 极大似然估计（Maximize Likelihood Estimator, MLE）定义为\hat \theta = \underset {\theta} {arg\ max}\ log\ p(D\mid \theta) 其中（log）似然函数为l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) 表示在参数为$\theta$的情况下，数据$D ={\bf x_i,y_i}^N_{i=1}$ 极大似然：选择数据出现概率最大的参数 线性回归的MLEp(y_i|x_i,\bf w,\sigma ^2) \sim N(y_i\mid \bf w^T \bf x_i, \sigma^2) = \frac 1{\sqrt{2\pi}\sigma} exp(-\frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2)) OLS的似然函数为l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) 极大似然可等价地写成极小负log似然损失（negative log likelihood, NLL） \begin{eqnarray}{NLL(\bf \theta)} &=& \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) \\ &=& - \sum_{i=1}^N log ((\frac 1{2 \pi \sigma^2})^ \frac 12 exp(- \frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))) \\ &=& \frac N2 log(2\pi \sigma ^2) + \frac 1{2 \sigma^2} \sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 正则回归等价于贝叶斯估计 假设残差的分布为$\epsilon \sim N(0, \sigma ^2)$,线性回归可写成：$p(y_i \mid \bf x_i, \theta) \sim N(y_i \mid \bf w^T \bf x_i，\sigma ^2)$$p(y\mid \bf X, \bf w, \sigma ^2) = N(\bf y \mid \bf X \bf w, \sigma ^2 \bf I_N) \propto exp(- \frac 1{2\sigma ^2}((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w)))$ 若假设参数为w的先验分布为 $w_j \sim N(0, \tau ^2)$ 偏向较小的系数值，从而得到的曲线也比较平滑$p(\bf w) =\prod_{j=1}^{D} N(w_j \mid 0, \tau ^2) \propto exp(- \frac 1{2\tau^2} \sum_{j=1}^D \bf w_j^2 = exp(- \frac 1{2\tau^2} ( \bf w^T \bf w ) )) $ 其中$1/\tau ^2$控制先验的强度 根据贝叶斯公式，得到参数的后验分布为$p(y\mid \bf X, \bf w, \sigma ^2) = \propto exp(- \frac 1{2\sigma ^2} ((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w) ) - \frac 1{2 \tau^2} ( w^Tw ) )$ 则最大后验估计(MAP)等价于最小目标函数$J(\bf w) = (\bf y - \bf X\bf w)^T(\bf y - \bf X\bf w) + \frac {\sigma ^2}{\tau^2} \bf w^T \bf w $ 对比岭回归的目标函数$J(\bf w) = \sum_{i=1}^N(y_i -\bf w^T\bf x_i)^2 + \lambda \Vert \bf w\Vert ^2_2$ 小结 线性回归模型可以放到机器学习一般框架 损失函数：L2损失，… 正则：无正则， L2正则，L1正则… 正则回归模型可视为先验为正则、似然为高斯分布的贝叶斯估计 L2正则：先验分布为高斯分布 L1正则：先验分布为Laplace分布 1.7 线性回归模型-优化算法 线性回归的目标函数 无正则的最小二乘线性回归（Ordinary Least Square, OLS）：J(w) = \sum_{i=1}^N(y_i - w^Tx_i)^2 L2正则的岭回归（Ridge Regression）模型：J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D w_j^2 L1正则的Lasso模型：J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D |w_j| 模型训练： 根据训练数据求目标函数取极小值的参数： $\hat w = \underset {w} {arg\ min} J(\bf w)$ 目标函数的最小值： 一阶的导数为0：$\frac{\partial J(w)} {\partial w}$ 二阶导数&gt;0：$\frac{\partial J^2(w)} {\partial w^2}$ OLS的优化求解： OLS的优化求解 OLS的目标函数写成矩阵形式：$J(w) = \sum ^N_{i=1}(y_i - w^Tx_i)^2 = (y - Xw)^T(y - Xw)$ 只取与w有关的项，得到$J(w) = w^T(X^TX)w - 2w^T(X^Ty)$ 求导 $\frac{\partial J(w)} {\partial w} = 2X^TXw - 2X^Ty = 0 \Rightarrow X^TXw = X^Ty$`$\hat w_{OLS} = (X^TX)^{-1}X^Ty$ OLS的优化求解 ——SVD OLS目标函数：$J(w) = \Vert y - Xw\Vert_2^2$ 相当于求 $y = Xw$ 如果X为方阵，可求逆：$w = X^{-1}y$ 如果𝐗不是方阵，可求Moore-Penrose广义逆：$𝐰 = 𝐗^{\dagger }𝐲$。 Moore-Penrose广义逆可采用奇异值分解(Singular Value Decomposition)实现：奇异值分解：$X = U \Sigma V^T$$X^{\dagger } = V \Sigma ^{\dagger} U^T$其中 $\Sigma = \begin{pmatrix}{\sigma_1}&amp;{0}&amp;{\cdots}&amp;{0}\\{0}&amp;{\sigma_2}&amp;{\cdots}&amp;{0}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{0}&amp;{0}&amp;{\cdots}&amp;{0}\\\end{pmatrix}$,$\Sigma ^{\dagger} = \begin{pmatrix}{\frac {1}{\sigma_1}}&amp;{0}&amp;{\cdots}&amp;{0}\\{0}&amp;{\frac{1}{\sigma_2}}&amp;{\cdots}&amp;{0}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{0}&amp;{0}&amp;{\cdots}&amp;{0}\\\end{pmatrix}$ OLS的优化求解——梯度下降 OLS目标函数：$J(w) = (y - Xw)^T(y - Xw)$梯度：$\nabla_w = - 2X^T(y - Xw^t)$参数更新：$w^{t+1} = w^t - \eta\nabla_w = w^t + 2\eta X^T(y - Xw^t)$ 岭回归的优化求解 岭回归的目标函数与OLS只相差一个正则项（也是w的二次函数） 岭回归的优化求解——SVD Lasso的优化条件 软&amp; 硬阈值 Lasso的优化求解——坐标轴下降法 为了找到一个函数的局部极小值，在每次迭代中可以在当前点处沿一个坐标方向进行一维搜索。 整个过程中循环使用不同的坐标方向。一个周期的一维搜索迭代过程相当于一个梯度迭代。 注意： 梯度下降方法是利用目标函数的导数（梯度）来确定搜索方向的，而该梯度方向可能不与任何坐标轴平行。 而坐标轴下降法是利用当前坐标系统进行搜索，不需要求目标函数的导数，只按照某一坐标方向进行搜索最小值。（在稀疏矩阵上的计算速度非常快，同时也是Lasso回归最快的解法） 小结 线性回归模型比较简单 当数据规模比较小时，可直接解析求解 scikit learn中的实现采用SVD分解实现 当数据规模较大时，可采用随机梯度下降 scikit learn提供一个SGDRegression类 岭回归求解类似OLS，采用SVD分解实现 Lasso优化求解采用坐标轴下降法 1.8 线性回归模型-模型选择 模型评估与模型选择 模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果 校验集划分（train_test_split、交叉验证） 评价指标（sklearn.metrics） 模型选择： 模型中通常有一些超参数，需要通过模型选择来确定 线性回归模型中的正则参数 OLS中的特征的数目 参数搜索范围：网格搜索（GridSearch） Scikit learn将交叉验证与网格搜索合并为一个函数 评价准则 模型训练好后，可用一些度量准则检查模型拟合的效果 开方均方误差（rooted mean squared error，RMSE）:$RMSE = \sqrt{\frac 1N \sum_{i=1}^N(\hat y_i - y_i)^2}$` 平均绝对误差（mean absolute error，MAE）：$MAE = \frac 1N \sum_{i=1}^N|\hat y_i - y_i|$ R2 score：既考虑了预测值与真值之间的差异，也考虑了问题本身真值之间的差异（ scikit learn 线性回归模型的缺省评价准则）$SS_{res} = \sum_{i=1}^N(\hat y_i - y_i)^2, SStot = \sum_{i=1}^N(y_i - \bar{y})^2, R^2 = 1 - \frac {SS_{res}}{SS_{tot}})$ 也可以检查残差的分布 还可以打印预测值与真值的散点图 线性回归中的模型选择Scikit learn中的model selection模块提供模型选择功能 对于线性模型，留一交叉验证（N折交叉验证，亦称为leave-oneout cross-validation，LOOCV）有更简便的计算方式，因此Scikit learn提供了RidgeCV类和LassoCV类实现了这种方式 后续课程将讲述一般模型的交叉验证和参数调优GridSearchCV RidgeCV RidgeCV中超参数λ用alpha表示 RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False) LassoCV LassoCV的使用与RidgeCV类似 Scikit learn还提供一个与Lasso类似的LARS（least angle regression，最小角回归），二者仅仅是优化方法不同，目标函数相同。 当数据集中特征维数很多且存在共线性时，LassoCV更合适。 小结：线性回归之模型选择 采用交叉验证评估模型预测性能，从而选择最佳模型 回归性能的评价指标 线性模型的交叉验证通常直接采用广义线性模型的留一交叉验证进行快速模型评估 Scikit learn中对RidgeCV和LassoCV实现该功能 1.9 波士顿房价预测案例详解——数据探索 第一步：理解任务，准备数据 数据读取 Pandas支持多种格式的数据 数据探索&amp;特征工程 数据规模 确定数据类型，是否需要进一步编码 特征编码 数据是否有缺失值 数据填补 查看数据分布，是否有异常数据点 离群点处理 查看两两特征之间的关系，看数据是否有冗余/相关 降维 数据概览 pandas:DataFrame Head():数据前5行，可查看每一列的名字及数据类型 Info(): 数据规模：行数&amp;列数 每列的数据类型、是否有空值 占用存储量 shape:行数&amp;列数 各属性的统计特性 直方图 每个取值在数据集中出现的样本数目 离群点 离群点：奇异点（outlier）,指远离大多数样本的样本点。通常认为这些点是噪声，对模型有坏影响 相关性 相关性：相关性可以通过计算相关系数或打印散点图来发现 相关系数： 散点图 可以通过两个变量之间的散点图直观感受二者的相关性 数据预处理 数据标准化（ Standardization ） 某个特征的所有样本取值为0均值、1方差 数据归一化（ Scaling ） 某个特征的所有样本取值在规定范围内 数据正规化（ Normalization ） 每个样本模长为1 数据二值化 根据特征值取值是否大于阈值将特征值变为0或1，可用类Binarizer 实现 数据缺失 数据类型变换 有些模型只能处理数值型数据。如果给定的数据是不同的类型，必须先将数据变成数值型。 第二步：模型确定和模型训练 1、确定模型类型 目标函数（损失函数、正则） 2、模型训练 优化算法（解析法，梯度下降、随机梯度下降…） 第三步：模型评估与模型选择 模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果 校验集划分（train_test_split、交叉验证） 评价指标 （sklearn.metics） 也可以检查残差的分布 还可以打印预测值与真值的散点图 模型选择：选择预测性能最好的模型 模型中通常有一些超参数，需要通过模型选择来确定 参数搜索范围：网格搜索（GridSearch） 1.10 波士顿房价预测-数据探索代码python 3.712345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns# 读入数据data = pd.read_csv("boston_housing.csv")# 数据探索print(data.head())data.info()print(data.isnull().sum())print(data.describe())# 目标y(房屋价格)的直方图/分布fig = plt.figure()sns.distplot(data.MEDV.values, bins=30, kde=True)plt.xlabel('Median value of owner_occupied homes', fontsize=12)plt.show()# 单个特征散点图plt.scatter(range(data.shape[0]), data["MEDV"].values, color='purple')plt.title("Distribution of Price")plt.show()# 删除y大于50的样本data = data[data.MEDV &lt; 50]print(data.shape)# 输入属性的直方图／分布# 犯罪率特征fig = plt.figure()sns.distplot(data.CRIM.values, bins=30, kde=False)plt.xlabel('crime rate', fontsize=12)plt.show()# 是否靠近charles riversns.countplot(data.CHAS, order=[0, 1]);plt.xlabel('Charles River');plt.ylabel('Number of occurrences');plt.show()# 靠近高速sns.countplot(data.RAD)plt.xlabel('index of accessibility to radial highways')plt.show()# 两两特征之间的相关性# 获得所有列的名字cols = data.columns# 计算相关性data_corr = data.corr().abs()# 相关性热图plt.subplots(figsize=(13, 9))sns.heatmap(data_corr, annot=True)sns.heatmap(data_corr, mask=data_corr &lt; 1, cbar=False)plt.savefig('house_coor.png')plt.show()# 输出强相关对threshold = 0.5corr_list = []size = data_corr.shape[0]for i in range(0, size): for j in range(i+1, size): if (data_corr.iloc[i,j] &gt;= threshold and data_corr.iloc[i, j] &lt; 1) or (data_corr.iloc[i, j] &lt; 0 and data_corr.iloc &lt;= -threshold): corr_list.append([data_corr.iloc[i, j], i, j])s_corr_list = sorted(corr_list, key=lambda x: -abs(x[0]))for v, i, j in s_corr_list: print("%s and %s = %.2f" % (cols[i], cols[j], v))for v, i, j in s_corr_list: sns.pairplot(data, height=6, x_vars=cols[i], y_vars=cols[j]) plt.show() 1.11 波士顿房价预测案例详解1.12 波士顿房价预测案例详解-代码讲解python 3.7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174# 波士顿房价预测案例——线性回归分析import numpy as np # 矩阵操作import pandas as pd # SQL数据处理from sklearn.metrics import r2_score # 评价回归预测模型的性能import matplotlib.pyplot as plt # 画图import seaborn as sns# 读入数据data = pd.read_csv("boston_housing.csv")# 1、数据准备# 从原始数据中分离输入特征x和输出yy = data['MEDV'].valuesX = data.drop('MEDV', axis=1)# 用于后续显示权重系数对应的特征columns = X.columns# 数据较少，将数据分割训练数据from sklearn.model_selection import train_test_split# 随机采样20%的数据构建测试样本，其余作为训练样本X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33,test_size=0.2)# print(X_train.shape)# 2、数据预处理/特征工程# 数据标准化from sklearn.preprocessing import StandardScaler# 分别初始化对特征和目标值的标准化器ss_X = StandardScaler()ss_y = StandardScaler()# 分别对训练和测试数据的特征以及目标值进行标准化处理X_train = ss_X.fit_transform(X_train)X_test = ss_X.transform(X_test)# 对y标准化不是必须# 对y标准化的好处是不同的问题的w差异不太大，同时正则参数的范围也有限y_train = ss_y.fit_transform(y_train.reshape(-1, 1))y_test = ss_y.transform(y_test.reshape(-1, 1))# 3、确定模型类型# 3.1 尝试缺省参数的线性回归# 线性回归from sklearn.linear_model import LinearRegression# 使用默认配置初始化lr = LinearRegression()# 训练模型参数lr.fit(X_train, y_train)# 预测y_test_pred_lr = lr.predict(X_test)y_train_pred_lr = lr.predict(X_train)# 看看各特征的权重系数，系数的绝对值大小可视为该特征的重要性fs = pd.DataFrame(&#123;"columns": list(columns), "coef": list((lr.coef_.T))&#125;)fs.sort_values(by=['coef'], ascending=False)print(fs)# 模型评价# 测试集print('The r2 score of LinearRegression on test is', r2_score(y_test, y_test_pred_lr))# 训练集print('The r2 score of LinearRegression on train is', r2_score(y_train, y_train_pred_lr))# 在训练集上观察残差的分布，看是否符合模型假设：噪声为0均值的高斯噪声f, ax = plt.subplots(figsize=(7, 5))f.tight_layout()ax.hist(y_train - y_train_pred_lr, bins=40, label='Residuals Linear', color='b', alpha=.5)ax.set_title("Histogram of Residuals")ax.legend(loc='best')plt.show()# 还可以观察预测值与真值的散点图plt.figure(figsize=(4, 3))plt.scatter(y_train, y_train_pred_lr)plt.plot([-3, 3],[-3, 3], '--k')plt.axis('tight')plt.xlabel('True price')plt.ylabel('Predicted price')plt.tight_layout()plt.show()# 线性模型，随机梯度下降优化模型参数# 随机梯度下降一般在大数据集上应用，其实本项目不适合用from sklearn.linear_model import SGDRegressor# 使用默认配置初始化线sgdr = SGDRegressor(max_iter=1000)# 训练：参数估计sgdr.fit(X_train, y_train)# 预测sgdr.coef_print('The value of default measurement of SGDRegressor on test is', sgdr.score(X_test, y_test))print('The value of default measurement of SGDRegressor on train is', sgdr.score(X_train, y_train))# 3.2 正则化的线性回归（L2正则--&gt;岭回归）from sklearn.linear_model import RidgeCV# 设置超参数（正则参数）范围alphas = [0.01, 0.1, 1, 10, 100]# 生成一个RidgeCVridge = RidgeCV(alphas=alphas, store_cv_values=True)# 模型训练ridge.fit(X_train, y_train)# 预测y_test_pred_ridge = ridge.predict(X_test)y_train_pred_ridge = ridge.predict(X_train)# 评估，使用r2_score评价模型在测试集和训练集上的性能print('The r2 score of RidgeCV on test is', r2_score(y_test, y_test_pred_ridge))print('The r2 score of RidgeCV on test is', r2_score(y_train, y_train_pred_ridge))# 可视化mse_mean = np.mean(ridge.cv_values_, axis=0)plt.plot(np.log10(alphas), mse_mean.reshape(len(alphas), 1))# plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])plt.xlabel('log(alpha)')plt.ylabel('mse')plt.show()print('alpha is:', ridge.alpha_)# 看看各特征的权重系数，系数的绝对值大小可视为该特制的重要性fs = pd.DataFrame(&#123;"columns": list(columns), "coef_lr": list(lr.coef_.T), "coef_ridge": list(ridge.coef_.T)&#125;)fs.sort_values(by=['coef_lr'], ascending=False)print(fs)# 3.3 正则化的线性回归（L1正则--&gt;Lasso）from sklearn.linear_model import LassoCV# 生成一个LassoCV实例lasso = LassoCV()# 训练（内含CV）lasso.fit(X_train, y_train)# 测试y_test_pred_lasso = lasso.predict(X_test)y_train_pred_lasso = lasso.predict(X_train)# 评估， 使用r2_score评价模型在测试集和训练集上的性能print('The r2 score of LassoCV on test is', r2_score(y_test, y_test_pred_lasso))print('The r2 score of LassoCV on train is', r2_score(y_train, y_train_pred_lasso))# 可视化mses = np.mean(lasso.mse_path_, axis=1)plt.plot(np.log10(lasso.alphas_), mses)# plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])plt.xlabel('log(alpha)')plt.ylabel('mse')plt.show()print('alpha is:', lasso.alpha_)# 看看各特征的权重系数，系数的绝对值大小可视为该特制的重要性fs = pd.DataFrame(&#123;"columns": list(columns), "coef_lr": list(lr.coef_.T), "coef_ridge": list(lasso.coef_.T)&#125;)fs.sort_values(by=['coef_lr'], ascending=False)print(fs)]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>学习笔记</tag>
        <tag>人工智能</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉基础入门 学习笔记]]></title>
    <url>%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8.html</url>
    <content type="text"><![CDATA[一、 计算机视觉和深度学习概述 计算机视觉回顾 计算机视觉（computer vision）定义 数据（静态图片，视频） 算法（机器学习算法，神经网络）本质上是一个回归+分类 计算机视觉的重要性 三大任务：图像识别（image classification）车牌识别，人脸识别 三大任务：目标检测（object detection = classification + localization）行人检测和车辆检测 三大任务：图像分割图像语义分割个体分割 = 检测 + 分割 视觉目标跟踪（tracking） 视频分割 图像风格迁移 生成对抗网络（GAN） 视频生成 深度学习介绍 2006 Hinton bp(反向传播) 2012 Krizhevsky A 深度学习 深度卷积 RNN LSTM 持续信息 视觉识别，语音识别，DeepMind, AlphaGo 人脸识别：LFW 错误率5% -&gt; 0.5% 图像分割 VGGNet, GoogleNet, ResNet, DenseNet 常见的深度学习开发平台 Torch, TensorFlow, MatConvNetTheano, Caffe 课程介绍 图像识别：Alexnet, VGGnet, GoogleNet, ResNet, DenseNet 目标检测Fast-rcnn, faster-rcnn, Yolo, Retina-Net 图像分割FCN, Mask-Rcnn 目标跟踪GORURN， ECO 图像生成GAN， WGAN 光流FlowNet 视频分割Segnet 二、 图像分类与深度卷积网络的模型 图像分类 图像分类的挑战光照变化形变类内变化 图像分类定义 目标分类框架 泛化能力如何提高泛化能力？ 需要用图像特征来描述图像 训练和测试的流程 图像特征 color: Qutantize RGB values global shape: PCA space local shape: shape context texture: Filter banks SIFT, Hog, LBP, Harr 支持向量机（SVM） 超平面与支持向量 最大化间隔 svm分类（python）以lris兰花分类为例 程序实现 更好的特征 CNN特征 学习出来的 如何学习？ 构造神经网络 神经网络原理 神经网络做图像分类 神经网络搭建 神经网络的基本单元：神经元 激励函数 Sigmoid、tanh、ReLU、Leaky ReLU、Maxout、ELU 卷积层 卷积滤波的计算 卷积层可视化 池化层（pooling layer） 特征表达更加紧凑，同时具有位移不变性 全连接层 损失函数 交叉熵损失函数（SIGMOID_CROSS_ENTROPY_LOSS) 应用于二分类问题 Softmax 损失函数（SOFTMAX_LOSS) 多分类问题 欧式距离损失函数（EUCLIDEAN_LOSS）回归问题 对比损失函数（Contrastive loss）用来计算两个图像之间的相似度 Triplet loss 训练网络 网络训练和测试 卷积神经网络介绍 Alexnet, VGGnet, GoogleNet, ResNet, DenseNet 训练技巧， 防止过拟合（泛化能力不强） 数据增强（Data augmentation） 水平翻转， 随机裁剪和平移变换，颜色、光照变换 Dropout 其他有助于训练的手段 L1， L2正则化 Batch Normalization 利用caffe搭建深度网络做图像分类]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>计算机视觉</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
</search>
