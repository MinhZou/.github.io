<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python中pandas库.loc, .iloc, .ix 的用法及例子]]></title>
    <url>%2F2019%2F10%2F15%2FPython%E4%B8%ADpandas%E5%BA%93.loc%2C%20.iloc%2C%20.ix%20%E7%9A%84%E7%94%A8%E6%B3%95%E5%8F%8A%E4%BE%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[一 .loc.loc：根据DataFrame的具体标签选取列，当每列已有column name时，用 df [ ‘a’ ] 就能选取出一整列数据。如果你知道column names 和index，且两者都很好输入，可以选择 .loc 123import numpy as npimport pandas as pddf = pd.DataFrame(np.arange(0,200,2).reshape(10,10), columns = list('abcdefghij')) 123456789101112&gt;&gt;&gt; print(df) a b c d e f g h i j0 0 2 4 6 8 10 12 14 16 181 20 22 24 26 28 30 32 34 36 382 40 42 44 46 48 50 52 54 56 583 60 62 64 66 68 70 72 74 76 784 80 82 84 86 88 90 92 94 96 985 100 102 104 106 108 110 112 114 116 1186 120 122 124 126 128 130 132 134 136 1387 140 142 144 146 148 150 152 154 156 1588 160 162 164 166 168 170 172 174 176 1789 180 182 184 186 188 190 192 194 196 198 12345&gt;&gt;&gt; print(df.loc[[1,5],['a','b']]) a b1 20 225 100 102 123456789101112&gt;&gt;&gt; print(df.loc[:,['a']]) a0 01 202 403 604 805 1006 1207 1408 1609 180 123456789101112131415&gt;&gt;&gt; print(df.loc[0,['a']])a 0Name: 0, dtype: int32&gt;&gt;&gt; print(df.loc[0,:])a 0b 2c 4d 6e 8f 10g 12h 14i 16j 18Name: 0, dtype: int32 二 .iloc .iloc：根据标签的所在位置，从0开始计数，选取列，如果索引是数字，就使用.iloc 12&gt;&gt;&gt; print(df.iloc[2,1])42 123456&gt;&gt;&gt; print(df.iloc[:5, 5])0 101 302 503 704 90 1234567891011&gt;&gt;&gt; print(df.iloc[:-1,5])0 101 302 503 704 905 1106 1307 1508 170Name: f, dtype: int32 1234567891011&gt;&gt;&gt; print(df.iloc[:,5])0 101 302 503 704 905 1106 1307 1508 1709 190 123456789101112&gt;&gt;&gt; print(df.iloc[:,:]) a b c d e f g h i j0 0 2 4 6 8 10 12 14 16 181 20 22 24 26 28 30 32 34 36 382 40 42 44 46 48 50 52 54 56 583 60 62 64 66 68 70 72 74 76 784 80 82 84 86 88 90 92 94 96 985 100 102 104 106 108 110 112 114 116 1186 120 122 124 126 128 130 132 134 136 1387 140 142 144 146 148 150 152 154 156 1588 160 162 164 166 168 170 172 174 176 1789 180 182 184 186 188 190 192 194 196 198 三 .ix .ix 混合使用下标和名称进行选取 1234567891011121314151617181920212223&gt;&gt;&gt; print(df.ix[[1,3],[1,3]]) b d1 22 263 62 66&gt;&gt;&gt; print(df.ix[1:3, [1,2]]) b c1 22 242 42 443 62 64&gt;&gt;&gt; print(df.loc[:,['a']]) a0 01 202 403 604 805 1006 1207 1408 1609 180&gt;&gt;&gt; print(df.loc[0,['a']])a 0]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习》第7章 朴素贝叶斯 公式推导]]></title>
    <url>%2F2019%2F07%2F15%2F%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC7%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%20%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[第7章 朴素贝叶斯贝叶斯判定准则贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择那个能使条件风险$R(c | \boldsymbol{x})$最小的类别标记，即 h^{\star}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \min } R(c | \boldsymbol{x})此时，$h^{\star}$称为贝叶斯最优分类器。 已知条件风险$R(c | \boldsymbol{x})$的计算公式为 R\left(c_{i} | \boldsymbol{x}\right)=\sum_{j=1}^{N} \lambda_{i j} P\left(c_{j} | \boldsymbol{x}\right)若目标是最小化分类错误率，则误判损失$\lambda_{i j}$对应为0/1损失，也即 \lambda_{i j}=\left\{\begin{array}{l}{0, \text { if } i=j} \\ {1, \text { otherwise }}\end{array}\right.那么条件风险$R(c | \boldsymbol{x})$的计算公式可以进一步展开为 \begin{aligned} R\left(c_{i} | \boldsymbol{x}\right) &=1 \times P\left(c_{1} | \boldsymbol{x}\right)+\ldots+1 \times P\left(c_{i-1} | \boldsymbol{x}\right)+0 \times P\left(c_{0} | \boldsymbol{x}\right)+1 \times P\left(c_{i+1} | \boldsymbol{x}\right)+\ldots+1 \times P\left(c_{N} | \boldsymbol{x}\right) \\ &=P\left(c_{1} | \boldsymbol{x}\right)+\ldots+P\left(c_{i-1} | \boldsymbol{x}\right)+P\left(c_{i+1} | \boldsymbol{x}\right)+\ldots+P\left(c_{N} | \boldsymbol{x}\right) \end{aligned}由于$\sum_{j=1}^{N} P\left(c_{j} | \boldsymbol{x}\right)=1$，所以 R\left(c_{i} | \boldsymbol{x}\right)=1-P\left(c_{i} | \boldsymbol{x}\right)于是，最小化错误率的贝叶斯最优分类器为 h^{\star}(x)=\underset{c \in \mathcal{Y}}{\arg \min } R(c | \boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \min }(1-P(c | \boldsymbol{x}))=\underset{c \in \mathcal{Y}}{\arg \max } P(c | \boldsymbol{x})多元正态分布参数的极大似然估计已知对数似然函数为 L L\left(\boldsymbol{\theta}_{c}\right)=\sum_{\boldsymbol{x} \in D_{e}} \log P\left(\boldsymbol{x} | \boldsymbol{\theta}_ {c}\right)为了便于后续计算，我们令1og的底数为e，则对数似然函数可化为 L L\left(\boldsymbol{\theta}_{c}\right)=\sum_{\boldsymbol{x} \in D_{c}} \ln P\left(\boldsymbol{x} | \boldsymbol{\theta}_ {c}\right)由于$P\left(\boldsymbol{x} | \boldsymbol{\theta}_ {c}\right)=P(\boldsymbol{x} | c) \sim \mathcal{N}\left(\boldsymbol{\mu}_{c}, \boldsymbol{\sigma}_{c}^{2}\right)$，所以 P\left(\boldsymbol{x} | \boldsymbol{\theta}_{c}\right)=\frac{1}{\sqrt{(2 \pi)^{d}\left|\boldsymbol{\Sigma}_{c}\right|}} \exp \left(-\frac{1}{2}\left(\boldsymbol{x}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}_ {c}\right)\right)其中，d表示$\boldsymbol{x}$的维数，$\boldsymbol{\Sigma}_{c}=\boldsymbol{\sigma}_{c}^{2}$为对称正定协方差矩阵，$\left|\boldsymbol{\Sigma}_ {c}\right|$表示$\sum_{c}$的行列式，将上式代入对数似然函数可得 L L\left(\boldsymbol{\theta}_{c}\right)=\sum_{\boldsymbol{x} \in D_{e}} \ln \left[\frac{1}{\sqrt{(2 \pi)^{d}\left|\boldsymbol{\Sigma}_{c}\right|}} \exp \left(-\frac{1}{2}\left(\boldsymbol{x}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}_{c}\right)\right)\right]令$\left|D_{c}\right|=N$，则对数似然函数可化为 \begin{aligned} L L\left(\boldsymbol{\theta}_{c}\right) &=\sum_{i=1}^{N} \ln \left[\frac{1}{\sqrt{(2 \pi)^{d}\left|\boldsymbol{\Sigma}_{c}\right|}} \exp \left(-\frac{1}{2}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right)\right] \\ &=\sum_{i=1}^{N} \ln \left[\frac{1}{\sqrt{(2 \pi)^{d}}} \cdot \frac{1}{\sqrt{\left|\boldsymbol{\Sigma}_{c}\right|}} \cdot \exp \left(-\frac{1}{2}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right)\right] \\&=\sum_{i=1}^{N}\left\{\ln \frac{1}{\sqrt{(2 \pi)^{d}}}+\ln \frac{1}{\sqrt{\left|\Sigma_{c}\right|}}+\ln \left[\exp \left(-\frac{1}{2}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right)\right]\right\}\end{aligned} \begin{aligned} L L\left(\boldsymbol{\theta}_{c}\right) &=\sum_{i=1}^{N}\left\{-\frac{d}{2} \ln (2 \pi)-\frac{1}{2} \ln \left|\boldsymbol{\Sigma}_{c}\right|-\frac{1}{2}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right.\\ &=-\frac{N d}{2} \ln (2 \pi)-\frac{N}{2} \ln \left|\mathbf{\Sigma}_{c}\right|-\frac{1}{2} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right) \end{aligned}由于参数$\boldsymbol{\theta}_ {c}$的极大似然估$\hat{\boldsymbol{\theta}}_ {c}$为 \hat{\boldsymbol{\theta}}_{c}=\underset{\boldsymbol{\theta}_{c}}{\arg \max } L L\left(\boldsymbol{\theta}_ {c}\right)所以接来下只需要求出使得对数似然函数$L L\left(\boldsymbol{\theta}_ {c}\right)$取到最大值的$\hat{\boldsymbol{\mu}}_ {c}$和$\hat{\mathbf{\Sigma}}_ {c}$，也就求出了$\hat{\boldsymbol{\theta}}_{c}$ 对$L L\left(\boldsymbol{\theta}_ {c}\right)$关于$\boldsymbol{\mu}_ {c}$，求偏导 \begin{aligned} \frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\mu}_{c}} &=\frac{\partial}{\partial \boldsymbol{\mu}_{c}}\left[-\frac{N d}{2}-\ln (2 \pi)-\frac{N}{2} \ln \left|\mathbf{\Sigma}_{c}\right|-\frac{1}{2} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right] \\ &=\frac{\partial}{\partial \boldsymbol{\mu}_{c}}\left[-\frac{1}{2} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right] \\&=-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol \mu_{c}}\left[\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right] \\&=-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol{\mu}_{c}}\left[\left(\boldsymbol{x}_{i}^{T}-\boldsymbol{\mu}_{c}^{T} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right]\right. \\&=-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol\mu_{c}}\left[\left(\boldsymbol{x}_{i}^{T}-\boldsymbol{\mu}_{c}^{T}\right)\left(\boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}-\boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}\right)\right] \\&=-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol{\mu}_{c}}\left[\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}-\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}-\boldsymbol{\mu}_{c}^{T} \mathbf{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}+\boldsymbol{\mu}_{c}^{T} \mathbf{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}\right]\end{aligned}由于$\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_ {c}$的计算结果为标量，所以 \boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}=\left(\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}\right)^{T}=\boldsymbol{\mu}_{c}^{T}\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T} \boldsymbol{x}_{i}=\boldsymbol{\mu}_{c}^{T}\left(\boldsymbol{\Sigma}_{c}^{T}\right)^{-1} \boldsymbol{x}_{i}=\boldsymbol{\mu}_{c}^{T} \mathbf{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}于是上式可以进一步化为 \frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\mu}_{c}}=-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol{\mu}_{c}}\left[\boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}-2 \boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}+\boldsymbol{\mu}_{c}^{T} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}\right]由矩阵微分公式$\frac{\partial \boldsymbol{a}^{T} \boldsymbol{x}}{\partial \boldsymbol{x}}=\boldsymbol{a}, \frac{\partial \boldsymbol{x}^{T} \boldsymbol{B} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\boldsymbol{B}+\boldsymbol{B}^{T}\right) \boldsymbol{x}$可得 \begin{aligned} \frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\mu}_{c}} &=-\frac{1}{2} \sum_{i=1}^{N}\left[0-\left(2 \boldsymbol{x}_{i}^{T} \boldsymbol{\Sigma}_{c}^{-1}\right)^{T}+\left(\boldsymbol{\Sigma}_{c}^{-1}+\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T}\right) \boldsymbol{\mu}_{c}\right] \\ &=-\frac{1}{2} \sum_{i=1}^{N}\left[-\left(2\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T} \boldsymbol{x}_{i}\right)+\left(\boldsymbol{\Sigma}_{c}^{-1}+\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T}\right) \boldsymbol{\mu}_{c}\right] \\ &=-\frac{1}{2} \sum_{i=1}^{N}\left[-\left(2 \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}\right)+2 \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}\right] \\ &=\sum_{i=1}^{N} \Sigma_{c}^{-1} \boldsymbol{x}_{i}-N \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c} \end{aligned}令偏导数等于0可得 \begin{array}{c}{\frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\mu}_{c}}=\sum_{i=1}^{N} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}-N \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}=0} \\ {N \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}=\sum_{i=1}^{N} \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{x}_{i}} \\ {N \boldsymbol{\Sigma}_{c}^{-1} \boldsymbol{\mu}_{c}=\boldsymbol{\Sigma}_{c}^{-1} \sum_{i=1}^{N} \boldsymbol{x}_{i}} \\ {N \boldsymbol{\mu}_{c}=\sum_{i=1}^{N} \boldsymbol{x}_{i}} \\ \boldsymbol{\mu}_{c}=\frac{1}{N} \sum_{i=1}^{N} \boldsymbol{x}_{i} \Rightarrow \hat{\boldsymbol{\mu}}_{c}=\frac{1}{N} \sum_{i=1}^{N} \boldsymbol{x}_{i} \end{array}对$L L\left(\boldsymbol{\theta}_{c}\right)$关于$\Sigma_{c}$求偏导 \begin{aligned} \frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\Sigma}_{c}} &=\frac{\partial}{\partial \boldsymbol{\Sigma}_{c}}\left[-\frac{N d}{2} \ln (2 \pi)-\frac{N}{2} \ln \left|\boldsymbol{\Sigma}_{c}\right|-\frac{1}{2} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right] \\ &=\frac{\partial}{\partial \boldsymbol{\Sigma}_{c}}\left[-\frac{N}{2} \ln \left|\boldsymbol{\Sigma}_{c}\right|-\frac{1}{2} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right] \\&=-\frac{N}{2} \cdot \frac{\partial}{\partial \boldsymbol{\Sigma}_{c}}\left[\ln \left|\boldsymbol{\Sigma}_{c}\right|\right]-\frac{1}{2} \sum_{i=1}^{N} \frac{\partial}{\partial \boldsymbol{\Sigma}_{c}}\left[\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\right]\end{aligned}由矩阵微分公式$\frac{\partial|\mathbf{X}|}{\partial \mathbf{X}}=|\mathbf{X}| \cdot\left(\mathbf{X}^{-1}\right)^{T}, \frac{\partial \boldsymbol{a}^{T} \mathbf{X}^{-1} \boldsymbol{b}}{\partial \mathbf{X}}=-\mathbf{X}^{-T} \boldsymbol{a} \boldsymbol{b}^{T} \mathbf{X}^{-T}$可得 \begin{aligned} \frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\Sigma}_{c}} &=-\frac{N}{2} \cdot \frac{1}{\left|\boldsymbol{\Sigma}_{c}\right|} \cdot\left|\boldsymbol{\Sigma}_{c}\right| \cdot\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T}-\frac{1}{2} \sum_{i=1}^{N}\left[-\boldsymbol{\Sigma}_{c}^{-T}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-T}\right] \\ &=-\frac{N}{2} \cdot\left(\boldsymbol{\Sigma}_{c}^{-1}\right)^{T}-\frac{1}{2} \sum_{i=1}^{N}\left[-\boldsymbol{\Sigma}_{c}^{-T}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-T}\right] \\ &=-\frac{N}{2} \boldsymbol{\Sigma}_{c}^{-1}+\frac{1}{2} \sum_{i=1}^{N}\left[\boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-1}\right] \end{aligned}令偏导数等于0可得 \frac{\partial L L\left(\boldsymbol{\theta}_{c}\right)}{\partial \boldsymbol{\Sigma}_{c}}=-\frac{N}{2} \boldsymbol{\Sigma}_{c}^{-1}+\frac{1}{2} \sum_{i=1}^{N}\left[\boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-1}\right]=0$-\frac{N}{2} \boldsymbol{\Sigma}_{c}^{-1}=-\frac{1}{2} \sum_{i=1}^{N}\left[\boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-1}\right]$$N \boldsymbol{\Sigma}_{c}^{-1}=\sum_{i=1}^{N}\left[\boldsymbol{\Sigma}_{c}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \boldsymbol{\Sigma}_{c}^{-1}\right]$$N \boldsymbol{\Sigma}_{c}^{-1}=\boldsymbol{\Sigma}_{c}^{-1}\left[\sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T}\right] \boldsymbol{\Sigma}_{c}^{-1}$$N=\mathbf{\Sigma}_{c}^{-1}\left[\sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T}\right]$$\boldsymbol{\Sigma}_{c} N=\sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T}$$\mathbf{\Sigma}_{c}=\frac{1}{N} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T} \Rightarrow \hat{\boldsymbol{\Sigma}}_{c}=\frac{1}{N} \sum_{i=1}^{N}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right)^{T}$ 朴素贝叶斯分类器已知最小化分类错误率的贝叶斯最优分类器为 h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c | \boldsymbol{x})又由贝叶斯定理可知 P(c | \boldsymbol{x})=\frac{P(\boldsymbol{x}, c)}{P(\boldsymbol{x})}=\frac{P(c) P(\boldsymbol{x} | c)}{P(\boldsymbol{x})}所以 h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } \frac{P(c) P(\boldsymbol{x} | c)}{P(\boldsymbol{x})}=\underset{c \in \mathcal{Y}}{\arg \max } P(c) P(\boldsymbol{x} | c)已知属性条件独立性假设为 P(\boldsymbol{x} | c)=P\left(x_{1}, x_{2}, \ldots, x_{d} | c\right)=\prod_{i=1}^{d} P\left(x_{i} | c\right)所以 h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c) \prod_{i=1}^{d} P\left(x_{i} | c\right)此即为朴素贝叶斯分类器的表达式 对于P（c）：它表示的是样本空间中各类样本所占的比例，根据大数定律，当训练集包含充足的独立同分布样本时，P（c）可通过各类样本出现的频率来进行估计，也即 P(c)=\frac{\left|D_{c}\right|}{|D|}其中，D表示训练集，$|D|$表示D中的样本个数，$D_{c}$表示训练集D中第c类样本组成的集合，$\left|D_{c}\right|$表示集合$D_{c}$中的样本个数。 对于$P\left(x_{i} | c\right) :$若样本的第i个属性$x_{i}$取值为连续值我们假设该属性的取值服从正态分布，也即 P\left(x_{i} | c\right) \sim \mathcal{N}\left(\mu_{c, i}, \sigma_{c, i}^{2}\right) \Rightarrow P\left(x_{i} | c\right)=\frac{1}{\sqrt{2 \pi} \sigma_{c, i}} \exp \left(-\frac{\left(x_{i}-\mu_{c, i}\right)^{2}}{2 \sigma_{c, i}^{2}}\right)其中正态分布的参数可以用极大似然估计法推得：$\mu_{C, i}$和$\sigma_{c, i}^{2}$即为第c类样本在第i个属性上取值的均值和方差 对于$P\left(x_{i} | c\right) :$若样本的第i个属性$x_{i}$取值为离散值：同样根据极大似然估计法，我们用其频率值作为其概率值的估计值，也即 P\left(x_{i} | c\right)=\frac{\left|D_{c, x_{i}}\right|}{\left|D_{c}\right|}其中，$D_{c, x_{i}}$表示$D_{c}$中在第个属性上取值为$x_{i}$的样本组成的集合。 例：现将一枚6面骰子抛掷10次，抛掷出的点数分别为2、3、2、5、4、.6、1、3、4、2，试基于此抛掷结果估计这枚骰子抛掷出各个点数的概率。解：设这枚骰子抛掷出点数的概率为$P_{i}$，根据极大似然估计法可以写出似然函数为 L(\theta)=P_{1} \times P_{2}^{3} \times P_{3}^{2} \times P_{4}^{2} \times P_{5} \times P_{6}其对数似然函数即为 \begin{aligned} L(\theta) & :=\ln L(\theta)=\ln \left(P_{1} \times P_{2}^{3} \times P_{3}^{2} \times P_{4}^{2} \times P_{5} \times P_{6}\right) \\ &=\ln P_{1}+3 \ln P_{2}+2 \ln P_{3}+2 \ln P_{4}+\ln P_{5}+\ln P_{6} \end{aligned}由于$P_{i}$之间满足如下约束 P_{1}+P_{2}+P_{3}+P_{4}+P_{5}+P_{6}=1所以此时最大化对数似然函数属于带约束的最优化问题，也即 \begin{array}{ll}{\underset{\theta}{max}} & {L(\theta)=\ln P_{1}+3 \ln P_{2}+2 \ln P_{3}+2 \ln P_{4}+\ln P_{5}+\ln P_{6}} \\ {\text {s.t.}} & {P_{1}+P_{2}+P_{3}+P_{4}+P_{5}+P_{6}=1}\end{array}由拉格朗日乘子法可得拉格拉格朗日函数为 \mathcal{L}(\theta)=\ln P_{1}+3 \ln P_{2}+2 \ln P_{3}+2 \ln P_{4}+\ln P_{5}+\ln P_{6}+\lambda\left(P_{1}+P_{2}+P_{3}+P_{4}+P_{5}+P_{6}-1\right)对拉格朗日函数$\mathcal{L}(\theta)$分别关于$P_{i}$求偏导，然后令其等于0可得 \begin{aligned}\frac{\partial \mathcal{L}(\theta)}{\partial P_{1}}&=\frac{\partial}{\partial P_{1}}\left[\ln P_{1}+3 \ln P_{2}+2 \ln P_{3}+2 \ln P_{4}+\ln P_{5}+\ln P_{6}+\lambda\left(P_{1}+P_{2}+P_{3}+P_{4}+P_{5}+P_{6}-1\right)\right]=0\\&=\frac{\partial}{\partial P_{1}}\left(\ln P_{1}+\lambda P_{1}\right)=0\\&=\frac{1}{P_{1}}+\lambda=0\\&\Rightarrow \lambda=-\frac{1}{P_{1}}\end{aligned}同理可求得： \lambda=-\frac{1}{P_{1}}=-\frac{3}{P_{2}}=-\frac{2}{P_{3}}=-\frac{2}{P_{4}}=-\frac{1}{P_{5}}=-\frac{1}{P_{6}}又因为 P_{1}+P_{2}+P_{3}+P_{4}+P_{5}+P_{6}=1所以最终解得 P_{1}=\frac{1}{10}, P_{2}=\frac{3}{10}, P_{3}=\frac{2}{10}, P_{4}=\frac{2}{10}, P_{5}=\frac{1}{10}, P_{6}=\frac{1}{10}此时抛掷出各个点数的概率值与其频率值相等 EM算法1.EM算法的引入为什么需要EM算法概率模型有时既含有观测变量，又含有隐变量或者潜在变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或者贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法。 2.EM算法的例子三硬币模型《统计学习方法》例9.1（三硬币模型）：假设有3枚硬币，分别记作A，B，c.这些硬币正面出现的概率分别 T，р和q。进行如下掷硬币试验：先掷硬币A，根据其结果选出硬币B或硬币C，正面选硬币B，反面选硬币C；然后掷选出的硬币，掷硬币的结果，出现E面记作1，出现反面记作0；独立地重复n此实验（这里，n=10），观测结果如下：1，1，0，1，0，0，1，0，1，1假设只能观测到掷硬币的结果，不能观测掷硬币的过程。问如何估计三硬币正面出现的概率，即三硬币模型的参数。 解：对每一次试验可以进行如下建模 \begin{aligned} P(y | \theta) &=\sum_{z} P(y, z | \theta)=\sum_{z} P(z | \theta) P(y | z, \theta) \\ &=P(z=1 | \theta) P(y | z=1, \theta)+P(z=0 | \theta) P(y | z=0, \theta) \\ &=\left\{\begin{array}{ll}{\pi p+(1-\pi) q,} & {\text { if } y=1} \\ {\pi(1-p)+(1-\pi)(1-q),} & {\text { if } y=0} \end{array}\right.\\ &=\pi p^{y}(1-p)^{1-y}+(1-\pi) q^{y}(1-q)^{1-y}\end{aligned}其中，随机变量y是观测变量，表示一次试验观测的结果是1或0；随机变量z是隐变量，表示未观测到的掷硬币A的结果；$\theta=(\pi, p, q)$是模型参数。将观测数据表示为$Y=\left(Y_{1}, Y_{2}, \ldots, Y_{n}\right)^{T}$，未观测数据表示为$Z=\left(Z_{1}, Z_{2}, \ldots, Z_{n}\right)^{T}$则观测数据的似然函数为： P(Y | \theta)=\sum_{Z} P(Z | \theta) P(Y | Z, \theta)=\prod_{j=1}^{n} P\left(y_{j} | \theta\right)=\prod_{j=1}^{n}\left[\pi p_{j}^{y_{j}}(1-p)^{1-y_{j}}+(1-\pi) q^{y_{j}}(1-q)^{1-y_{j}}\right]考虑求模型参数$\theta=(\pi, p, q)$的极大似然估计，即使用对数似然函数来进行参数估计可得： \begin{aligned} \hat{\theta} &=\arg \max _{\theta} \ln P(Y | \theta) \\ &=\arg \max _{\theta} \ln \prod_{j=1}^{n}\left[\pi p^{y_{j}}(1-p)^{1-y_{j}}+(1-\pi) q^{y_{j}}(1-q)^{1-y_{j}}\right] \\ &=\arg \max _{\theta} \sum_{j=1}^{n} \ln \left[\pi p^{y_{j}}(1-p)^{1-y_{j}}+(1-\pi) q^{y_{j}}(1-q)^{i-y_{j}}\right] \end{aligned}上式没有解析解，也就是没办法直接解出$\pi, p, q$恰好等于某个常数，只能用送代的方法来进行求解。 3.EM算法的导出Jensen（琴生）不等式若f是凸函数，则： f\left(t x_{1}+(1-t) x_{2}\right) \leq t f\left(x_{1}\right)+(1-t) f\left(x_{2}\right)ps:$t x_{1}+(1-t) x_{2}$ 代表取遍$x_{1}$和$x_{2}$之间所有的点其中，$t \in[0,1]$。同理，若f是凹函数，则只需将上式中的$\leq$换成$\geq$即可。将上式中的t推广到n个同样成立，也即： f\left(t_{1} x_{1}+t_{2} x_{2}+\ldots+t_{n} x_{n}\right) \leq t_{1} f\left(x_{1}\right)+t_{2} f\left(x_{2}\right)+\ldots+t_{n} f\left(x_{n}\right)其中，$t_{1}, t_{2}, \dots, t_{n} \in[0,1], t_{1}+t_{2}+\ldots+t_{n}=1$。在概率论中常以以下形式出现： \varphi(E[X]) \leq E[\varphi(X)]其中，$X$是随机变量，$\varphi$是凸函数，$E[X]$表示X的期望。 EM算法的推导我们面对一个含有隐变量的概率模型，目标是极大化观测数据Y关于参数日的对数似然函数，即极大化： L(\theta)=\ln P(Y | \theta)=\ln \sum_{Z} P(Y, Z | \theta)=\ln \left(\sum_{Z} P(Y | Z, \theta) P(Z | \theta)\right)注意到这一极大化的主要困难是上式中有未观测数据Z并有包含和（Z为离散型时）或者积分（Z为连续型时）的对数。EM算法采用的是通过迭代逐步近似极大化$L(\theta)$：假设在第次迭代后$\theta$的估计值$\theta^{(i)}$，我们希望新的估计值$\theta$能使 $L(\theta)$增加，即$L(\theta)&gt;L\left(\theta^{(i)}\right)$，并逐步达到极大值。为此，我们考虑两者的差： \begin{aligned} L(\theta)-L\left(\theta^{(i)}\right) &=\ln \left(\sum_{Z} P(Y | Z, \theta) P(Z | \theta)\right)-\ln P\left(Y | \theta^{(i)}\right) \\ &=\ln \left(\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right)}\right)-\ln P\left(Y | \theta^{(i)}\right) \end{aligned}套用琴生不等式可得： \geq \sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right)}-\ln P\left(Y | \theta^{(i)}\right) \begin{aligned} L(\theta)-L\left(\theta^{(i)}\right) & \geq \sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right)}-\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \cdot \ln P\left(Y | \theta^{(i)}\right) \\ &=\sum_{Z} P\left(Z | Y, \theta^{(i)}\right)\left(\ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right)}-\ln P\left(Y | \theta^{(i)}\right)\right) \\ &=\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right) P\left(Y | \theta^{(i)}\right)} \end{aligned} L(\theta) \geq L\left(\theta^{(i)}\right)+\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right) P\left(Y | \theta^{(i)}\right)}令： B\left(\theta, \theta^{(i)}\right)=L\left(\theta^{(i)}\right)+\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right) P\left(Y | \theta^{(i)}\right)}则： L(\theta) \geq B\left(\theta, \theta^{(i)}\right)即函数$B\left(\theta, \theta^{(i)}\right)$是$L(\theta)$的一个下界，此时若设$\theta^{(i+1)}$使得$B\left(\theta, \theta^{(i)}\right)$达到极大也即 B\left(\theta^{(i+1)}, \theta^{(i)}\right) \geq B\left(\theta^{(i)}, \theta^{(i)}\right)由于$B\left(\theta^{(i)}, \theta^{(i)}\right)=L\left(\theta^{(i)}\right)$，所以可以进一步推得： L\left(\theta^{(i+1)}\right) \geq B\left(\theta^{(i+1)}, \theta^{(i)}\right) \geq B\left(\theta^{(i)}, \theta^{(i)}\right)=L\left(\theta^{(i)}\right) L\left(\theta^{(i+1)}\right) \geq L\left(\theta^{(i)}\right)因此，任何可以使$B\left(\theta, \theta^{(i)}\right)$增大的$\theta$，也可以使$L(\theta)$增大，于是问题就转化为了求解能使得$B\left(\theta, \theta^{(i)}\right)$达到极大的$\theta^{(i+1)}$，即 \begin{aligned} \theta^{(i+1)} &=\underset{\theta}{\arg \max } B\left(\theta, \theta^{(i)}\right) \\ &=\underset{\theta}{\arg \max }\left(L\left(\partial^{(i)}\right)+\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln \frac{P(Y | Z, \theta) P(Z | \theta)}{P\left(Z | Y, \theta^{(i)}\right) P\left(Y | \theta^{(i)}\right)}\right) \\ &=\arg \max _{\theta}\left(\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln (P(Y | Z, \theta) P(Z | \theta))\right) \\ &=\underset{\theta}{\arg \max }\left(\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln P(Y, Z | \theta)\right) \\ &=\underset{\theta}{\arg \max } Q\left(\theta, \theta^{(i)}\right) \end{aligned}到此即完成了EM算法的一次迭代，求出的$\theta^{(i+1)}$作为下一次迭代的初始$\theta(i)$。综上可以总结出EM算法的”E步”和”M步”分别为：E步：计算完全数据的对数似然函数$\ln P(Y, Z | \theta)$关于在给定观测数据Y和当前参数$\theta$下对未观测数据Z的条件概率分布$P\left(Z | Y, \theta^{(i)}\right)$期望，即Q函数： Q\left(\theta, \theta^{(i)}\right)=E_{Z}\left[\ln P(Y, Z | \theta) | Y, \theta^{(i)}\right]=\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln P(Y, Z | \theta)M步：求使得Q函数到达极大的$\theta^{(i+1)}$。 4.EM算法求解例子用EM算法求解三硬币模型《统计学习方法》例9.1（三硬币模型）：假设有3枚硬币，分别记作A，B，c.这些硬币正面出现的概率分别 T，р和q。进行如下掷硬币试验：先掷硬币A，根据其结果选出硬币B或硬币C，正面选硬币B，反面选硬币C；然后掷选出的硬币，掷硬币的结果，出现E面记作1，出现反面记作0；独立地重复n此实验（这里，n=10），观测结果如下：1，1，0，1，0，0，1，0，1，1假设只能观测到掷硬币的结果，不能观测掷硬币的过程。问如何估计三硬币正面出现的概率，即三硬币模型的参数。 解：对每一次试验可以进行如下建模 \begin{aligned} P(y | \theta) &=\sum_{z} P(y, z | \theta)=\sum_{z} P(z | \theta) P(y | z, \theta) \\ &=P(z=1 | \theta) P(y | z=1, \theta)+P(z=0 | \theta) P(y | z=0, \theta) \\ &=\left\{\begin{array}{ll}{\pi p+(1-\pi) q,} & {\text { if } y=1} \\ {\pi(1-p)+(1-\pi)(1-q),} & {\text { if } y=0} \end{array}\right.\\ &=\pi p^{y}(1-p)^{1-y}+(1-\pi) q^{y}(1-q)^{1-y}\end{aligned}其中，随机变量y是观测变量，表示一次试验观测的结果是1或0；随机变量z是隐变量，表示未观测到的掷硬币A的结果；$\theta=(\pi, p, q)$是模型参数。 求解思路：E步：导出Q函数M步：求使得Q函数达到极大的$\theta^{(i+1)}=\left(\pi^{(i+1)}, p^{(i+1)}, q^{(i+1)}\right)$ E步：导出Q函数 \begin{aligned} Q\left(\theta | \theta^{(i)}\right) &=\sum_{Z} P\left(Z | Y, \theta^{(i)}\right) \ln P(Y, Z | \theta) \\ &=\sum_{z_{1}, z_{2}, \ldots, z_{N}}\left\{\prod_{i=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right) \ln \left[\prod_{j=1}^{N} P\left(y_{j}, z_{j} | \theta\right)\right]\right\} \\ &=\sum_{z_{1}, z_{2}, \ldots, z_{N}}\left\{\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right)\left[\sum_{j=1}^{N} \ln P\left(y_{j}, z_{j} | \theta\right)\right]\right\} \\ &=\sum_{z_{1}, z_{2}, \ldots, z_{N}}\left\{\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right)\left[\ln P\left(y_{1}, z_{1} | \theta\right)+\sum_{j=2}^{N} \ln P\left(y_{j}, z_{j} | \theta\right)\right]\right\} \end{aligned} \begin{aligned} Q\left(\theta | \theta^{(i)}\right) &=\sum_{z_{i}, z_{2}, \ldots, z_{N}}\left\{\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right) \cdot \ln P\left(y_{1}, z_{1} | \theta\right)+\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right)\left[\sum_{j=2}^{N} \ln P\left(y_{j}, z_{j} | \theta\right)\right]\right\} \\ &=\sum_{z_{1}, z_{2}, \ldots, z_{N}}\left\{\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right) \cdot \ln P\left(y_{1}, z_{1} | \theta\right)\right\}+\sum_{z_{i}, z_{2}, \ldots, z_{N}}^{N}\left\{\prod_{j=1}^{N} P\left(z_{j} | y_{j}, \theta^{(i)}\right)\left[\sum_{j=2}^{N} \ln P\left(y_{j}, z_{j} | \theta\right)\right]\right\} \end{aligned}]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习》第6章 支持向量模型 公式推导]]></title>
    <url>%2F2019%2F07%2F12%2F%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC6%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%20%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[第6章 支持向量模型支持向量机公式推导SVM问题的原始形式推导1. 常见的几何性质（欧氏空间） \left\{\begin{array}{l}{w^{T} x_{1}+b=0} \\ {w^{T} x_{2}+b=0} \\ {w^{T}\left(x_{1}-x_{2}\right)=0}\end{array}\right.$w$是法向量 \left\{\begin{array}{l}{w^{T} x_{1}+b=0} \\ {\lambda w^{T} \frac{w}{\|w\|}+b=0} \\ {\text { offset }} =\frac{-b}{\|w\|}\end{array}\right.$\frac{-b}{|\boldsymbol{w}|}$是原点到平面的”距离” r=\frac{\left|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right|}{\|\boldsymbol{w}\|}r是点到平面的距离 \begin{aligned} r &=\left\|\frac{\boldsymbol{w}^{T} \boldsymbol{x}}{\|\boldsymbol{w}\|^{2}} \boldsymbol{w}-\frac{-b}{\|\boldsymbol{w}\|^{2}} \boldsymbol{w}\right\| \\ &=\left\|\frac{\boldsymbol{w}^{T} \boldsymbol{x}}{\|\boldsymbol{w}\|^{2}}+\frac{b}{\|\boldsymbol{w}\|^{2}}\right\|\|\boldsymbol{w}\| \\ &=\frac{\left\|\boldsymbol{w}^{T} \boldsymbol{x}+b\right\|}{\|\boldsymbol{w}\|} \\ &=\frac{\left|\boldsymbol{w}^{T} \boldsymbol{x}+b\right|}{\|\boldsymbol{w}\|} \end{aligned}SVM原始公式的导出(核心是最大化间隔)（1）任一数据点到平面距离（假设w可以分开数据集）: r_{i}=\frac{y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)}{\|\boldsymbol{w}\|}（2）最接近平面的距离 d=\min _{i} y_{i}\left(\frac{\boldsymbol{w}^{T}}{\|\boldsymbol{w}\|} \boldsymbol{x}_{i}+\frac{b}{\|\boldsymbol{w}\|}\right)（3）所有可能距离中，最大的距离。是间隔的一半 \left\{\begin{array}{l}{d^{\star}=\max _{w, b} \min _{i} y_{i}\left(\frac{\boldsymbol{w}^{T}}{\|\boldsymbol{w}\|} \boldsymbol{x}_{i}+\frac{b}{\|\boldsymbol{w}\|}\right)} \\ {\gamma=2 d^{\star}=\max _{\boldsymbol{w}, b} \min _{i} 2 \times \boldsymbol{y}_{i}\left(\frac{\boldsymbol{w}^{T}}{\|\boldsymbol{w}\|} \boldsymbol{x}_{i}+\frac{b}{\|\boldsymbol{w}\|}\right)}\end{array}\right. \begin{array}{l}{\max _{w, b} 2 d} \\ {\text { s.t. } y_{i}\left(\frac{w^{T}}{\|w\|} x_{i}+\frac{b}{\|\boldsymbol{w}\|}\right) \geq d \text { for } i=1, \ldots, m} \\ {d>0}\end{array}引入新的定义：函数间隔$\hat{d}=|\boldsymbol{w}| d$ \begin{array}{l}{\max _{\boldsymbol{w}, b} \frac{2 \hat{d}}{\|\boldsymbol{w}\|}} \\ {\text { s.t. } \boldsymbol{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq \hat{d} \text { for } i=1, \ldots, N} \\ {\hat{d}>0}\end{array}实际上，$\hat{d}$取值并不影响最优化问题的解！所以我们可以始终取$\hat{d}$=1 \begin{array}{ll}{\max _{w, b}} & {\frac{2}{\|w\|}} \\ {\text { s.t. }} & {y_{i}\left(w^{T} x_{i}+b\right) \geq 1, \quad i=1,2, \dots, m}\end{array} \begin{array}{ll}{\min _{w, b}} & {\frac{1}{2}\|w\|^{2}} \\ {\text { s.t. }} & {y_{i}\left(w^{T} x_{i}+b\right) \geq 1, \quad i=1,2, \ldots, m}\end{array}SVM的性质① 最大间隔超平面的存在唯一性② 支持向量和间隔边界 存在性易得，现证唯一性：假设有两个最优解$\left(\boldsymbol{w}_{1}^{\star}, b_{1}^{\star}\right),\left(\boldsymbol{w}_{2}^{\star}, b_{2}^{\star}\right)$1）证$\boldsymbol{w}_{1}^{\star}=\boldsymbol{w}_{2}^{\star}$根据假设，有$\left|\boldsymbol{w}_{1}^{\star}\right|=\left|\boldsymbol{w}_{2}^{\star}\right|=c \neq 0$,假设$w=\frac{w_{1}^{\star}+w_{2}^{\star}}{2}, b=\frac{b_{1}^{\star}+b_{2}^{\star}}{2}$$c \leq| | w| |=\frac{\left|w_{i}^{\star}+w_{2}^{\star}\right|}{2} \leq \frac{ | w_{i}^{\star}|+| w_{2}^{\star} |}{2}=c \Rightarrow | w |\frac{\left|w_{1}^{\star}+w_{2}^{\star}\right|}{2}=\frac{\left|w_{1}^{\star}\right|+\left|w_{2}^{\star}\right|}{2}\Rightarrow \quad w_{1}^{\star}=k w_{2}^{\star}$$\Rightarrow k=\left\{\begin{array}{l}{1} \\ {-1}\end{array} \Rightarrow w=\left\{\begin{array}{ll}{w_{i}^{\star}=w_{i}^{\star}} &amp; {\text { if } k=1} \\ {0} &amp; {\text { if } k=-1}\end{array} \Rightarrow w_{1}^{\star}=w_{2}^{\star}\right.\right.$2）证$b_{1}^{\star}=b_{2}^{\star}$，设$x_{1}^{\prime}, x_{2}^{\prime} \in\left\{x_{i} | y_{i}=+1\right\} \quad x_{1}^{\prime \prime}, x_{2}^{\prime \prime} \in\left\{x_{i} | y_{i}=-1\right\}$，且有$\left\{\begin{array}{l}{\left(w^{T} x_{1}^{\prime}+b_{1}^{\star}\right)=1} \\ {\left(w^{T} x_{2}^{\prime}+b_{2}^{\star}\right)=1} \\ {\left(w^{T} x_{1}^{\prime \prime}+b_{1}^{\star}\right)=-1} \\ {\left(w^{T} x_{2}^{\prime \prime}+b_{2}^{\star}\right)=-1}\end{array}\right.$ $\left\{\begin{array}{l}{b_{1}^{\star}=-\frac{1}{2} w^{T}\left(\boldsymbol{x}_{1}^{\prime}+\boldsymbol{x}_{1}^{\prime \prime}\right)} \\ {b_{2}^{\star}=-\frac{1}{2} \boldsymbol{w}^{T}\left(\boldsymbol{x}_{2}^{\prime}+\boldsymbol{x}_{2}^{\prime \prime}\right)}\end{array} \Rightarrow b_{1}^{\star}-b_{2}^{\star}=-\frac{1}{2}\left[\boldsymbol{w}^{T}\left(\boldsymbol{x}_{1}^{\prime}-\boldsymbol{x}_{2}^{\prime}\right)+\boldsymbol{w}^{T}\left(\boldsymbol{x}_{1}^{\prime \prime}-\boldsymbol{x}_{2}^{\prime \prime}\right)\right]\right.$ $\left\{\begin{array}{1}{\left(w^{T} x_{2}^{\prime}+b_{1}^{\star}\right)\geq 1=w^{T} x_{1}^{\prime}+b_{1}^{\star}} \\ {\left(w^{T} x_{1}^{\prime}+b_{2}^{\star}\right)\geq 1=w^{T} x_{2}^{\prime}+b_{2}^{\star}} \\ {\left(w^{T} x_{2}^{\prime \prime}+b_{1}^{\star}\right)\geq 1=w^{T} x_{1}^{\prime \prime}+b_{1}^{\star}} \\ {\left(w^{T} x_{1}^{\prime \prime}+b_{2}^{\star}\right)\geq 1=w^{T} x_{2}^{\prime \prime}+b_{2}^{\star}}\end{array}\right.\Rightarrow\left\{\begin{array}{l}{\boldsymbol{w}^{T}\left(\boldsymbol{x}_{1}^{\prime}-\boldsymbol{x}_{2}^{\prime}\right)=0} \\ {\boldsymbol{w}^{T}\left(\boldsymbol{x}_{1}^{\prime \prime}-\boldsymbol{x}_{2}^{\prime \prime}\right)=0}\end{array} \Rightarrow b_{1}^{\star}=b_{2}^{\star}\right.$ SVM问题的对偶形式推导1. 拉格朗日函数的介绍优化问题的一般形式 \begin{array}{ll}{\min _{x}} & {f_{0}(x)} \\ {\text { s.t. }} & {f_{i}(x) \leq 0, \quad i=1, \ldots, m} \\ {} & {h_{i}(x)=0, \quad i=1, \ldots, p}\end{array} \begin{array}{cl}{\min _{x}} & {f_{0}(x)} \\ {\text { s.t. }} & {f_{i}(x) \geq 0, \quad i=1, \ldots, m} \\ {} & {h_{i}(x)=0, \quad i=1, \ldots, p}\end{array} \begin{array}{ll}{\max _{x}} & {f_{0}(x)} \\ {\text { s.t. }} & {f_{i}(x) \leq 0, \quad i=1, \ldots, m} \\ {} & {h_{i}(x)=0, \quad i=1, \ldots, p}\end{array}概念：拉格朗日函数是拉格朗日乘子法的核心部分，所以当我们看到拉格朗日函数时，不假思素地将其对应到拉格朗日乘子法上核心：拉格朗日乘子法，其思想就是把有约束优化问题转变为等价的无约束优化问题（形式上） 拉格朗日函数与原始问题的关系： \begin{array}{ll}{\min _{x}} & {f_{0}(x)} \\ {\text { s.t. }} & {f_{i}(x) \leq 0, \quad i=1, \ldots, m} \\ {} & {h_{i}(x)=0, \quad i=1, \ldots, p}\end{array} \begin{array}{c}{\mathcal{L}(x, \lambda, \nu)=f_{0}(x)+\sum_{i=1}^{m} \lambda_{i} f_{i}(x)+\sum_{i=1}^{p} \nu_{i} h_{i}(x)} \\ {\text { s.t. } \quad \lambda_{i} \geq 0, \quad i=1, \ldots, m}\end{array} \begin{array}{rl}{\min _{x} \max _{\lambda, v}} & {\mathcal{L}(x, \lambda, \nu)} \\ {\text { s.t. }} & {\lambda_{i} \geq 0, \quad i=1, \ldots, m}\end{array}证明：$\begin{array}{ll}{\min _{x}} &amp; {f_{0}(x)} \\ {\text { s.t. }} &amp; {f_{i}(x) \leq 0, \quad i=1, \ldots, m} \\ {} &amp; {h_{i}(x)=0, \quad i=1, \ldots, p}\end{array}\Leftrightarrow\begin{array}{rl}{\min _{x} \max _{\lambda, v}} &amp; {\mathcal{L}(x, \lambda, \nu)} \\ {\text { s.t. }} &amp; {\lambda_{i} \geq 0, \quad i=1, \ldots, m}\end{array}$记：$\theta_{p}(x)=\max _{\lambda, v} \mathcal{L}(x, \lambda, \nu)$s.t. $\quad \lambda_{i} \geq 0, \quad i=1, \ldots, m$则：$\theta_{P}(x)=\left\{\begin{array}{ll}{f_{0}(x)} &amp; {\text { for } x \text { that satisfied the origin constraint }} \\ {+\infty} &amp; {\text { otherwise }}\end{array}\right.$验证上述性质：①若存在$\boldsymbol{x}$使得某个$f_{i}(x)&gt;0$则我们可令$0 \leq \lambda_{i} \rightarrow+\infty$，进而有$\theta_{p}(x)=+\infty$②若存在$\boldsymbol{x}$使得某个$h_{i}(x) \neq 0$则我们可令$v_{i} h_{i}(x) \rightarrow+\infty$，进而有$\theta_{p}(x)=+\infty$③若$x \in\left\{x | \forall i, v_{i}, \lambda_{i} \geq 0, \lambda_{i} f_{i}(x) \leq 0, v_{i} h_{i}(x)=0\right\}$，则有$\max _{\lambda, v} \mathcal{L}(x, \lambda, \nu)=f_{0}(x)$ 拉格朗日乘子法在SVM问题上的应用原始形式：$\begin{array}{ll}{\min _{\boldsymbol{w}, b}} &amp; {\frac{1}{2}|\boldsymbol{w}|^{2}} \\ {\text { s.t. }} &amp; {\boldsymbol{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq 1, \quad i=1,2, \ldots, m}\end{array}\leftrightarrow\begin{array}{ll}{\min _{\boldsymbol{w}, b}} &amp; {\frac{1}{2}|\boldsymbol{w}|^{2}} \\ {\text { s.t. }} &amp; {1-\boldsymbol{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \leq 0, \quad i=1,2, \ldots, m}\end{array}$ 拉格朗日函数：$\mathcal{L}(\boldsymbol{w}, b, \alpha)=\frac{1}{2}|\boldsymbol{w}|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-\boldsymbol{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right)$s.t. $\quad \alpha_{i} \geq 0 \quad$ for $i=1, \ldots, m$ 等价问题：$\begin{array}{cl}{\min _{\boldsymbol{w}, b} \max _{\alpha}} &amp; {\mathcal{L}(\boldsymbol{w}, b, \alpha)} \\ {\text { s.t. }} &amp; {\alpha_{i} \geq 0 \text { for } i=1, \ldots, m}\end{array}$ 2. “对偶”概念的介绍进一步求解SVM的拉格朗日对偶问题$\begin{array}{cl}{\max _{\alpha} \min _{\alpha}} &amp; {\frac{1}{2}|\boldsymbol{w}|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-\boldsymbol{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right)} \\ {\text { s.t. }} &amp; {\alpha_{i} \geq 0 \quad \text { for } i=1, \ldots, m}\end{array}$ ①必要条件$\frac{\partial \mathcal{L}}{\partial w}=0 \Rightarrow w=\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}$$\frac{\partial \mathcal{L}}{\partial b}=0 \Rightarrow \sum_{i=1}^{m} \alpha_{i} y_{i}=0$②回代得式$\max _{a} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}$s.t. $\sum_{i=1}^{m} \alpha_{i} y_{i}=0$ $\begin{aligned} \frac{\partial \mathcal{L}}{\partial \boldsymbol {w}}=0 &amp;=\frac{\partial}{\partial \boldsymbol{w}}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}-\sum_{i=1}^{m} \alpha_{i} y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right) \\ &amp;=\frac{\partial}{\partial \boldsymbol{w}}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}-\sum_{i=1}^{m} \alpha_{i} {y}_{i} \boldsymbol{w}^{T} \boldsymbol{x}_{i}-\sum_{i=1}^{m} \alpha_{i} {y}_{i} \boldsymbol{x}_{i} b\right) \\&amp;=\frac{\partial}{\partial \boldsymbol{w}}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}-\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{w}^{T} x_{i}\right)\\&amp;=\frac{\partial}{\partial \boldsymbol{w}}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}-\boldsymbol{w}^{T} \sum_{i=1}^{m} \alpha_{i} {y}_{i} \boldsymbol{x}_{i}\right)\\&amp;=\boldsymbol{w}-\sum_{i=1}^{m} a_{i} y_{i} \boldsymbol{x}_{i}\end{aligned}$ $\begin{aligned} \frac{\partial \mathcal{L}}{\partial b}=0&amp;=\frac{\partial}{\partial b}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}-\sum_{i=1}^{m} \alpha_{i} y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right)\\&amp;=\frac{\partial}{\partial b}\left(\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}-\sum_{k=1}^{m} \alpha_{i} y_{i} \boldsymbol{w}^{T} \boldsymbol{x}_{i}-\sum_{i=1}^{m} \alpha_{i} y_{i} b\right)\\&amp;=\frac{\partial}{\partial b}\left(-\sum_{i=1}^{m} \alpha_{i} y_{i} b\right)\\&amp;=\frac{\partial}{\partial b}\left(-b \sum_{i=1}^{m} \alpha_{i}{y}_{i}\right)\\&amp;=-\sum_{i=1}^{m} \alpha_{i} y_{i}\end{aligned}$ $\begin{aligned} \boldsymbol{w} &amp;=\sum_{i=1}^{m} \alpha_{i} \boldsymbol{y}_{i} \boldsymbol{x}_{i} \\ 0 &amp;=\sum_{i=1}^{m} \alpha_{i} \boldsymbol{y}_{i} \end{aligned}$ $\begin{aligned} \min _{w, b} \mathcal{L}(\boldsymbol{w}, b, \alpha) &amp;=\frac{1}{2}|\boldsymbol{w}|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-{y}_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right) \\ &amp;=\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}-\boldsymbol{w}^{T}\left(\sum_{i=1}^{m} \alpha_{i} {y}_{i} \boldsymbol{x}_{i}\right)-b\left(\sum_{i=1}^{m} \alpha_{i} {y}_{i}\right)+\left(\sum_{i=1}^{m} \alpha_{i}\right)\\&amp;=\frac{1}{2} \boldsymbol{w}^{T}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\right)-\boldsymbol{w}^{T}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\right)-b \cdot 0+\left(\sum_{i=1}^{m} \alpha_{i}\right) \\&amp;=\left(\sum_{i=1}^{m} \alpha_{i}\right)+\left(\frac{1}{2} \boldsymbol{w}^{T}-\boldsymbol{w}^{T}\right)\left(\sum_{i=1}^{m} \alpha_{i} {y}_{i} \boldsymbol{x}_{i}\right)\\&amp;=\left(\sum_{i=1}^{m} \alpha_{i}\right)-\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\right)^{T}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\right)\\&amp;=\left(\sum_{i=1}^{m} \alpha_{i}\right)-\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{T}\right)\left(\sum_{j=1}^{m} \alpha_{j} y_{j} \boldsymbol{x}_{j}\right)\\&amp;=\left(\sum_{i=1}^{m} \alpha_{i}\right)-\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{T}\left(\sum_{j=1}^{m} \alpha_{j} y_{j} \boldsymbol{x}_{j}\right)\right)\\&amp;=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{T} \boldsymbol{x}_{j}\\&amp;=\mathcal{L}(\alpha)\end{aligned}$ 3. KKT条件的介绍用途：KKT条件就是一组方程，用于部分最优化问题的求解。优化问题：$\left\{\begin{aligned} \text { optimize } &amp; f(x) \\ \text { s.t. } &amp; g_{i}(x) \leq 0 \\ &amp; h_{j}(x)=0 \end{aligned}\right.$ $\Rightarrow$ KKT条件 $\left\{\begin{aligned} \nabla_{x} \mathcal{L} &amp;=0 &amp;\text{(stationary equation)}\\ g_{j}(\boldsymbol{x}) &amp;=0, \quad j=1, \ldots, m &amp;\text{(primal feasibility)}\\ h_{k}(x) &amp; \leq 0 &amp;\text{(primal feasibility)}\\ \lambda_{k} &amp; \geq 0 &amp;\text{(dual feasibility)}\\ \lambda_{k} h_{k}(\boldsymbol{x}) &amp;=0, \quad k=1, \ldots, p &amp;\text{(complementary slackness)}\end{aligned}\right.$ 使用方法：①假设待优化的的函数为$f : \mathbb{R}^{n} \rightarrow \mathbb{R}$，约束为$y_{i} : \mathbb{R}^{n} \rightarrow \mathbb{R}, h_{i} : \mathbb{R}^{n} \rightarrow \mathbb{R}$，并且优化问题满足regularity condition，那么，若$\mathcal{x}^{\star}$是局部最优值，其必然满足KKT条件必要性）②假设待优化的的函数$f(x)$和不等式约束$g_{i}(x)$是凸函数，等式约束$h_{i}(x)$是仿射函数，且不等式约束等号能够成立（严格可行）。那么满足KKT条件的x点就是全局最优（充要条件）。 理解KKT条件：考虑不等式约束：①解在边界上②解在非边界上考虑等式+不等式约束：①等式约束②不等式约束（解在边界上）③不等式约束（解非边界上） SVM问题的求解算法（SMO）1. SMO算法的思路讲解重点：①每次迭代过程仅优化两个参数，有闭式解②启发式寻找每次优化的两个参数，有效减少迭代次数思路：①设置$\alpha$列表，并设其初值为0（每个数据点对应一个$\alpha_{i}$）②选取两个待优化变量（为了方便，记为$\alpha_{1}, \alpha_{2} $）③解释地求解两个变量的最优解$\alpha_{1}^{\star}, \alpha_{2}^{\star}$，并更新至$\alpha$的列表④检查更新后的$\alpha$列表是否在某个精度范围内满足KKT条件，若不满足，返回② 2. SMO算法的简单实现核函数软间隔与支持向量回归公式推导Soft-Margin SVM公式推导 Soft-Margin SVM的原始形式导出Hard-Margin \begin{array}{ll}{\min _{w, b}} & {\frac{1}{2}\|w\|^{2}} \\ {\text { s.t. }} & {y_{i}\left(w^{T} x_{i}+b\right) \geq 1, \quad i=1,2, \ldots, m}\end{array}Naive-Soft-Margin \begin{array}{l}{\min _{w, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{0 / 1}\left(y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)-1\right)} \\ {\ell_{0 / 1}(z)=\left\{\begin{array}{l}{1, \text { if } z]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习》第4章 决策树模型 公式推导]]></title>
    <url>%2F2019%2F07%2F03%2F%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC4%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%20%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[第4章 决策树模型ID3算法公式推导1. 信息熵与信息增益信息熵：熵是度量样本集合纯度最常用的一种指标，代表一个系统中蕴含多少信息量，信息量越大表明一个系统不确定性就越大，就存在越多的可能性，即信息熵越大。假定当前样本集合D中第类样本所占的比$p_{k}(k=1,2, \ldots,|y|)$，则D的信息熵为： \operatorname{Ent}(D)=-\sum_{k=1}^{ | \mathcal{Y |}} p_{k} \log _{2} p_{k}信息熵满足下列不等式： 0 \leq \operatorname{Ent}(D) \leq \log _ {2}|\mathcal{Y}|y表示样本D中的类别数 已知集合D的信息熵的定义为： \operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|} p_{k} \log _{2} p_{k}其中，$|\mathcal{Y}|$表示样本类别总数，$p_{k}$表示第k类样本所占的比例，且$0 \leq p_{k} \leq 1, \sum_{k=1}^{n} p_{k}=1$ 若令$|\mathcal{Y}|=n, p_{k}=x_{k}$，那么信息熵Ent（D）就可以看作一个n元实值函数，即 \operatorname{Ent}(D)=f\left(x_{1}, \ldots, x_{n}\right)=-\sum_{k=1}^{n} x_{k} \log _{2} x_{k}其中：$0 \leq x_{k} \leq 1, \sum_{k=1}^{n} x_{k}=1$引入拉格朗日乘子$\lambda$：$L\left(x_{1}, \ldots, x_{n}, \lambda\right)=-\sum_{k=1}^{n} x_{k} \log _{2} x_{k}+\lambda\left(\sum_{k=1}^{n} x_{k}-1\right)$ 对L分别关于$x, \lambda$求一阶偏导，并令偏导等于0： \begin{aligned} \frac{\partial L\left(x_{1}, \ldots, x_{n}, \lambda\right)}{\partial x_{1}} &=\frac{\partial}{\partial x_{1}}\left[-\sum_{k=1}^{n} x_{k} \log _{2} x_{k}+\lambda\left(\sum_{k=1}^{n} x_{k}-1\right)\right]=0 \\ &=-\log _{2} x_{1}-x_{1} \cdot \frac{1}{x_{1} \ln 2}+\lambda=0 \\ &=-\log _{2} x_{1}-\frac{1}{\ln 2}+\lambda=0 \\ & \Rightarrow \lambda=\log _{2} x_{1}+\frac{1}{\ln 2} \end{aligned}同理推得： \lambda=\log _{2} x_{1}+\frac{1}{\ln 2}=\log _{2} x_{2}+\frac{1}{\ln 2}=\ldots=\log _{2} x_{n}+\frac{1}{\ln 2}对于任意的x，满足约束条件： \sum_{k=1}^{n} x_{k}=1因此： x_{1}=x_{2}=\ldots=x_{n}=\frac{1}{n}最大值点还是最小值点需要做个简单的验证：$x_{1}=x_{2}=\ldots=x_{n}=\frac{1}{n}$时： f\left(\frac{1}{n}, \ldots, \frac{1}{n}\right)=-\sum_{k=1}^{n} \frac{1}{n} \log _{2} \frac{1}{n}=-n \cdot \frac{1}{n} \log _{2} \frac{1}{n}=\log _{2} n$x_{1}=1, x_{2}=x_{3}=\ldots=x_{n}=0$时： f(1,0, \ldots, 0)=-1 \cdot \log _{2} 1-0 \cdot \log _{2} 0 \ldots-0 \cdot \log _{2} 0=0显然$\log _{2} n \geq 0$，所以$x_{1}=x_{2}=\ldots=x_{n}=\frac{1}{n}$为最大值点，最大值为$\log _{2} n$。 下面考虑求$f\left(x_{1}, \ldots, x_{2}\right)$的最小值，仅考虑$0 \leq x_{k} \leq 1, f\left(x_{1}, \ldots, x_{n}\right)$可以看作是n个互不相关一元函数的加和，即： \begin{array}{c}{f\left(x_{1}, \ldots, x_{n}\right)=\sum_{k=1}^{n} g\left(x_{k}\right)} \\ {g\left(x_{k}\right)=-x_{k} \log _{2} x_{k}, 0 \leq x_{k} \leq 1}\end{array}求$g\left(x_{1}\right)$的最小值，首先对$g\left(x_{1}\right)$关于$x_{1}$，求一阶和二阶导数： \begin{array}{l}{g^{\prime}\left(x_{1}\right)=\frac{d\left(-x_{1} \log _{2} x_{1}\right)}{d x_{1}}=-\log _{2} x_{1}-x_{1} \cdot \frac{1}{x_{1} \ln 2}=-\log _{2} x_{1}-\frac{1}{\ln 2}} \\ {g^{\prime \prime}\left(x_{1}\right)=\frac{d\left(-\log _{2} x_{1}-\frac{1}{\ln 2}\right)}{d x_{1}}=-\frac{1}{x_{1} \ln 2}}\end{array}在定义域$0 \leq x_{k} \leq 1$上，始终有$g^{\prime \prime}\left(x_{1}\right)=-\frac{1}{x_{1} \ln 2}&lt;0$，即$g\left(x_{1}\right)$为开口向下的凹函数，最小值在边界$x_{1}=0$或$x_{1}=1$处取得： \begin{array}{l}{g(0)=-0 \log _{2} 0=0} \\ {g(1)=-1 \log _{2} 1=0}\end{array}$g\left(x_{1}\right)$的最小值即为0，同理可得$g\left(x_{2}\right), \ldots, g\left(x_{n}\right)$的最小值也为0，那么$f\left(x_{1}, \ldots, x_{n}\right)$的最小值此时也为0如果令某个$x_{k}=1$，那么根据约束条件$\sum_{k=1}^{n} x_{k}=1$可知： x_{1}=x_{2}=\ldots=x_{k-1}=x_{k+1}=\ldots=x_{n}=0将其代入$f\left(x_{1}, \ldots, x_{n}\right)$，得 f(0,0, \ldots, 0,1,0, \ldots, 0)=-0 \log _{2} 0-0 \log _{2} 0 \ldots-0 \log _{2} 0-1 \log _{2} 1-0 \log _{2} 0 \ldots-0 \log _{2} 0=0所以$x_{k}=1, x_{1}=x_{2}=\ldots=x_{k-1}=x_{k+1}=\ldots=x_{n}=0$，一定是$f\left(x_{1}, \ldots, x_{n}\right)$在满足约束条件下的最小值点，其最小值为0.所以： 0 \leq \operatorname{Ent}(D) \leq \log _{2} n证明：$0 \leq \operatorname{Ent}(D) \leq \log_{2}|y|$其中：$p_{1}+\ldots+p_{|y|}=1$$-p_{|y|} \log _{2} p_{|y|} \geq 0$两个概率变量x,y满足$x+y=p, 0&lt;p \leq 1$，p为一定值，当x越靠近$\frac{p}{2}$时，下面的z越大 z=-\left(x \log _{2} x+(p-x) \log_{2}(p-x)\right)对x求导： \begin{aligned} z^{\prime} &=-\left(\log_{2} x+x \frac{1}{x}-\log_{2}(p-x)-(p-x) \frac{1}{p-x}\right) \\ &=-\log_{2} \frac{x}{p-x} \end{aligned}当 $x=\frac{p}{2}$时，$z^{\prime}=0$；当$x&lt;\frac{p}{2}$时，$z^{\prime}&gt;0$；z单调递增当$x&gt;\frac{p}{2}$时，$z^{\prime}&lt;0$；z单调递减 证明命题： 假设$p=\frac{1}{n}$，定义 x_{1}=\min \left(p_{i}\right)-\left(x_{1} \log _{2} x_{1}+x_{2} \log _{2} x_{2}\right)所以当$p=\frac{1}{|y|}$时，信息熵取得最大值$\log_{2}|y|$ 样本D中只有一类样本，此时信息熵最小，其值为： \operatorname{Ent}(D)=-1 \log_{2} 1-0 \log_{2} 0-\ldots-0 \log_{2} 0=0当只有一类样本时，表征系统就是确定的，因此，系统的信息量最小。 信息增益：假定离散属性a有V个可能的取值$\left\{a^{1}, a^{2}, \ldots a^{v}\right\}$，如果使用特征a来对数据集D进行划分，则会产生V个分支结点，其中第个结点包含了数据集D中所有在特征a上取值为$a^{V}$的样本总数，记为$D^{\nu}$，再考虑到不同的分支结点所包含的样本数量不同，给分支节点赋予不同的权重，这样对样本数越多的分支节点的影响就会越大，因此，就能够计算出特征对样本集D进行划分所获得的“信息增益”： \operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)2. ID3算法流程ID3算法流程1）初始化信息增益的阈值$\varepsilon$；2）判断样本是否为同一类输出$D^{v}$，如果是，则返回单节点树T。标记类别为$D^{v}$；3）判断属性是否为空，如果是，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。4）计算所有属性A中的各个属性（一共个）对输出D的信息增益，选择信息增益最大的属性a。5）如果属性a的信息增益小于阈值$\varepsilon$，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。6）否则，按属性a的不同属性值$\left\{a^{1}, a^{2}, \ldots, a^{v}\right\}$.，将对应的样本输出D分成不同的类别$D^{v}$，每个类别产生一个子节点。对应属性值为$a^{v}$。返回增加了节点的树T。7）对于所有的子节点，令$D=D^{v}, A=A-a$递归调用2-6步，得到子树并返回。 ID3算法虽然提出了新思路，但是还是有很多值得改进的地方。a）ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。b）ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。如果校正这个问题呢？c）ID3算法对于缺失值的情况没有做考虑d）没有考虑过拟合的问题 C4.5算法公式推导1. 增益率增益率：信息增益偏向于选择取值较多的属性，容易过拟合基于信息增益的缺点，C4.5算法不直接使用信息增益，而是使用一种叫增益率的方法来选择最优属性进行划分，对于样本集D中的离散属性a，增益率为： \operatorname{Gain}_{-} \operatorname{ratio}(D, a)=\frac{\operatorname{Gain}(D, a)}{I V(a)}IV值： I V(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log_{2} \frac{\left|D^{v}\right|}{|D|}IV（a）是属性a的固有值。 增益率可能产生一个问题就是，对可取数值数目较少的属性有所偏好。因此C4.5算法并不是直接选择使用增益率最大的候选划分属性，而是使用了一个启发式算法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益率最高的。 2. C4.5算法的不足：C4.5虽然改进或者改善了ID3算法的几个主要的问题，仍然有优化的空间。1）由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。2）C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。3）C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。4）C4.5由于使用了熵模型，里面有大量的耗时的对数运算，如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。 CART算法公式推导1. 基尼值与基尼指数基尼值：用于度量数据集的纯度，Gimi（D）反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，因此，Gimi（D）越小，则数据集的纯度越高.假定当前样本集合D中第k类样本所占的比例为$p_{k}(k=1,2, \dots,|y|)$，则D的基尼值为： \begin{aligned} \operatorname{Gini}(D) &=\sum_{k=1}^{|y|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\ &=p_{1} \cdot\left(p_{2}+p_{3}+\ldots+p_{k}\right)+\ldots+p_{k} \cdot\left(p_{1}+p_{2}+p_{3}+\ldots+p_{k-1}\right) \\ &=\left(p_{1}+p_{2}+\ldots+p_{k}\right)-\left(p_{1}^{2}+p_{2}^{2}+\ldots+p_{k}^{2}\right) \\ &=1-\sum_{k=1}^{|y|} p_{k}^{2} \end{aligned}基尼指数：表示在样本集合中一个随机选中的样本被分错的概率。基尼指数越大，样本的不确定性也就越大。 \operatorname{Gini}_{-} \operatorname{index}(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)$D^{v}$表示第v个类别的样本集。 2.CART算法流程CART和ID3的思路区别不大。 剪枝预剪枝（pre-pruning）预剪枝（pre-pruning）：预剪枝就是在构造决策树的过程中，先对每个结点在划分前进行估计，如果当前结点的划分不能带来决策树模型泛化性增加。则不对当前结点进行划分并且将当前结点标记为叶结点。 后剪枝（post-purning）后剪枝就是先把整颗决策树构造完毕，然后自底向上的对非叶结点进行考察，若将该结点对应的子树换为叶结点能够带来泛化性能的提升，则把该子树替换为叶结点。 对比预剪枝和后剪枝，能够发现，后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛华性能往往也要优于预剪枝决策树。但后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，因此其训练时间开销要比未剪枝决策树和预剪枝决策树都大得多。 连续值与缺失值的处理连续值的处理：样本集D中的连续属性a，假设属性a有n个不同的取值，对其进行大小排序，记为$\left\{a^{1}, a^{2}, \ldots, a^{n}\right\}$，根据属性可得到n-1个划分点t，划分点t的集合为： T_{a}=\left\{\frac{a^{i}+a^{i+1}}{2} | 1 \leqslant i \leqslant n-1\right\}对于取值集合$T_{a}$中的每个t值会将属性a离散为一个属性值只有两个值，分别是$\{a&gt;t\}$和$\{a \leq t\}$的属性，计算新属性的信息增益，找到信息增益最大的，值即为该属性的最优划分点。 \begin{aligned} \operatorname{Gain}(D, a) &=\max _{t \in T_{o}} \operatorname{Gain}(D, a, t) \\ &=\max _{t \in I_{a}} \operatorname{Ent}(D)-\sum_{\lambda \in[-,+\}} \frac{\left|D_{t}^{\lambda}\right|}{|D|} \operatorname{Ent}\left(D_{t}^{\lambda}\right) \end{aligned}缺失值的处理：在决策树中处理含有缺失值的样木的时候，需要解决两个问题：1、如何在属性值缺失的情况下进行划分属性的选择？2、给定划分属性，若样本在该属性上的值是缺失的，那么该如何对这个样本进行划分？ 对于第一个问题：我们可以根据$\widetilde{D}$（即在该属性上没有缺失的样本集）来计算属性a的信息增益或者其它指标。我们只要再给根据计算出来的值一个权重，就可以表示训练集D中属性a的优劣。 假定属性a有V个可取值$\left\{a^{1}, a^{2} \ldots, a^{v}\right\}$令$\tilde{D}^{v}$表示D中在属性a上取值为$a^{v}$的样本予集，$\tilde{D}_ {k}$表示$\tilde{D}$中属于第k类$(k=1,2, \dots,|y|)$）样本子集，则显然有$\tilde{D}=\cup_{k=1}^{v_{1}} \tilde{D}_{k}, \tilde{D}=\cup_{v=1}^{V} \tilde{D}^{v}$，假定我们为每个样本 赋予一个权重$w_{x}$（在决策树的初始阶段，根节点中个样本的权重初始化为1），并定义： \rho=\frac{\sum_{x \in \tilde{D}} w_{x}}{\sum_{x \in D} w_{x}}表示无缺失值样本作占的比例 \tilde{p}_{k}=\frac{\sum_{x \in \tilde{D}_{k}} w_{x}}{\sum_{x \in \tilde{D}} w_{x}} \quad(1 \leqslant k \leqslant|y|)表示无缺失值样本中第k类所占的比例 \tilde{r}_{v}=\frac{\sum_{x \in \tilde{D}^{v}} w_{x}}{\sum_{x \in \tilde{D}} w_{x}}(1 \leqslant v \leqslant V)表示无缺失值样本中在属性a上取值$a^{v}$的样本所占的比例 信息增益推广为： \begin{aligned} \operatorname{Gain}(D, a) &=\rho \times \operatorname{Gain}(\tilde{D}, a) \\ &=\rho \times\left(\operatorname{Ent}(\tilde{D})-\sum_{v=1}^{V} \tilde{r}_{v} \operatorname{Ent}\left(\tilde{D}^{v}\right)\right) \end{aligned} \operatorname{Ent}(\tilde{D})=-\sum_{k=1}^{|y|} \tilde{p}_{k} \log _{2} \tilde{p}_{k}]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[范数分类]]></title>
    <url>%2F2019%2F06%2F25%2F%E8%8C%83%E6%95%B0%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[范数即对一个线性空间X，定义泛函|| · ||：X→R满足 $|x| \geq 0$ and $|x|=0 \leftrightarrow x=0$ for $x \in X$（正定） $|c x|=|c||x|$ for $c \in \mathbb{R}, x \in X$（齐次） $|a+b| \leq|a|+|b|$ for any $a, b \in X$（三角不等式） L-P范数L-P范数不是一个范数，而是一组范数$L_{p}=|\mathbf{x}|_{p}=\sqrt[p]{\sum_{i=1}^{n} x_{i}^{p}}, \mathbf{x}=\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ L0范数当P=0时，也就是L0范数，L0范数并不是一个真正的范数，它主要被用来度量向量中非零元素的个数，可以定义为：$|x|_{0}=\sum_{i=1}^{k}\left|x_{i}\right|^{0}$L0范数表示向量中非零元素的个数。如果我们使用L0来规则化参数向量w，就是希望w的元素大部分都为零。L0范数的这个属性，使其非常适用于机器学习中的稀疏编码。在特征选择中，通过最小化L0范数来寻找最少最优的稀疏特征项。但是，L0范数的最小化问题是NP难问题。而L1范数是L0范数的最优凸近似，它比L0范数要更容易求解。因此，优化过程将会被转换为更高维的范数（例如L1范数）问题。 L1范数L1范数是向量中各个元素绝对值之和，也被称作“Lasso regularization”（稀疏规则算子）$|x|_{1}=\sum_{i=1}^{k}\left|x_{i}\right|$ 在机器学习特征选择中，稀疏规则化能够实现特征的自动选择。一般来说，输入向量X的大部分元素（也就是特征）都是和最终的输出Y没有关系或者不提供任何信息的，在最小化目标函数的时候考虑这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确Y的预测。稀疏规则化算子的引入就是为了完成特征自动选择，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 L0范数与L1范数都可以实现稀疏，而L1范数比L0具有更好的优化求解特性而被广泛使用。 L0范数本身是特征选择的最直接的方案，但因为之前说到的理由，其不可分，且很难优化，因此实际应用中我们使用L1来得到L0的最优凸近似。 总结一下上两段的结论就是：L1范数和L0范数可以实现稀疏，L1因为拥有比L0更好的优化求解特性而被广泛应用。这样我们大概知道了可以实现稀疏，但是为什么我们希望稀疏？让参数稀疏有什么好处呢？这里有两个理由： 特征选择(Feature Selection)：大家希望稀疏规则化的一个关键原因在于它能实现特征的自动选择。一般来说，X的大部分元素（也就是特征）都是和最终的输出没有关系或者不提供任何信息的，在最小化目标函数的时候考虑这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0 可解释性(Interpretability)：另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设这是个回归模型：$y=\sum_{i=1}^{1000} w_{i} * x_{i}+b$当然了，为了让y限定在的范围，一般还得加个Logistic函数。 通过学习，如果最后学习到的就只有很少的非零元素，例如只有5个非零的，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个都非0，医生面对这1000种因素只能一脸懵逼不知如何是好。 L2范数$|x|_{2}=\sqrt{\sum_{i=1}^{k}\left|x_{i}\right|^{2}}$L2范数是最常用的范数了，我们用的最多的度量距离欧氏距离就是一种L2范数。在回归里面，有人把加了L2范数项的回归c称为“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。它被广泛的应用在解决机器学习里面的过拟问题合。 为什么L2范数可以防止过拟合？回答这个问题之前，我们得先看看L2范数实际上是什么。 L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项最小，可以使得的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这是有很大的区别的。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？因为当限制了参数很小，实际上就限制了多项式某些分量的影响很小，这样就相当于减少参数个数。 总结下：通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。 L2范数的好处是什么呢？ 1）学习理论的角度： 从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。 2）优化计算的角度： 从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。 来源：https://baijiahao.baidu.com/s?id=1607333156323286278&amp;wfr=spider&amp;for=pchttps://blog.csdn.net/zouxy09/article/details/24971995]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>范数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[距离，范数，向量空间，欧几里得空间（欧式空间），希尔伯特空间，巴拿赫空间]]></title>
    <url>%2F2019%2F06%2F25%2F%E8%B7%9D%E7%A6%BB%EF%BC%8C%E8%8C%83%E6%95%B0%EF%BC%8C%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%EF%BC%8C%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%A9%BA%E9%97%B4%EF%BC%88%E6%AC%A7%E5%BC%8F%E7%A9%BA%E9%97%B4%EF%BC%89%EF%BC%8C%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4%EF%BC%8C%E5%B7%B4%E6%8B%BF%E8%B5%AB%E7%A9%BA%E9%97%B4%2F</url>
    <content type="text"><![CDATA[距离直线距离、向量距离、折线距离等等都是具体的距离，这里谈的距离是一个抽象的概念，需要满足非负、自反、三角不等式三个条件。设X是任一非空集，对X中任意两点x，y，有一实数d（x，y）与之对应且满足：1.d（x，y）≥ 0，且d（x，y）= 0 当且仅当 x=y；2.d（x， y） = d（y，x）；3.d（x，y）≤ d（x，z）+d（z，y）。称d（x，y）为X中的一个距离。 向量空间距离 + 线性结构⟶线性空间 = 向量空间线性结构如向量的加法、数乘，使其满足加法的交换律、结合律、零元、负元；数乘的交换律、单位一；数乘与加法的结合律（两个）共八点要求，从而形成一个线性空间，这个线性空间就是向量空间 范数范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。可以简单把范数当作距离来理解。范数分为：向量范数和矩阵范数。向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。并满足一定的条件，即①非负性；②齐次性；③三角不等式。它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。 ||x|| ≥0; ||ax||=|a|||x||; ||x+y||≤||x||+||y||。 范数的集合⟶ 赋范空间赋范空间 +线性结构⟶线性赋范空间 距离的集合⟶ 度量空间度量空间 +线性结构⟶线性度量空间 欧式空间有限维线性内积空间称作欧几里得空间已经构成的线性赋范空间上继续扩展，添加内积运算，使空间中有角的概念，形成内积空间：线性赋范空间+内积运算⟶ 内积空间有限维的内积空间也就是欧氏空间内积空间+有限维⟶欧几里德空间 希尔伯特空间线性完备内积空间称作希尔伯特空间线性赋范空间+内积运算⟶ 内积空间内积空间+完备性⟶ 希尔伯特空间其中完备性的意思就是空间中的极限运算不能跑出该空间 巴拿赫空间线性完备赋范空间称作巴拿赫空间范数的集合⟶ 赋范空间赋范空间 +线性结构⟶线性赋范空间线性赋范空间+完备性⟶巴拿赫空间]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习》第3章 线性回归模型 公式推导]]></title>
    <url>%2F2019%2F06%2F24%2F%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC3%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[第3章 线性回归模型一元线性回归公式推导1. 求解偏置b推导思路：①由最小二乘法导出损失函数$E(w, b)$ \begin{aligned} E_{(w, b)} &=\sum_{i=1}^{m}\left(y_{i}-f\left(x_{i}\right)\right)^{2} \\ &=\sum_{i=1}^{m}\left(y_{i}-\left(w x_{i}+b\right)\right)^{2} \\ &=\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2} \end{aligned}②证明损失函数$E(w, b)$是关于w和b的凸函数 求$A=f_{x x}^{\prime \prime}(x, y)$ ：\begin{aligned} \frac{\partial E_{(w, b)}}{\partial w} &=\frac{\partial}{\partial w}\left[\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\right] \\ &=\sum_{i=1}^{m} \frac{\partial}{\partial w}\left(y_{i}-w x_{i}-b\right)^{2} \\ &=\sum_{i=1}^{m} 2 \cdot\left(y_{i}-w x_{i}-b\right) \cdot\left(-x_{i}\right) \\ &=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right) \end{aligned}\begin{aligned} \frac{\partial^{2} E_{(w, b)}}{\partial w^{2}} &=\frac{\partial}{\partial w}\left(\frac{\partial E_{(w, b)}}{\partial w}\right) \\ &=\frac{\partial}{\partial w}\left[2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)\right] \\ &=\frac{\partial}{\partial w}\left[2 w \sum_{i=1}^{m} x_{i}^{2}\right] \\ & = 2 \sum_{i=1}^{m} x_{i}^{2} \end{aligned} 求$B=f_{x y}^{\prime \prime}(x, y)$：\begin{aligned} \frac{\partial^{2} E_{(w, b)}}{\partial w \partial b} &=\frac{\partial}{\partial b}\left(\frac{\partial E_{(w, b)}}{\partial w}\right) \\ &=\frac{\partial}{\partial b}\left[2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)\right] \\ &=\frac{\partial}{\partial b}\left(-2 \sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right] \\ &=\frac{\partial}{\partial b}\left(-2 \sum_{i=1}^{m} y_{i} x_{i}+2 \sum_{i=1}^{m} b x_{i}\right) \\ &=\frac{\partial}{\partial b}\left(2 \sum_{i=1}^{m} b x_{i}\right)=2 \sum_{i=1}^{m} x_{i} \end{aligned} 求$C=f_{y y}^{\prime \prime}(x, y)$：\begin{aligned} \frac{\partial E_{(w, b)}}{\partial b} &=\frac{\partial}{\partial b}\left[\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\right] \\ &=\sum_{i=1}^{m} \frac{\partial}{\partial b}\left(y_{i}-w x_{i}-b\right)^{2} \\ &=\sum_{i=1}^{m} 2 \cdot\left(y_{i}-w x_{i}-b\right) \cdot(-1) \\ &=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right) \end{aligned}\begin{aligned} \frac{\partial^{2} E_{(w, b)}}{\partial b^{2}} &=\frac{\partial}{\partial b}\left(\frac{\partial E_{(w, b)}}{\partial b}\right) \\ &=\frac{\partial}{\partial b}\left[2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)\right] \\ &=\frac{\partial}{\partial b}(2 m b) \\ &=2m \end{aligned} 证明A=2 \sum_{i=1}^{m} x_{i}^{2} \qquad B=2 \sum_{i=1}^{m} x_{i} \qquad C=2 m\begin{aligned} A C-B^{2} &=2 m \cdot 2 \sum_{i=1}^{m} x_{i}^{2}-\left(2 \sum_{i=1}^{m} x_{i}\right)^{2}=4 m \sum_{i=1}^{m} x_{i}^{2}-4\left(\sum_{i=1}^{m} x_{i}\right)^{2}=4 m \sum_{i=1}^{m} x_{i}^{2}-4 \cdot m \cdot \frac{1}{m} \cdot\left(\sum_{i=1}^{m} x_{i}\right)^{2} \\ &=4 m \sum_{i=1}^{m} x_{i}^{2}-4 m \cdot \overline{x} \cdot \sum_{i=1}^{m} x_{i}=4 m\left(\sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m} x_{i} \overline{x}\right)=4 m \sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}\right) \end{aligned}又因为：$\sum_{i=1}^{m} x_{i} \overline{x}=\overline{x} \sum_{i=1}^{m} x_{i}=\overline{x} \cdot m \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} x_{i}=m \overline{x}^{2}=\sum_{i=1}^{m} \overline{x}^{2}$=4 m \sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}-x_{i} \overline{x}+x_{i} \overline{x}\right)=4 m \sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}-x_{i} \overline{x}+\overline{x}^{2}\right)=4 m \sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)^{2}所以：$A C-B^{2}=4 m \sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)^{2} \geq 0$也即损失函数$E(w, b)$是关于w和b的凸函数得证③对损失函数$E(w, b)$关于b求一阶偏导数\frac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)④令一阶偏导数等于0解出b\begin{aligned} \frac{\partial E_{(w, b)}}{\partial b} &=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)=0 \\ & m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)=0 \\ b &=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right) \end{aligned}b=\frac{1}{m} \sum_{i=1}^{m} y_{i}-w \cdot \frac{1}{m} \sum_{i=1}^{m} x_{i}=\overline{y}-w \overline{x} 2. 求解权重w推导思路：①由最小二乘法导出损失函数$E(w, b)$②证明损失函数$E(w, b)$是关于w和b的凸函数③对损失函数$E(w, b)$关于w求一阶偏导数④令一阶偏导数等于0解出w \begin{aligned} \frac{\partial E_{(w, b)}}{\partial w}= 2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)=0 \\ w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}=0 \\ w \sum_{i=1}^{m} x_{i}^{2} =\sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m} b x_{i} \end{aligned}将$b=\overline{y}-w \overline{x}$代入$w \sum_{i=1}^{m} x_{i}^{2}=\sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m} b x_{i}$得 w \sum_{i=1}^{m} x_{i}^{2}=\sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m}(\overline{y}-w \overline{x}) x_{i}w \sum_{i=1}^{m} x_{i}^{2}=\sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i}+w \overline{x} \sum_{i=1}^{m} x_{i}w \sum_{i=1}^{m} x_{i}^{2}-w \overline{x} \sum_{i=1}^{m} x_{i}=\sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i}w\left(\sum_{i=1}^{m} x_{i}^{2}-\overline{x} \sum_{i=1}^{m} x_{i}\right)=\sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i}w=\frac{\sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i}}{\sum_{i=1}^{m} x_{i}^{2}-\overline{x} \sum_{i=1}^{m} x_{i}}根据 \overline{y} \sum_{i=1}^{m} x_{i}=\frac{1}{m} \sum_{i=1}^{m} y_{i} \sum_{i=1}^{m} x_{i}=\overline{x} \sum_{i=1}^{m} y_{i}\overline{x} \sum_{i=1}^{m} x_{i}=\frac{1}{m} \sum_{i=1}^{m} x_{i} \sum_{i=1}^{m} x_{i}=\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}得 w=\frac{\sum_{i=1}^{m} y_{i} x_{i}-\overline{x} \sum_{i=1}^{m} y_{i}}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}3. 将w向量化w=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}将$\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}=\overline{x} \sum_{i=1}^{m} x_{i}=\sum_{i=1}^{m} x_{i} \overline{x}$代入分母可得 \begin{aligned} w &=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m} x_{i} \overline{x}} \\ &=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \overline{x}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}\right)} \end{aligned}由于 \sum_{i=1}^{m} y_{i} \overline{x}=\overline{x} \sum_{i=1}^{m} y_{i}=\frac{1}{m} \sum_{i=1}^{m} x_{i} \sum_{i=1}^{m} y_{i}=\sum_{i=1}^{m} x_{i} \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} y_{i}=\sum_{i=1}^{m} x_{i} \overline{y}\sum_{i=1}^{m} y_{i} \overline{x}=\overline{x} \sum_{i=1}^{m} y_{i}=\overline{x} \cdot m \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} y_{i}=m \overline{x} \overline{y}=\sum_{i=1}^{m} \overline{x} \overline{y}\sum_{i=1}^{m} x_{i} \overline{x}=\overline{x} \sum_{i=1}^{m} x_{i}=\overline{x} \cdot m \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} x_{i}=m \overline{x}^{2}=\sum_{i=1}^{m} \overline{x}^{2}所以 \begin{aligned} w &=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \overline{x}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}\right)}=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \overline{x}-y_{i} \overline{x}+y_{i} \overline{x}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}-x_{i} \overline{x}+x_{i} \overline{x}\right)} \\ &=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \overline{x}-x_{i} \overline{y}+\overline{x} \overline{y}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}-x_{i} \overline{x}+\overline{x}^{2}\right)}=\frac{\sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)\left(y_{i}-\overline{y}\right)}{\sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)^{2}} \end{aligned}令 \boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{m}\right)^{T} \quad \boldsymbol{y}=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{T}\boldsymbol{x}_{d}=\left(x_{1}-\overline{x}, x_{2}-\overline{x}, \ldots, x_{m}-\overline{x}\right)^{T} \quad \boldsymbol{y}_{d}=\left(y_{1}-\overline{y}, y_{2}-\overline{y}, \ldots, y_{m}-\overline{y}\right)^{T}则 \begin{aligned} w &=\frac{\sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)\left(y_{i}-\overline{y}\right)}{\sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)^{2}} \\ &=\frac{\boldsymbol{x}_{d}^{T} \boldsymbol{y}_{d}}{\boldsymbol{x}_{d}^{T} \boldsymbol{x}_{d}} \end{aligned}多元线性回归公式推导求解权重$\hat{w}$的公示推导推导思路：将w和b组合成$\hat{\boldsymbol{w}}$： f\left(\boldsymbol{x}_{i}\right)=\boldsymbol{w}^{T} \boldsymbol{x}_{i}+bf\left(\boldsymbol{x}_{i}\right)=\left(\begin{array}{llll}{w_{1}} & {w_{2}} & {\cdots} & {w_{d}}\end{array}\right)\left(\begin{array}{c}{x_{i 1}} \\ {x_{i 2}} \\ {\vdots} \\ {x_{i d}}\end{array}\right)+bf\left(\boldsymbol{x}_{i}\right)=w_{1} x_{i 1}+w_{2} x_{i 2}+\ldots+w_{d} x_{i d}+bf\left(\boldsymbol{x}_{i}\right)=w_{1} x_{i 1}+w_{2} x_{i 2}+\ldots+w_{d} x_{i d}+w_{d+1} \cdot 1f\left(\boldsymbol{x}_{i}\right)=\left(\begin{array}{llllll}{w_{1}} & {w_{2}} & {\cdots} & {w_{d}} & {w_{d+1}}\end{array}\right)\left(\begin{array}{c}{x_{i 1}} \\ {x_{i 2}} \\ {\vdots} \\ {x_{i d}} \\ {1}\end{array}\right)f\left(\hat{\boldsymbol{x}}_{i}\right)=\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{i}①由最小二乘法导出损失函数$E_{\hat{\boldsymbol{w}}}$ \begin{aligned} E_{\hat{\boldsymbol{w}}} &=\sum_{i=1}^{m}\left(y_{i}-f\left(\hat{\boldsymbol{x}}_{i}\right)\right)^{2} \\ &=\sum_{i=1}^{m}\left(y_{i}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{i}\right)^{2} \end{aligned}\mathbf{X}=\left(\begin{array}{ccccc}{x_{11}} & {x_{12}} & {\dots} & {x_{1 d}} & {1} \\ {x_{21}} & {x_{22}} & {\dots} & {x_{2 d}} & {1} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\vdots} \\ {x_{m 1}} & {x_{m 2}} & {\dots} & {x_{m d}} & {1}\end{array}\right)=\left(\begin{array}{cc}{\boldsymbol{x}_{1}^{\mathrm{T}}} & {1} \\ {\boldsymbol{x}_{2}^{\mathrm{T}}} & {1} \\ {\vdots} & {\vdots} \\ {\boldsymbol{x}_{m}^{\mathrm{T}}} & {1}\end{array}\right)=\left(\begin{array}{c}{\hat{\boldsymbol{x}}_{1}^{T}} \\ {\hat{\boldsymbol{x}}_{2}^{T}} \\ {\vdots} \\ {\hat{\boldsymbol{x}}_{m}^{T}}\end{array}\right)\boldsymbol{y}=\left(y_{1}, y_{2}, \dots, y_{m}\right)^{T}\begin{aligned} E_{\hat{\boldsymbol{w}}} &=\sum_{i=1}^{m}\left(y_{i}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{i}\right)^{2} \\ &=\left(y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1}\right)^{2}+\left(y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2}\right)^{2}+\ldots+\left(y_{m}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{m}\right)^{2} \end{aligned}抽象成向量的内积： E_{\hat{\boldsymbol{w}}}=\left(y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1} \quad y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2} \quad \cdots \quad y_{d}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{d}\right)\left(\begin{array}{c}{y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1}} \\ {y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2}} \\ {\vdots} \\ {y_{d}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{d}}\end{array}\right)又因为： \left(\begin{array}{c}{y_{1}-\hat{w}^{T} \hat{x}_{1}} \\ {y_{2}-\hat{w}^{T} \hat{x}_{2}} \\ {\vdots} \\ {y_{d}-\hat{w}^{T} \hat{x}_{d}}\end{array}\right)=\left(\begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{d}}\end{array}\right)-\left(\begin{array}{c}{\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1}} \\ {\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2}} \\ {\vdots} \\ {\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{d}}\end{array}\right)=\left(\begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{d}}\end{array}\right)-\left(\begin{array}{c}{\hat{\boldsymbol{x}}_{1}^{T} \hat{\boldsymbol{w}}} \\ {\hat{\boldsymbol{x}}_{2}^{T} \hat{\boldsymbol{w}}} \\ {\vdots} \\ {\hat{\boldsymbol{x}}_{d}^{T} \hat{\boldsymbol{w}}}\end{array}\right)\left(\begin{array}{c}{\hat{\boldsymbol{x}}_{1}^{T} \hat{\boldsymbol{w}}} \\ {\hat{\boldsymbol{x}}_{2}^{T} \hat{\boldsymbol{w}}} \\ {\vdots} \\ {\hat{\boldsymbol{x}}_{d}^{T} \hat{\boldsymbol{w}}}\end{array}\right)=\left(\begin{array}{c}{\hat{\boldsymbol{x}}_{1}^{T}} \\ {\hat{\boldsymbol{x}}_{2}^{T}} \\ {\vdots} \\ {\hat{\boldsymbol{x}}_{d}^{T}}\end{array}\right) \cdot \boldsymbol{\hat { w }}=\mathbf{X} \cdot \hat{\boldsymbol{w}}所以 \left(\begin{array}{c}{y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1}} \\ {y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2}} \\ {\vdots} \\ {y_{d}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{d}}\end{array}\right)=\left(\begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{d}}\end{array}\right)-\left(\begin{array}{c}{\hat{\boldsymbol{x}}_{1}^{T} \hat{\boldsymbol{w}}} \\ {\hat{\boldsymbol{x}}_{2}^{T} \hat{\boldsymbol{w}}} \\ {\vdots} \\ {\hat{\boldsymbol{x}}_{d}^{T} \hat{\boldsymbol{w}}}\end{array}\right)=\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}}E_{\hat{w}}=\left(\begin{array}{cccc}{y_{1}-\hat{w}^{T} \hat{x}_{1}} & {y_{2}-\hat{w}^{T} \hat{x}_{2}} & {\cdots} & {y_{d}-\hat{w}^{T} \hat{x}_{d}}\end{array}\right)\left(\begin{array}{c}{y_{1}-\hat{w}^{T} \hat{x}_{1}} \\ {y_{2}-\hat{w}^{T} \hat{x}_{2}} \\ {\vdots} \\ {y_{d}-\hat{w}^{T} \hat{x}_{d}}\end{array}\right)$ $=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})②证明损失函数$E_{\hat{\boldsymbol{w}}}$是关于$\hat{w}$的凸函数 凸集定义：设集合$D \in R^{n}$，如果对任意的$x, y \in D$与E意的$a \in[0,1]$，有$a \boldsymbol{x}+(1-a) \boldsymbol{y} \in D$，则称集$D$凸集。 凸集的几何意义是：若两个点属于此集合，则这两点连线上的任意一点均属于此集合。 梯度定义：设n元函数$f(\boldsymbol{x})$对自变量$\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{T}$的各分量$x_{i}$的偏导数$\frac{\partial f(\boldsymbol{x})}{\partial x_{i}}(i=1,2, \dots, n)$都存在，则称函数$f(\boldsymbol{x})$在$\mathcal{x}$处一阶可导，并称向量 \nabla f(\boldsymbol{x})=\left(\begin{array}{c}{\frac{\partial f(\boldsymbol{x})}{\partial x_{1}}} \\ {\frac{\partial f(\boldsymbol{x})}{\partial x_{2}}} \\ {\vdots} \\ {\frac{\partial f(\boldsymbol{x})}{\partial x_{n}}}\end{array}\right)为函数$f(\boldsymbol{x})$在$\boldsymbol{x}$处的一阶导数或梯度，记为$\nabla f(\boldsymbol{x})$（列向量） Hessian（海塞）矩阵定义：设n元函数$f(\boldsymbol{x})$对自变量$\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{T}$的各分量$\mathcal{x}_{i}$的二阶偏导数$\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{i} \partial x_{j}} \quad(i=1,2, \ldots, n ; j=1,2, \dots, n)$都存在，则称函数$f(\boldsymbol{x})$在点$\mathcal{x}$处二阶可导，并称矩阵 \nabla^{2} f(\boldsymbol{x})=\left[\begin{array}{cccc}{\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{1}^{2}}} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{1} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{2} \partial x_{1}}} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{2}^{2}}} & {\cdots} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{2} \partial x_{n}}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{n} \partial x_{1}}} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{n} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{n}^{2}}}\end{array}\right]为$f(\boldsymbol{x})$在$\mathcal{x}$处的二阶导数或Hessian矩阵，记为$\nabla^{2} f(\boldsymbol{x})$，若$f(\boldsymbol{x})$对$\mathcal{x}$各变元的所有二阶偏导数都连续，则$\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{i} \partial x_{j}}=\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{j} \partial x_{i}}$此时$\nabla^{2} f(\boldsymbol{x})$为对称矩阵。 多元实值函数凹凸性判定定理：设$D \subset R^{n}$是非空开凸集，$f : D \subset R^{n} \rightarrow R$，且$f(\boldsymbol{x})$在D上二阶连续可微，如果$f(\boldsymbol{x})$的Hessian矩阵$\nabla^{2} f(\boldsymbol{x})$在D上是正定的，则$f(\boldsymbol{x})$是D上的严格凸函数。 凸充分性定理：若$f : R^{n} \rightarrow R$是凸函数，且$f(\boldsymbol{x})$一阶连续可微，则$\boldsymbol{x}^{ \ast }$是全局解的充分必要条件是$\nabla f\left(\boldsymbol{x}^{ \ast }\right)=\mathbf{0}$，其中$\nabla f(\boldsymbol{x})$为$f(\boldsymbol{x})$关于$\mathcal{x}$的一阶导数（也称梯度）参考文献：王燕军，梁治安，最优化基础理论与方法.复旦大学出版社，2011 \begin{aligned} \frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}} &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})\right] \\ &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[\left(\boldsymbol{y}^{T}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T}\right)(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})\right] \\ &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[\boldsymbol{y}^{T} \boldsymbol{y}-\boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}\right] \\ &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[-\boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}\right] \end{aligned}【标量-向量】的矩阵微分公式为：（分母布局）默认采用： \frac{\partial y}{\partial x}=\left(\begin{array}{c}{\frac{\partial y}{\partial x_{1}}} \\ {\frac{\partial y}{\partial x_{2}}} \\ {\vdots} \\ {\frac{\partial y}{\partial x_{n}}}\end{array}\right)（分子布局）： \frac{\partial y}{\partial \boldsymbol{x}}=\left(\begin{array}{ccc}{\frac{\partial y}{\partial x_{1}}} & {\frac{\partial y}{\partial x_{2}}} & {\cdots} & {\frac{\partial y}{\partial x_{n}}}\end{array}\right)其中，$\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{T}$为n维列向量$y$为$\mathcal{x}$的n元标量函数 由【标量-向量】的矩阵微分公式可推得： \frac{\partial \boldsymbol{x}^{T} \boldsymbol{a}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{a}^{T} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\begin{array}{c}{\frac{\partial\left(a_{1} x_{1}+a_{2} x_{2}+\ldots+a_{n} x_{n}\right)}{\partial x_{1}}} \\ {\frac{\partial\left(a_{1} x_{1}+a_{2} x_{2}+\ldots+a_{n} x_{n}\right)}{\partial x_{2}}} \\ {\vdots} \\ {\frac{\partial\left(a_{1} x_{1}+a_{2} x_{2}+\ldots+a_{n} x_{n}\right)}{\partial x_{n}}}\end{array}\right)=\left(\begin{array}{c}{a_{1}} \\ {a_{2}} \\ {\vdots} \\ {a_{n}}\end{array}\right)=a同理可推得：$\frac{\partial \boldsymbol{x}^{T} \mathbf{B} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\mathbf{B}+\mathbf{B}^{T}\right) \boldsymbol{x}$ \begin{aligned} \frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}} &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[-\boldsymbol{y}^{T} \mathbf{X} \boldsymbol{w}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}\right] \\ &=-\frac{\partial \boldsymbol{y}^{T} \mathbf{X} \hat{w}}{\partial \hat{\boldsymbol{w}}}-\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}}{\partial \hat{\boldsymbol{w}}}+\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}} \end{aligned}由矩阵微分公式$\frac{\partial \boldsymbol{x}^{T} \boldsymbol{a}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{a}^{T} \boldsymbol{x}}{\partial \boldsymbol{x}}=\boldsymbol{a}, \frac{\partial \boldsymbol{x}^{T} \mathbf{B} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\mathbf{B}+\mathbf{B}^{T}\right) \boldsymbol{x}$可得： \begin{aligned} \frac{\partial E_{\hat{w}}}{\partial \hat{w}} &=-\mathbf{X}^{T} y-\mathbf{X}^{T} \boldsymbol{y}+\left(\mathbf{X}^{T} \mathbf{X}+\mathbf{X}^{T} \mathbf{X}\right) \hat{w} \\ &=2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-\boldsymbol{y}) \end{aligned}\begin{aligned} \frac{\partial^{2} E_{\hat{w}}}{\partial \hat{w} \partial \hat{w}^{T}} &=\frac{\partial}{\partial \hat{w}}\left(\frac{\partial E_{\hat{w}}}{\partial \hat{w}}\right) \\ &=\frac{\partial}{\partial \hat{w}}\left[2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-y)\right] \\ &=\frac{\partial}{\partial \hat{w}}\left(2 \mathbf{X}^{T} \mathbf{X} \hat{w}-2 \mathbf{X}^{T} \boldsymbol{y}\right) \\ &=2 \mathbf{X}^{T} \mathbf{X} \end{aligned}此即为Hessian矩阵③对损失函数$E_{\hat{\boldsymbol{w}}}$关于$\hat{w}$求一阶导数 \frac{\partial E_{\hat{w}}}{\partial \hat{w}}=2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-\boldsymbol{y})④令一阶导数等于0解出$\hat{\boldsymbol{w}}^{ \ast }$ \frac{\partial E_{\hat{w}}}{\partial \hat{w}}=2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-\boldsymbol{y})=02 \mathbf{X}^{T} \mathbf{X} \hat{w}-2 \mathbf{X}^{T} \boldsymbol{y}=02 \mathbf{X}^{T} \mathbf{X} \hat{w}=2 \mathbf{X}^{T} \boldsymbol{y}\hat{w}=\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \boldsymbol{y}对数几率回归公式广义线性模型 指数族分布指数族（Exponential family）分布是一类分布的总称，该类分布的分布律（或者概率密度函数）的一般形式如下： p(y ; \eta)=b(y) \exp \left(\eta^{T} T(y)-a(\eta)\right)其中，$\eta$称为该分布的自然参数；$T(y)$为充分统计量，视具体的分布而定，通常是等于随机变量y本身；$a(\eta)$为配分函数；$b(y)$为关于随机变量y的函数，常见的伯努利分布和正态分布均属于指数族分布。 证明伯努利分布属于指数族分布：已知伯努利分布的分布律为： p(y)=\phi^{y}(1-\phi)^{1-y}其中$y \in\{0,1\}, \phi$中为y=1的概率，即$p(y=1)=\phi$，对上式恒等变形可得： \begin{aligned} p(y) &=\phi^{y}(1-\phi)^{1-y} \\ &=\exp \left(\ln \left(\phi^{y}(1-\phi)^{1-y}\right)\right) \\ &=\exp \left(\ln \phi^{y}+\ln (1-\phi)^{1-y}\right) \end{aligned} \begin{aligned} p(y) &=\exp (y \ln \phi+(1-y) \ln (1-\phi)) \\ &=\exp (y \ln \phi+\ln (1-\phi)-y \ln (1-\phi)) \\ &=\exp (y(\ln \phi-\ln (1-\phi))+\ln (1-\phi)) \\ &=\exp \left(y \ln \left(\frac{\phi}{1-\phi}\right)+\ln (1-\phi)\right) \end{aligned}对比指数族分布的一般形式$p(y ; \eta)=b(y) \exp \left(\eta^{T} T(y)-a(\eta)\right)$可知：伯努利分布的指数族分布对应参数为： \begin{aligned} b(y) &=1 \\ \eta &=\ln \left(\frac{\phi}{1-\phi}\right) \\ T(y) &=y \\ a(\eta) &=-\ln (1-\phi)=\ln \left(1+e^{\eta}\right) \end{aligned} 广义线性模型的三条假设 在给$\mathcal{x}$ 的条件下，假设随机变量$y$服从某个指数族分布 在给定$\mathcal{x}$的条件下，我们的目标是得到一个模型$h(\boldsymbol{x})$能预估出$T(y)$的期望值； 假设该指数族分布中的自然参数$\eta$和$\mathcal{x}$呈线性关系，即$\eta=\boldsymbol{w}^{T} \boldsymbol{x}$参考文献：Andrew Ng.cs229-notes1 对数几率回归对数几率回归是在对一个二分类问题进行建模，并且假设被建模的随机变量y取值为0或1，因此我们可以很自然得假设y服从伯努利分布。此时，如果我们想要构建一个线性模型来预测在给定$\mathcal{x}$的条件下y的取值的话，可以考虑使用广义线性模型来进行建模。 对数几率回归的广义线性模型推导已知y是服从伯努利分布，而伯努利分布属于指数族分布，所以满足广义线性模型的第一条假设，接着根据广义线性模型的第二条假设我们可以推得模型$h(\boldsymbol{x})$的表达式为： h(\boldsymbol{x})=E[T(y | \boldsymbol{x})]由于伯努利分布的$T(y | \boldsymbol{x})=y | \boldsymbol{x}$所以： h(\boldsymbol{x})=E[y | \boldsymbol{x}]又因为$E[y | \boldsymbol{x}]=1 \times p(y=1 | \boldsymbol{x})+0 \times p(y=0 | \boldsymbol{x})=p(y=1 | \boldsymbol{x})=\phi$，所以 h(\boldsymbol{x})=\phi在前面证明伯努利分布属于指数族分布时我们知道： \begin{array}{l}{\begin{array}{l}{\eta=\ln \left(\frac{\phi}{1-\phi}\right)} \\ {e^{\eta}=\frac{\phi}{1-\phi}} \\ {e^{-\eta}=\frac{1-\phi}{\phi}} \\ {e^{-\eta}=\frac{1}{\phi}-1} \\ {1+e^{-\eta}=\frac{1}{\phi}} \\ {\frac{1}{1+e^{-\eta}}=\phi}\end{array}}\end{array}将$\phi=\frac{1}{1+e^{-\eta}}$代入$h(\boldsymbol{x})$的表达式可得： h(\boldsymbol{x})=\phi=\frac{1}{1+e^{-\eta}}根据广义模型的第三条假设$\eta=\boldsymbol{w}^{T} \boldsymbol{x}, h(\boldsymbol{x})$最终可化为： h(\boldsymbol{x})=\phi=\frac{1}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}}=p(y=1 | \boldsymbol{x})此即为对数几率回归模型。 3、 对数几率回归的参数估计极大似然估计法极大似然估计法：设总体的概率密度函数（或分布律）为$f\left(y, w_{1}, w_{2}, \dots, w_{k}\right), y_{1}, y_{2}, \dots, y_{m}$为从该总体中抽出的样本。因为$y_{1}, y_{2}, \dots, y_{m}$相互独立且同分布，于是，它们的联合概率密度函数（或联合概率）为： L\left(y_{1}, y_{2}, \ldots, y_{m} ; w_{1}, w_{2}, \dots, w_{k}\right)=\prod_{i=1}^{m} f\left(y_{i}, w_{1}, w_{2}, \dots, w_{k}\right)其中，$w_{1}, w_{2}, \dots, w_{k}$被看作固定但是未知的参数。当我们已经观测到一组样本观测值$y_{1}, y_{2}, \dots, y_{m}$时，要去估计未知参数，一种直观的想法就是，哪一组参数值使得现在的样本观测值出现的概率最大，哪一组参数可能就是真正的参数，我们就用它作为参数的估计值，这就是所谓的极大似然估计。极大似然估计的具体方法：通常记$L\left(y_{1}, y_{2}, \ldots, y_{m} ; w_{1}, w_{2}, \dots, w_{k}\right)=L(\boldsymbol{w})$，称其为似然函数。于是求$\boldsymbol{w}$的极大似然估计就归结为求$L(\boldsymbol{w})$的最大值点。由于对数函数是单调递增函数，所以 \ln L(\boldsymbol{w})=\ln \left(\prod_{i=1}^{m} f\left(y_{i}, w_{1}, w_{2}, \ldots, w_{k}\right)\right)=\sum_{i=1}^{m} \ln f\left(y_{i}, w_{1}, w_{2}, \ldots, w_{k}\right)与$L(\boldsymbol{w})$有相同的最大值点，，而在许多情况下，求$\ln L(\boldsymbol{w})$的最大值点比较简单，于是，我们就将求$L(\boldsymbol{w})$的最大值点转化为了求$\ln L(\boldsymbol{w})$的最大值点，通常称$\ln L(\boldsymbol{w})$为对数似然函数。对数几率回归的极大似然估计：已知随机变量y取1和0的概率分别为 \begin{array}{l}{p(y=1 | \boldsymbol{x})=\frac{e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}} \\ {p(y=0 | \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}}\end{array}令$\boldsymbol{\beta}=(\boldsymbol{w} ; b), \hat{\boldsymbol{x}}=(\boldsymbol{x} ; 1)$，则$\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$可简写为$\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}$，于是上式可化简为： \begin{aligned} p(y=1 | \boldsymbol{x}) &=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}} \\ p(y=0 | \boldsymbol{x}) &=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}} \end{aligned}记 \begin{array}{l}{p(y=1 | \boldsymbol{x})=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}=p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})} \\ {p(y=0 | \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}=p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})}\end{array}于是，使用一个小技巧即可得到随机变量y的分布律表达式 p(y | \boldsymbol{x} ; \boldsymbol{w}, b)=y \cdot p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})+(1-y) \cdot p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})或者 p(y | \boldsymbol{x} ; \boldsymbol{w}, b)=\left[p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{y}\left[p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{1-y}根据对数似然函数的定义可知 \ln L(\boldsymbol{w})=\sum_{i=1}^{m} \ln f\left(y_{i}, w_{1}, w_{2}, \dots, w_{k}\right)由于此时的y为离散型，所以将对数似然函数中的概率密度函数换成分布律即可 \ell(\boldsymbol{w}, b) :=\ln L(\boldsymbol{w}, b)=\sum_{i=1}^{m} \ln p\left(y_{i} | \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)将$p(y | \boldsymbol{x} ; \boldsymbol{w}, b)=y \cdot p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})+(1-y) \cdot p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})$代入对数似然函数可得 \ell(\boldsymbol{\beta})=\sum_{i=1}^{m} \ln \left(y_{i} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)+\left(1-y_{i}\right) p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)由于$p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}, \quad p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}$所以上式可化为 \begin{aligned} \ell(\boldsymbol{\beta}) &=\sum_{i=1}^{m} \ln \left(\frac{y_{i} e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}+\frac{1-y_{i}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}\right) \\ &=\sum_{i=1}^{m} \ln \left(\frac{y_{i} e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-y_{i}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}\right) \\ &=\sum_{i=1}^{m}\left(\ln \left(y_{i} e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-y_{i}\right)-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right) \end{aligned}由于 $y_{i} \in\{0,1\}$，所以当$y_{i}=0$时： \ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(\ln \left(0 \cdot e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-0\right)-\ln \left(1+e^{\boldsymbol{\beta}^{T} \boldsymbol{x}_{i}}\right)\right)=\sum_{i=1}^{m}\left(\ln 1-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{x}_{i}}\right)\right)=\sum_{i=1}^{m}\left(-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)当$y_{i}=1$时： \ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(\ln \left(1 \cdot e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-1\right)-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)=\sum_{i=1}^{m}\left(\ln e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)=\sum_{i=1}^{m}\left(\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)综合可得 \ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(y_{i} \boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)若$p(y | \boldsymbol{x} ; \boldsymbol{w}, b)=\left[p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{y}\left[p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{1-y}$，将其代入对数似然函数可得 \begin{aligned} \ell(\boldsymbol{\beta}) &=\sum_{i=1}^{m} \ln \left(\left[p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right]^{y_{i}}\left[p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right]^{1-y_{i}}\right) \\ &=\sum_{i=1}^{m}\left[\ln \left(\left[p_{1} ; \boldsymbol{\beta}\right)\right]^{y_{i}}\right)+\ln \left(\left[p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right]^{1-y_{i}}\right) ] \\ &=\sum_{i=1}^{m}\left[y_{i} \ln \left(p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)+\left(1-y_{i}\right) \ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right] \\ &=\sum_{i=1}^{m}\left\{y_{i}\left[\ln \left(p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)-\ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right]+\ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right\} \end{aligned} \ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left[y_{i} \ln \left(\frac{p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)}{p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)}\right)+\ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right]由于$p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}, \quad p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}$，上式可以化为： \begin{aligned} \ell(\boldsymbol{\beta}) &=\sum_{i=1}^{m}\left[y_{i} \ln \left(e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)+\ln \left(\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}\right)\right] \\ &=\sum_{i=1}^{m}\left(y_{i} \boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right) \end{aligned}]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数字图像处理（Digital Image Processing, DIP）]]></title>
    <url>%2F2019%2F06%2F17%2F%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%EF%BC%88Digital%20Image%20Processing%EF%BC%89%2F</url>
    <content type="text"><![CDATA[绪论图像由来在计算机2D图像用f(x,y),3D图像用f(x,y,z)表示图像（Image）：也叫位图，保存方式为点阵存储，也称为点阵图像或绘制图像图形（Graphic）：也叫矢量图，用数学方法描述存储，也称为面向对象的图像或绘图图像图像的数字化丢失信息原因：采样和编码丢失数据 图像的种类伪彩图像：图像中每个像素点用RGB索引表示 图像的处理流程数字图像处理的基本流程：图像预处理 - 图像分割 - 图像识别 - 图像建模模式识别：通过计算机用数学的方法来对不同模式进行自动处理和判读 图像的基本算法图像的灰度直方图和二值化 Histogram and Threshold; Binary OperationHistogram: 横轴表示灰阶，纵轴表示每个灰阶的像素数量灰度直方图的具有双峰性灰度直方图的双峰可以对应于图像中的前景和背景取最优二值化值的算法有Entropy Method, Otsu algorithm, Isodata algorithm最优二值化处于两个峰值之间二值化后的图像只有两个灰阶 图像卷积及其滤波 Convolution; Correlation图像卷积运算的过程：将卷积核覆盖上图像做运算前，要先将卷积核旋转180度，卷积核中心依次覆盖在图像上的每一个像素点上进行相乘并累加作为卷积核中心的值，卷积运算每一个步骤得到的值要存储新开的内存中，不能直接在原图上进行修改提取图像边缘：卷积核本质是利用梯度变化图像除噪：中值滤波，均值滤波，高斯滤波 图像的基本操作和特征 Basic Image Operations; Region Property图像间的基本操作：点操作，线性点操作， 非线性点操作，应用：对比度增强，二值化，边缘线，切割，暗亮调节，图像间噪声去除（星空图），多次曝光，背景去除，运动检测，图像提取（全1），几何操作（旋转，平移），插值图像的邻域操作：四邻域， 八邻域，可用于图像分割图像特征：周长，面积，中心，半径，直径，中心距（Centroid Moments）,朝向（orientation）,极值点，曲率，中值，平均值，extreme points VTK and ITK图像处理工具库 数学形态学二值形态学 Binary Morphology集合思想考虑结构化要素Structuring element(SE)：类似于kernel，原点可不在中心 膨胀（Dilation）：$D(F, K)=F \oplus K=\bigcup_{b \in K}(\{a+b | a \in F\})$求并集，填充小孔小洞 腐蚀（Erosion）：$E(F, K)=F \ominus K=\bigcap_{b \in K}(\{a-b | a \in F\})$求交集，删去细枝末节，不满足交换律，不可逆 开运算（Opening）：$O(F, K)=F \circ K=(F \ominus K) \oplus K$先腐蚀，后膨胀，去除孤立，狭小的区域，结构元素大小的不同将导致滤波效果的不同，图像进行重复的开操作会不会产生新的不同的结果 闭运算（Closing）：$C(F, K)=F \bullet K=(F \oplus K) \ominus K$先膨胀，后腐蚀， 填平小湖（即小孔），弥合小裂缝，结构元素大小的不同将导致滤波效果的不同 Hit-and-Miss：$\mathrm{F} \otimes \mathrm{K}= \left(\mathrm{F} \ominus \mathrm{K}_{1}\right) \cap\left(\mathrm{F}^{\mathrm{c}} \ominus \mathrm{K}_{2}\right)^{\mathrm{c}}, \mathrm{K}_{1} \cap \mathrm{K}_{2}=\varnothing, \mathrm{K}_{1} \in \mathrm{K}, \mathrm{K}_{2} \in \mathrm{K}$用于求取特殊的Pattern，可以得到轮廓线、孤立的点 Pattern Spectrum： 柱状图，在进行过程中需要变换SE，可以区分图像中不同形状的图案，不同尺寸的图案，不同形态的图像Pattern Spectrum不同 Distance Transform：重复腐蚀，相同SE，distance transform得到的结果中，所有邻域内的最大值点就是skeleton应用：Shape Inspection 不同SE进行开运算 灰度形态学 Grayscale Morphology灰度曲线考虑 灰度膨胀（Grayscale Dilation）：$D_{G}(F, K)=F \oplus_{g} K=\max_{[a, b] \in K}\{F(m+a, n+b)+K(a, b)\}$例：白区变白变大，黑区变小，会提高图像的整体亮度 灰度腐蚀（Grayscale Erosion）：$E_{G}(F, K)=F \ominus_{g} K=\min_{[ a, b ] \in K}\{F(m-a, n-b)-K(a, b)\}$例：白区变黑变小，黑区变大 灰度开运算（Grayscale Opening）：$O_{G}(F, K)=F \circ_{g} K=\left(F \ominus_{g} K\right) \oplus_{g} K$例：抹去小的白区，大的白区变化不大，可能会使图像变模糊 灰度闭运算（Grayscale Closing）：$C_{G}(F, K)=F \bullet_{g} K=\left(F \oplus_{g} K\right) \ominus_{g} K$例：小黑区变白，大的黑区不怎么变化，可能会使图像变模糊 分水岭算法 WatershedCondition Erosion：M（Maker）种子， V（Mask） 模板，需要不断重复灰度膨胀操作，可以获取图像中的特定区域，可以用来进行灰度重建Grayscale Reconstruction: Grayscale Opening by Reconstruction（OBR） Grayscale Closing by Reconstruction （CBR）分水岭算法（Watershed）：模仿地理结构中的山川，沟壑，可用于图像分割测地距离（Gedesic Distance）： 彩色图像和三维图像彩色图像RGBCMYKHIS 色相（Hue），饱和度（Saturation）和亮度 （Intensity） 三维图像光学断层成像 图像分割与形状轮廓模型重复性（Reproducibility）从大到小排序：全自动分割，手工勾画大体轮廓后自动分割，自动分割后手工修正，纯手工分割 形变模型DDC（Discrete Dynamic Contour）力：$\vec{f}_{i}^{tot}(t)=w^{i m} \vec{f}_{i}^{i m}+w^{i n} \vec{f}_{i}^{i n}+w^{d}\vec{f}_{i}^{d}$$\vec f_{i}^{i m}(t)$：$\vec{f}_{i}^{i m}=-\vec{\nabla} E\left(x_{i}, y_{i}\right)$图像力，使轮廓朝着能量降低的方向运动$\vec f_{i}^{i n}(t)$：内部约束力，通过减小局部曲率，使边缘变得光滑$\vec f_{i}^{d}(t)$：阻止顶点振动，使轮廓形变稳定 图像分割蛇形算法（snake）：水平集算法（Level Set Algorithm）：通过增加一个维度后的零水平集来表示轮廓，容易地对多个目标进行分割，容易地表示复杂的结构，初始轮廓的敏感性相对较小，在实际运用中，可以采用快速行进（Fast marching）与水平集的组合，以提高运算速度同时保证最后结果的精确 图像配准图像的配准（registration）：图像的融合（fusion），图像的匹配（matching）图像的叠加（superimposition） 多模态医学图像配准图像重采样上采样（Super-Sampling）：像素点数目增多下采样（Sub-Sampling）：像素点数目减少，像素点对应的实际空间范围增大 刚体配准线性配准（linear registration）：刚体配准是线性配准中的一部分 平移(Translation)， 缩放（Scaling）， 旋转（Rotation） Rigid3D， Rigid2D B-Splines 仿射变换（affine transformation） Intra-modal 均方误差（Mean Squared Difference）(MSD, minimise), 三维灰度直方图 正规化的互相关（Normalized cross correlation）（maximise） Entropy of difference（minimise） Mutal information （maximise） 正规化的互信息（Normalized mutual information）(maximise) Entropy correlation Coefficient(maximise) AIR (Automatic image registration) cost function (minimise) 非刚体配准可以用薄板样条（Thin plate spline）描述非线性形变（Non-linear deformation）三次b样条（Cubic B-spline）可以用来描述非线性形变非刚体配准的相似度测度（Non-rigid Registration Similarity Metrics） 均方误差（Mean squared error）理想值为0 正规化的互相关（Normalized cross correlation）NCC理想值为1 正规化的互信息（Normalized mutual information）NMI理想值为2 理想的联合直方图（Joint histogram）完全集中在对角线上 互信息测度(mutual information measures)互信息（Mutual information）联合熵（Joint entropy）联合熵的大小是对联合直方图（Joint histogram）的分散程度的描述 多解像度配准和评价评价配准效果多解像度（multi-resolution）图像配准 加快配准速度 增强配准鲁棒性（Robustness） 避免陷入局部最优值来源：mooc《数字图像处理》 顾力栩]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫基础知识整理]]></title>
    <url>%2F2019%2F06%2F02%2FPython%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[整理下Python涉及的基本知识，巩固和方便查阅。 Python爬虫的流程 获取网页request、urllib和selenium（模拟登陆），多进程多线程抓取、登陆抓取、突破IP封禁和服务器抓取 解析网页html文档，json格式文本可用re正则表达式、BeautifulSoup和lxml解析，二进制数据（如图片视频）保存 存储数据保存特定格式的文件如：txt, xls, csv等，存入MySQL数据库和存入MongoDB数据库 HTML+CSSHTMLHTML就是说明一个网页中包含哪些元素，HTML标签指由成对尖括号包围的关键词，解析网页常常需要根据这些标签进行定位。举几个例子：12345678&lt;a&gt; 定义锚&lt;dd&gt; 定义定义列表中项目的描述。&lt;div&gt; 定义文档中的节&lt;h1&gt; to &lt;h6&gt; 定义 HTML 标题&lt;li&gt; 定义列表的项目&lt;p&gt; 定义段落&lt;span&gt; 定义文档中的节&lt;ul&gt; 定义无序列表。 CSSCSS指层叠样式表 (Cascading Style Sheets)，就是用来让网页中的元素显示的更加美观。CSS 规则由两个主要的部分构成：选择器，以及一条或多条声明，每条声明由一个属性和一个值组成，例如：12345p&#123;color:red;text-align:center;&#125; id 和 class 选择器： 12345678910111213141516171819202122&lt;style&gt;#center&#123; text-align:center; color:red;&#125; &lt;/style&gt;&lt;body&gt; &lt;p id="center"&gt;Hello World!&lt;/p&gt;&lt;/body&gt;&lt;style&gt;p.center&#123; text-align:center;&#125;&lt;/style&gt;&lt;body&gt; &lt;p class="center"&gt;Hello World!&lt;/p&gt;&lt;/body&gt; id选择器和class选择器的区别就是class可以在多个元素中使用。id 选择器以 “#” 来定义class选择器以一个点”.”号显示 获取网页request库 请求方式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758requests.get('http://www.minhzou.top/') # 请求指定的页面信息，并返回实体主体requests.post('url') # 从请求服务器接受所指定的文档作为对所标识的URI的新的从属实体requests.put('url') # 从客户端向服务器传送的数据取代指定的文档的内容requests.head('url') # 只请求页面的首部response = requests.get('http://www.minhzou.top/')response = response.text# 添加headersheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'&#125;response = requests.get('http://www.minhzou.top/',headers = headers)# 使用params带参数请求data = &#123;'key1': 'value1','key2': 'value2'&#125;response = requests.get("http://www.minhzou.top/", params=data)# 解析jsonresponse = requests.get('http://www.minhzou.top/')print(response.json())print(json.loads(response.text))# .content获取二进制文件print（response.content）# post请求data = &#123;'key1': 'value1','key2': 'value2'&#125;response = requests.post("http://www.minhzou.top/", data=data)# 响应response = requests.get('http://www.minhzou.top/')response.status_coderesponse.headersresponse.cookiesresponse.url# 代理设置proxies = &#123; "http": "http://127.0.0.1:1234", "https": "https://127.0.0.1:1234",&#125;response = requests.get("http://www.minhzou.top/", proxies=proxies) # 超时设置import requestsfrom requests.exceptions import ReadTimeouttry: response = requests.get("http://www.minhzou.top/", timeout = 0.5) print(response.status_code)except ReadTimeout: print('Timeout') urllib库1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556urllib.request # 请求模块urllib.error # 异常处理模块urllib.parse # url解析模块urllib.robotparser # robots.txt解析模块# get请求import urllib.requestresponse = urllib.request.urlopen('http://www.baidu.com')print(response.read().decode('utf-8')) # response.read() 获取到网页的内容# urlopen常用参数urllib.requeset.urlopen(url,data,timeout)# 使用data参数 post请求data = bytes(urllib.parse.urlencode(&#123;'word': 'hello'&#125;), encoding='utf8')print(data)response = urllib.request.urlopen('http://www.minhzou.top/', data=data)print(response.read())# request添加头部信息from urllib import request, parseurl = 'https://www.lagou.com/'headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36', 'Host': 'www.lagou.com'&#125;dict = &#123; 'first': 'true', 'pn': '1', 'kd': 'python'&#125;data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, headers=headers, method='POST')response = request.urlopen(req)print(response.read().decode('utf-8'))# 代理，ProxyHandlerimport urllib.requestproxy_handler = urllib.request.ProxyHandler(&#123; 'http': 'http://127.0.0.1:1234', 'https': 'https://127.0.0.1:1234'&#125;)opener = urllib.request.build_opener(proxy_handler)response = opener.open('http://www.minhzou.top/')print(response.read())# cookie,HTTPCookiProcessorimport http.cookiejar, urllib.requestcookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')for item in cookie: print(item.name+"="+item.value) selenium库12345678910111213141516171819202122232425262728293031323334353637383940414243from selenium import webdriverchrome_driver = 'E:\chromedriver_win32\chromedriver.exe' # 需要下载适合浏览器版本的driverbrowser = webdriver.Chrome(executable_path=chrome_driver)input = browser.find_element_by_id("q")browser.get(url)browser.close()# 8种查找元素的方法 / element改成elements 多元素查找find_element_by_namefind_element_by_idfind_element_by_xpathfind_element_by_link_textfind_element_by_partial_link_textfind_element_by_tag_namefind_element_by_class_namefind_element_by_css_selectorfrom selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get("http://www.baidu.com")input = browser.find_element(By.ID,"q")print(input)browser.close()# 元素交互操作# 交互动作# 获取元素属性input.get_attribute('class')# 获取文本值, ID，位置，标签名textidlocationtag_namesize# 浏览器的前进和后退browser.back()browser.forward()# 选项卡管理browser.execute_script('window.open()') # 新开选项卡browser.switch_to_window(browser.window_handles[1]) 解析网页re正则表达式1234567891011121314151. 原子普通字符：abc非打印字符：\n, \t通用字符： \w, \W, \d, \D, \s, \S原子表：[abc],[^abc]2. 元字符任意匹配元字符：. 边界限制元字符： ^ ,$限定符：* , ?, +, &#123;n&#125;, &#123;n,&#125;, &#123;n,m&#125;模式选择符：|模式单元符：() 3. 模式修正re.I, re.M, re.L, re.U, re.S4. 贪婪模式与懒惰模式贪婪模式.* 加问号变成懒惰模式 .* ? XPath选择器 基本语法 123456nodename # 选取此节点的所有子节点/ # 从根节点选取// # 从匹配选择的当前节点选择文档中的节点，而不考虑他们的位置. # 选取当前节点.. # 选取当前节点的父节点@ # 选取属性 核心思想：写XPath就是写地址 123456//标签1[@属性1=“属性值1”]/标签2[@属性=“属性2”]/.../test()//标签1[@属性1=“属性值1”]/标签2[@属性=“属性2”]/.../@属性n/artical/div[1]/artical/div[position()&lt;3] 前两个元素//div[price&gt;3.5]//div[@class="center"]/ul/li/text() CSS选择器 基本语法 1234567891011121314151617 * # 选取所有节点#title # 选取 id 为 title 的元素.col-md # 选取所有 class 包含 col-md 的元素li a # 选取所有 li 下的 a 元素ul + p # 选取 ul 后面的第一个 p 元素div#title &gt; ul # 选取 id 为 title 的 div 的第一个 ul 子元素ul ~ p # 选取 与 url 相邻的所有 p 元素span#title ::text # 选取 id 为 title 的 span 元素的文本值a.link::attr(href) # 选取 class 为 link 的 a 元素的 href 属性值div:not(#content-container) # 选取所有id为非content-container 的diva:nth-child(2) # 选取下面第二个标签，如果是a的话则选取，不是则不取a:nth-child(2n) # 选取第偶数个a元素a:nth-child(2n+1) # 选取第奇数个a元素a[title] # 选取所有拥有title属性的a元素a[href=”https://www.lagou.com/”] # 选取所有href属性为https://www.lagou.com/的a元素a[href*=”www.lagou.com”] # 选取所有href属性值中包含www.lagou.com的a元素a[href^=”http”] # 选取所有href属性值中以http开头的a元素 例子 1234# 选取文字item.css('::text').extract()[1]# div a 选取所有div下所有a元素response.css('div a') BeautifulSoup提取数据的方式：find选择器，css选择器 12345678# find选择器soup.find_all(attrs=&#123;'name': 'elements'&#125;title = house.find('div', class_='house-title').a.text.strip()rooms = house.find('div', class_='details-item').span.textsoup.find_all(re.compile(), html)# css选择器soup.select("header h3"）ranking = school.select('td:nth-of-type(1)')[0].text lxml提取数据的方式：XPath选择器，css选择器 数据保存123456789101112131415# 写入xlsworkbook = xlwt.Workbook(encoding='utf-8')worksheet = workbook.add_sheet('house_price', cell_overwrite_ok=True)for k, row in enumerate(information): for j, col in enumerate(row): worksheet.write(k, j, col)workbook.save('house_price.xls')# 写入csvwith open('ranking.csv', 'a+', newline='') as f: writer = csv.writer(f) writer.writerow(data)# 写入txtfilepath = "E:/data.txt" with open(filepath, "a+") as fp: fp.write(data) Scrapy框架Scrapy框架：分布式，异步处理请求，自动去重包含部分：Scrapy Engine:控制数据流，触发事件Scheduler: 调度器Downloader:获取页面数据Spiders:获取Item和跟进的urlItem Pipeline:处理被Spide提取的Item Downloader middlewares: 更换代理IP，cookies, User-Agent，自动重试等Spider middleware：输出、处理异常等 只简单的在Scrapy框架上写过爬虫，因为要求不高所以很多功能自然也没用上，但是先了解了整个Scrapy，方便如果以后用得到拾起来。 App爬虫基本原理就是抓包分析python还可以使用uiautomato操作Android手机模拟人为操作 其他 异步更新技术AJAX(Asynchronous Javascript And XML，异步JavaScript和XML) 动态网页抓取的两种技术：通过浏览器审查元素解析真实网页地址和使用selenium模拟浏览器 Python中使用多线程的两种方法：① 函数式：调用_thread模块中的start_new_thread()函数产生新线程。 ② 调用Threading库创建线程，从threading.Thread继承。 小结整个python爬虫学习过程大概花了十多天，只爬过7-8个网站，但也遇到了很多情况，有了一定基础，正如有前辈说过的：如果你爬过80%的主流网站，下次遇到需求你就都能很快的响应。想起《曾国藩家书》的箴言：为学譬如熬肉，先须用猛火煮，然后用慢火温。到此，Python爬虫学习告一段落。 ps: 希望传上之后，这篇博客排版还能看吧。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卡特兰大数（Catalan大数）]]></title>
    <url>%2F2019%2F05%2F20%2F%E5%8D%A1%E7%89%B9%E5%85%B0%E5%A4%A7%E6%95%B0%EF%BC%88Catalan%E5%A4%A7%E6%95%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[卡特兰数（Catalan数）因为有很多应用，很重要也很常见，之前做某机试题就是以Catalan大数为背景，当然用java可以方便求解，但用C/C++公式求解对于大数很容易超过long long 型，所以要运用大整数技巧。 Catalan数的典型应用： 一个有n个结点的二叉树总共有多少种形态？n个非叶节点的满二叉树的形态数? 在一个圆上，有2*K个不同的结点，我们以这些点为端点，连K条线段，使得每个结点都恰好用一次。在满足这些线段将圆分成最少部分的前提下，请计算有多少种连线的方法（类似：将多边行划分为三角形问题。将一个凸多边形区域分成三角形区域(划分线不交叉)的方法数?） 出栈次序问题。一个栈(无穷大)的进栈序列为1,2,3,..n,有多少个不同的出栈序列? n*n的方格地图中，从一个角到另外一个角，不跨越对角线的路径数？ 括号化问题。一个合法的表达式由()包围，()可以嵌套和连接，如(())()也是合法 表达式，它们可以组成的合法表达式的个数为?…… 等等很多应用 Catalan数的公式递推关系：h(n)= h(0)h(n-1)+h(1)h(n-2) + … + h(n-1)h(0) (n&gt;=2)递推关系的解：h(n)=C(2n,n)/(n+1) (n=0,1,2,…)另类递推式：h(n)=h(n-1)(4n-2)/(n+1)递推关系的另类解：h(n)=c(2n,n)-c(2n,n-1)(n=0,1,2,…) java版Catalan大数123456789101112131415161718192021import java.io.BufferedInputStream;import java.math.BigInteger;import java.util.*;;public class test &#123; final static int maxn = 5000; static BigInteger[] C = new BigInteger[maxn]; public static void main(String[] args) &#123; Scanner input = new Scanner(new BufferedInputStream(System.in)); int n, k; while(input.hasNextInt()) &#123; n = input.nextInt(); C[0] = BigInteger.valueOf(1); for(k = 1; k &lt;= n; k++) &#123; C[k] = (C[k-1].multiply(BigInteger.valueOf(4*k - 2))).divide(BigInteger.valueOf(k+1)); &#125; System.out.println(C[n]); &#125; &#125;&#125; C语言版Catalan大数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//大卡特兰数// 递推公式 h(n) = ((4*n -2) / (n+1)) *h(n-1)#include &lt;stdio.h&gt;#define ll long longll a[100000000];int main()&#123; int n; while(scanf("%d", &amp;n) != EOF)&#123; a[0] = 1; //h(0) = 1; int d = 1; // 位数 for(int i = 1; i &lt;= n; i++)&#123; //处理乘法 ll num = 0; for(int j = 0; j &lt; d; j++)&#123; num = a[j]*(4*i -2) + num; a[j] = num % 10; num = num / 10; &#125; while(num &gt; 0)&#123; //最后计算完,处理进位 a[d] = num % 10; num = num / 10; d++; &#125; //处理除法 num = 0; for(int j = d - 1; j &gt;= 0; j--)&#123; num = a[j] + num*10; // if( num &lt; (i + 1)) a[j] = 0; else &#123; a[j] = num / (i + 1); num = num % (i + 1); &#125; &#125; while(a[d - 1] == 0 &amp;&amp; d &gt;= 2)&#123; //最后计算完，去除前面的0 d--; &#125; &#125; for(int i = d - 1; i &gt;= 0; i--)&#123; //倒序输出 printf("%lld", a[i]); &#125; printf("\n"); &#125; return 0;&#125;]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫实战——爬取特定微信公众号文章]]></title>
    <url>%2F2019%2F05%2F18%2FPython%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E7%88%AC%E5%8F%96%E7%89%B9%E5%AE%9A%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[前言学机器学习，学着学着就学偏了，就学着写爬虫了，目的主要是一熟悉下python，二解决自己某需求，这篇文章主要是解决需求写的代码中的主要部分——针对某一微信公众号的爬虫，当然除了学会了简单的爬虫，也运用了之前学习的很多知识，学有所用真的是一件非常开心的事情。 本篇目标 根据微信公众号在Sogou微信搜索，提取微信公众号链接 进入微信公众号界面，获取最近十篇微信文章链接 根据微信文章链接，解析网页，爬取所需文章内容 因为微信的反爬虫措施很多，每篇文章以及微信公众号的链接都是临时链接（从链接中包含timestamp字段可知）所以要另想办法，其中一种方法就是从Sogou微信搜索界面开始进行。 1. 确定URL直接在Sogou微信界面输入要搜索的微信公众号的具体名称，展示的第一条应该就是需要寻找的微信公众号，选取此时的url，例如：（已经输入搜索关键词）1https://weixin.sogou.com/weixin?type=1&amp;s_from=input&amp;query=%E5%B8%96%E6%9C%A8%E8%AE%B0&amp;ie=utf8&amp;_sug_=n&amp;_sug_type_= 2. 提取微信公众号的链接搜狗微信搜索出来的文章链接均为微信的临时链接，通过客户端查看的文章链接均为永久链接，所以只能从搜狗微信搜索界面进入。1234567891011121314151617# 网页有反爬虫机制，这里需要换上浏览器的headersheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'&#125;begin_url = 'https://weixin.sogou.com/weixin?type=1&amp;s_from=input&amp;query=%E5%B8%96%E6%9C%A8%E8%AE%B0&amp;ie=utf8&amp;_sug_=n&amp;_sug_type_='chrome_driver = 'E:\chromedriver_win32\chromedriver.exe'# 需要下载浏览器合适版本的driverbrowser = webdriver.Chrome(executable_path=chrome_driver)browser.get(begin_url)# 通过selenium用chrome模拟点击链接element = browser.find_element_by_link_text("帖木记")element.click()# 获取当前窗口的url，即微信公众号的链接browser.switch_to.window(browser.window_handles[1])now_url = browser.current_url 3. 获取微信文章的链接123456789101112131415161718192021# 获取文章id, 比如最近一天发的文章id为WXAPPMSG1000000013# 这里网页动态加载需要用selenium获取链接信息# 需要获取十篇只需要加个for循环对id_inc递减即可id_inc = 0start_id = 1000000013 + id_inctrue_id = "WXAPPMSG" + str(start_id)element = browser.find_element_by_id(true_id)content = element.get_attribute('innerHTML')# 无法直接获取链接，通过字符串切片获取文章的临时链接# 这种切片方式只获取了每天的第一条图文消息的链接str1 = 'hrefs="'right_url = content.partition(str1)[2]str2 = '"&gt;&lt;/span&gt;'right_url = right_url.partition(str2)[0]right_url = right_url.replace('amp;', '') # 去除多余字符# 前后合并形成一个完整的urlleft_url = 'https://mp.weixin.qq.com'url = left_url + right_urlprint(url) 4. 爬取微信文章的内容临时链接直接在浏览器中浏览不显示阅读数以及点赞数，所以只能获取文章的内容，数据需要想其他办法。 123456789101112131415# 用requests获取网页req = requests.get(url=url, headers=headers)# 用BeautifulSoup解析网页soup = BeautifulSoup(req.text, "html.parser")# 获取文章标题title = soup.h2.string.strip()# 获取文章内容contents = soup.find_all('p') # 等价于soup('p')with open('data.txt', "w+", encoding='utf-8') as fp: fp.write(title) for content in contents: fp.write(content.text + '\n') 5. 小结显然上面的代码显然可以优化的，主要是通过爬取微信文章用到了很多python库及技巧，比如：requests库，bs4库，selenium库和re正则表达式，字符串的处理，也包括验证码的识别，因为pytesseract识别率太低，自己又懒得去训练，也没必要花费选择现成的打码平台，所以就选择绕过验证码识别。当然最开心的是自己的小需求目前可以正常运行。 6. 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from bs4 import BeautifulSoupfrom selenium import webdriverfrom time import sleepimport requests# 网页有反爬虫机制，这里需要换上浏览器的headersheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'&#125;begin_url = 'https://weixin.sogou.com/weixin?type=1&amp;s_from=input&amp;query=%E5%B8%96%E6%9C%A8%E8%AE%B0&amp;ie=utf8&amp;_sug_=n&amp;_sug_type_='chrome_driver = 'E:\chromedriver_win32\chromedriver.exe' # 需要下载浏览器合适版本的driverbrowser = webdriver.Chrome(executable_path=chrome_driver)browser.get(begin_url)# 用selenium用chrome模拟点击链接element = browser.find_element_by_link_text("帖木记")element.click()# 获取当前窗口的url，即微信公众号的临时链接browser.switch_to.window(browser.window_handles[1])now_url = browser.current_urlsleep(1) # 停一秒# 获取文章id, 比如最近一天发的文章id为WXAPPMSG1000000013# 这里网页动态加载需要用selenium获取链接信息id_inc = 0start_id = 1000000013 + id_inctrue_id = "WXAPPMSG" + str(start_id)element = browser.find_element_by_id(true_id)content = element.get_attribute('innerHTML')# 无法直接获取链接，通过字符串切片获取文章的临时链接# 这种切片方式只获取了每天的第一条图文消息的链接str1 = 'hrefs="'right_url = content.partition(str1)[2]str2 = '"&gt;&lt;/span&gt;'right_url = right_url.partition(str2)[0]right_url = right_url.replace('amp;', '') # 去除多余字符# 前后合并形成一个完整的urlleft_url = 'https://mp.weixin.qq.com'url = left_url + right_url# 用requests获取网页req = requests.get(url=url, headers=headers)# 用BeautifulSoup解析网页soup = BeautifulSoup(req.text, "html.parser")# 文章标题获取title = soup.h2.string.strip()# 文章内容获取contents = soup.find_all('p') # 等价于soup('p')with open('data.txt', "w+", encoding='utf-8') as fp: fp.write(title) for content in contents: fp.write(content.text + '\n')]]></content>
      <categories>
        <category>实战</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向量，矩阵求导]]></title>
    <url>%2F2019%2F05%2F16%2F%E5%90%91%E9%87%8F%EF%BC%8C%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[向量求导有两种布局方式，分子布局和分母布局。分子布局和分母布局的操作结果可以通过转置来切换。分母布局：标量对向量求导，得到的结果跟分母上的向量保持一致。即，如果标量是对列向量求导，得到的导数也是列向量。分子布局：标量对向量求导，得到的结果是分母上的向量的转置。即，如果标量对列向量求导，得到的导数将是行向量。以下以分母布局为例： A=\left[ \begin{array}{cccc}{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\ {a_{21}} & {a_{22}} & {\cdots} & {a_{2 n}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots}\\ {a_{m 1}} & {a_{m 2}} & {\cdots} & {a_{m n}}\end{array}\right]，x=\left[ \begin{array}{c}{x_{1}} \\ {x_{2}} \\ {\vdots} \\ {x_{n}}\end{array}\right] A x=\left[ \begin{array}{c}{a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}} \\ {a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}} \\ {\vdots} \\ {a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}}\end{array}\right]_{m×1} \frac{\partial A x}{\partial x}=\left[ \begin{array}{cccc}{a_{11}} & {a_{21}} & {\ldots} & {a_{m 1}} \\ {a_{12}} & {a_{22}} & {\ldots} & {a_{m 2}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {a_{1 n}} & {a_{2 n}} & {\ldots} & {a_{m n}}\end{array}\right]=A^{T}以分母作为主序，x是列向量，Ax关于x求导数，那么对Ax的分量(每行)分别对x的分量求偏导数，然后整理排成一列，对每个分量都如此操作。 可以得到如下结论： \frac{\partial A x}{x}=A^{T} \frac{\partial A x}{x^{T}}=A \frac{\partial x^{T} A x}{x}=\left(A^{T}+A\right) x \frac{\partial x^{T} y}{x}=y元素对向量，矩阵求导： 元素对行向量求导设y是元素，$\mathbf{x}^{T}=\left[ \begin{array}{lll}{x_{1}} &amp; {\cdots} &amp; {x_{q}}\end{array}\right]$是q维行向量，则$\frac{\partial y}{\partial \mathbf{x}^{r}}=\left[\frac{\partial y}{\partial x_{1}} \ldots \frac{\partial y}{\partial x_{q}}\right]$ 元素对列向量求导设y是元素，$\mathbf{x}=\left[ \begin{array}{c}{x_{1}} \\ {\vdots} \\ {x_{p}}\end{array}\right]$是p维列向量，则$\frac{\partial y}{\partial \mathbf{x}}=\left[ \begin{array}{c}{\frac{\partial y}{\partial x_{1}}} \\ {\vdots} \\ {\frac{\dot{\partial} y}{\partial x_{p}}}\end{array}\right]$ 元素对矩阵求导设y是元素，$X=\left[ \begin{array}{ccc}{x_{11}} &amp; {\cdots} &amp; {x_{1 q}} \\ {\vdots} &amp; &amp; {\vdots} \\ {x_{p 1}} &amp; {\cdots} &amp; {y_{p q}}\end{array}\right]$是p×q矩阵，则$\frac{\partial y}{\partial X}=\left[ \begin{array}{ccc}{\frac{\partial y}{\partial x_{11}}} &amp; {\cdots} &amp; {\frac{\partial y}{\partial x_{1 q}}} \\ {\vdots} \\ {\frac{\partial y}{\partial x_{p 1}}} &amp; {\cdots} &amp; {\frac{\partial y}{\partial x_{p q}}}\end{array}\right]$```]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++梳理补缺]]></title>
    <url>%2F2019%2F04%2F28%2FC%20and%20C%2B%2B%E6%A2%B3%E7%90%86%E8%A1%A5%E7%BC%BA%2F</url>
    <content type="text"><![CDATA[1. 标准名字空间 &amp; C++标准输入输出流 标准名字空间 cout是标准名字空间std的一个名字。必须加上名字空间限定 std:: std::cout 例如不同班级都可能有同名的学生，如“张伟”计科::张伟 机械::张伟 也可以用：using std::cout using namespace std C++标准输入输出流 cout 是一个标准输出流变量（对象），代表控制台窗口 &lt;&lt;是一个运算符，假如o是一个输出流对象，x是一个数据， o&lt;&lt; x 123456789101112131415#include &lt;iostream&gt;#include &lt;fstream&gt; //文件输入输出 #include &lt;string&gt;using namespace std;int main()&#123; ofstream oF("1.txt"); oF &lt;&lt; 3.14 &lt;&lt;" "&lt;&lt; "hello world\n"; oF.close(); ifstream iF("1.txt"); double d; string str; iF &gt;&gt; d &gt;&gt; str; cout&lt;&lt;d&lt;&lt;" "&lt;&lt; str&lt;&lt;endl; return 0; &#125; 2. 引用变量和引用参数 引用变量 引用变量是其他变量的别名。如同一个人的外号或小名 既然是引用，定义引用变量时就必须指明其引用的是哪个变量int a = 3;int &amp;r = a; 引用变量“从一而终”， 一旦定义就不能再引用其他变量int &amp;r=a;int &amp;r = b; //错 引用变量和被引用的变量类型必须匹配double d;int &amp;r = d; 对引用变量的操作就是对它引用的变量的操作 12345678910#include &lt;iostream&gt;using namespace std;int main()&#123; int a = 3, &amp;r = a; cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; r&lt;&lt; endl; r = 5; cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; r &lt;&lt;endl; return 0;&#125; 函数的值形参 C函数的形参都是值参数，形参作为函数的局部变量有自己单独的内存块，当函数调用时，实参将值拷贝（赋值给）形参。对形参的修改不会影响实参 1234567891011121314151617#include &lt;iostream&gt;using namespace std;void swap(int x, int y)&#123; cout &lt;&lt; x &lt;&lt; '\t' &lt;&lt; y&lt;&lt; endl; int t = x; x = y; y = t; cout &lt;&lt; x &lt;&lt; '\t' &lt;&lt; y&lt;&lt; endl;&#125;int main()&#123; int a = 3, b = 4; cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b&lt;&lt; endl; swap(a,b); cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b &lt;&lt;endl; return 0; &#125; 函数的值形参：传递指针 12345678910111213141516#include &lt;iostream&gt;using namespace std;void swap(int *x, int *y)&#123; //指针 x, y cout &lt;&lt; x &lt;&lt; '\t' &lt;&lt; y&lt;&lt; endl; //输出的是a，b的地址 int t = *x; // 取x指针指向的值赋值给t *x = *y; // 同理 *y = t; // 同理 &#125;int main()&#123; int a = 3, b = 4; cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b&lt;&lt; endl; swap(&amp;a,&amp;b); // &amp;取地址 a, b地址 传给x，y cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b &lt;&lt;endl; return 0; &#125; 函数的引用形参：引用实参 12345678910111213141516#include &lt;iostream&gt;using namespace std;void swap(int &amp;x, int &amp;y)&#123; // &amp;引用变量 cout &lt;&lt; x &lt;&lt; '\t' &lt;&lt; y&lt;&lt; endl; int t = x; x = y; y = t; &#125;int main()&#123; int a = 3, b = 4; cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b&lt;&lt; endl; swap(a,b); cout &lt;&lt; a &lt;&lt; '\t' &lt;&lt; b &lt;&lt;endl; return 0; &#125; 3. 函数的默认形参、函数重载 函数的形参可以有默认值void print(char ch, int n = 1) 默认形参必须在非默认形参右边，即一律靠右add(x = 1, y, z = 3); // 错add(y, x = 1, z = 3); // OK 1234567891011#include &lt;iostream&gt;using namespace std;void print(char ch, int n = 1) &#123; for (int i = 0; i &lt; n; i++) cout &lt;&lt; ch;&#125;int main() &#123; print('*'); cout &lt;&lt; endl; print('*',3); cout &lt;&lt; endl; print('*',5); cout &lt;&lt; endl;&#125; 12345678910#include &lt;iostream&gt;using namespace std;int add(int x,int y=2,int z=3) &#123; return x + y + z;&#125;int main() &#123; cout &lt;&lt; add(5)&lt;&lt;endl; cout &lt;&lt; add(5,7) &lt;&lt; endl; cout &lt;&lt; add(5,7,9) &lt;&lt; endl;&#125; 函数重载 C++允许同一作用域里有同名的函数，只要他们的形参不同。如：int add(int x, int y);double add(double x, double y); 函数名和形参列表构造了函数的签名 函数重载不能根据返回类型区分函数。如int add(int x, int y);double add(int x, int y); 12345678910111213#include &lt;iostream&gt;using namespace std;int add(int x, int y = 2) &#123; return x + y ;&#125;double add(double x, double y = 2.0) &#123; return x + y;&#125;int main() &#123; cout &lt;&lt; add(5,3) &lt;&lt; endl; cout &lt;&lt; add(5.3, 7.8) &lt;&lt; endl; cout &lt;&lt; add((double)5, 7.8) &lt;&lt; endl;//歧义性加double强制转换 &#125; 4. 函数模板 函数模板 通用算法：函数模板。也称为泛型算法 用template关键字增加一个模板头，将数据类型变成类型模板参数 1234template&lt;typename T&gt; T add(T x, T y) &#123; return x + y; &#125; 给模板参数传递实际的模板参数 12cout &lt;&lt; add&lt;int&gt;(5, 3) &lt;&lt; endl;cout &lt;&lt; add&lt;double&gt;(5.3, 7.8) &lt;&lt; endl; 函数模板实例化 123456789101112131415161718#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;template&lt;typename T&gt; T add(T x, T y) &#123; return x + y;&#125;int main() &#123;#if 1 cout &lt;&lt; add&lt;int&gt;(5, 3) &lt;&lt; endl; cout &lt;&lt; add&lt;double&gt;(5.3, 7.8) &lt;&lt; endl; cout &lt;&lt; add&lt;int&gt;(4, 6) &lt;&lt; endl; cout &lt;&lt; add&lt;string&gt;("hello", "world") &lt;&lt; endl;#else cout &lt;&lt; add(5, 3) &lt;&lt; endl; cout &lt;&lt; add(5.3, 7.8) &lt;&lt; endl;#endif&#125; 模板参数自动推断 5. 用户自定义类型string和vector string 是一个用户定义类型，表示的是符串。string s = “hello”, s2(“world”); 用成员访问运算符.访问string类的成员12cout &lt;&lt; s.size() &lt;&lt; endl;string s3 = s.substr(1, 3); 内在的数组（静态数组） 123456789#include &lt;iostream&gt;using std::cout;int main() &#123;int arr[] = &#123; 10,20,30,40 &#125;; //大小固定，以后不能添加更多int值for (int i = 0; i &lt; 4; i++) &#123; cout &lt;&lt; arr[i] &lt;&lt; '\t'; cout &lt;&lt; '\n';&#125;#endif vector 向量，类似于数组，但可以动态增长。头文件 &lt;vector&gt; 是一个类模板，实例化产生一个类，如vector&lt;int&gt; 产生一个数据元素是int的vector&lt;int&gt; 类（向量）。 同样，可以通过vector&lt;int&gt; 类对象去访问其他成员，如成员函数。 同样可以用运算符进行一些运算。 6. 指针和动态内存分配 指针 指针就是地址，变量的指针就是变量的地址。可以用 取值地址符&amp; 获得一个变量的地址。如： &amp;var 指针变量就是存储指针（地址）的变量。如：T *p; //p是存储 “T类型变量的地址”的变量 通过取内容运算符 *可以得到一个指针变量指向的变量。*p就是p指向的那个变量 1234567891011121314151617181920/*指针就是地址，变量的指针就是变量的地址指针变量就是存储指针（地址）的变量*/#include &lt;iostream&gt;using namespace std;int main() &#123; int a=3; int *p = &amp;a; //取地址运算符&amp;用于获得a的地址：&amp;a cout &lt;&lt; p &lt;&lt; '\t' &lt;&lt; &amp;a &lt;&lt; endl; //取内容运算符*用于获得指针指向的变量(内存块) cout &lt;&lt; *p &lt;&lt; '\t' &lt;&lt; a &lt;&lt; endl; //*p就是a *p = 5; //即a = 5; cout &lt;&lt; *p &lt;&lt; '\t' &lt;&lt; a &lt;&lt; endl;#if 0 int *q = p; //q和p值相同，都是a的地址(指针) cout &lt;&lt; *p &lt;&lt; '\t' &lt;&lt; *q &lt;&lt; '\t' &lt;&lt; a &lt;&lt; endl; char *s = &amp;a; //int *#endif&#125; 1234567891011121314/*用指针访问数组元素*/#include &lt;iostream&gt;using namespace std;int main() &#123; int arr[] = &#123; 10,20,30,40 &#125;; int *p = arr;//数组名就是数组第一个元素的地址，即arr等于&amp;(arr[0]) // p[i]就是*(p+i) cout &lt;&lt; *(p + 2) &lt;&lt; '\t' &lt;&lt; p[2] &lt;&lt; '\t' &lt;&lt; arr[2] &lt;&lt; endl; for (int *q = p + 4; p &lt; q; p++) cout &lt;&lt; *p &lt;&lt; '\t'; cout &lt;&lt; '\n';&#125; 动态内存分配 123456789#include &lt;cstdio&gt;#include &lt;malloc.h&gt;int main()&#123; char *s = (char *)malloc(10*sizeof(s)); // char *s 指针， （char *）指针类型， sizeof s用内存大小 s = "hello"; puts(s); return 0; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344/*malloc free realloc 动态内存分配：new用于申请内存块、delete用于释放内存块 T *p = new T; delete p; T *q = new T[5]; delete[] q;*/#if 1 // 堆存储区#include &lt;iostream&gt;using namespace std;int main() &#123; int *p = new int; //malloc *p = 3; cout &lt;&lt; p &lt;&lt; '\t' &lt;&lt; *p &lt;&lt; endl; delete p; //如果没有这行，内存泄漏 p = new int; *p = 5; cout &lt;&lt; p &lt;&lt; '\t' &lt;&lt; *p &lt;&lt; endl; delete p;&#125;#else#include &lt;iostream&gt;using namespace std;int main() &#123; int n = 4; int *p = new int[n]; for (int i = 0; i &lt; n; i++) p[i] = 2 * i + 1; for (int *q = p + n; p &lt; q; p++) cout &lt;&lt; *p &lt;&lt; '\t'; cout &lt;&lt; '\n'; char *s = (char *)p; char ch = 'A'; int n2 = n * sizeof(int) / sizeof(char); for (int i = 0; i &lt; n2; i++) s[i] = ch + i; for (char *r = s+n2; s &lt; r; s++) cout &lt;&lt; *s; cout &lt;&lt; '\n'; delete[] p;&#125;#endif 7. 类和对象 面向对象编程 传统的过程式编程：变量（对象）就是一些存储数据的内存块，而过程（函数）对这些数据进行处理。 面向对象编程：程序是由不同种类的许多对象相互协作完成的。对象之间通过发送/接收消息来协作完成各种任务。由这些对象构成的程序也称为“对象式系统”。 面向对象设计 人驾驶车：设计对象“人”和“车” 从具有共同特征的许多抽象出某种概念，如“人”，“车” 某些概念之间可能存在某种关系 组合（包含）关系 继承和派生关系 不同思考方式：面向对象编程 vs 过程式编程 过程式编程：用内在类型（概念）如int、double表示数据，用面向这些机器类型的概念去解决复杂问题，不易于思考问题 面向对象编程：用现实世界中的概念（人、车、地图）来思考问题。更自然、更易于理解、易于查错、易于组装（组件式开发） C++的面向对象特性：用户定义类型 程序员定义自己的“用户定义类型” 如类类型，来表示各种应用问题中的各种概念 C++ 标准库已经提供了很多实用的“用户定义类型”， 是C++标准库的程序员实现的 cout是一个ostream类的对象（变量），cin是一个istream的对象（变量）。可以向它们发送消息：cout &lt;&lt; “hello world”; string是一个表示字符串的类。向一个string对象发送一个size()消息，查询该对象包含的字符数目string str = “hello world”; cout &lt;&lt; str.size(); 一个用户定义类型包括： 有哪些属性？ 有哪些操作（运算）？ 不同属性或操作的访问权限？哪些是（类）外部可以访问？哪些是仅仅内部才能访问的？ 面向对象设计要考虑多个用户定义类型的关系 不同类型的对象之间是继承还是包含关系？ 程序：哪些具体对象如何进行交互协作 类和对象 用struct或class关键字定义一个类。定义的类就是一个数据类型。struct student{ string name; double score;} ; 类类型的变量通常称为对象。如：student stu; 对象就是类的一个实例 成员访问运算符 访问类对象的成员stu.name = “LiPing”;stu.score = 78.5; 对象数组 和内在一样，可以定义类类型的数组。存储一组类对象。 类类型的指针变量 T是一个类类型，则T *就是T指针类型。如int *是int指针类型。 T *变量可以指向一个类对象。student stu;student *p = &stu;student students[3];p = students + 2; 间接访问运算符-&gt;、取内容运算符*student stu;student p = &stu;(\p).name = “LiPing”; //*p就是p指向的变量stup-&gt;score = 78; // p指向的类对象的成员score 指向可以指向动态分配的对象 类的成员函数 类体外定义成员函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344/*输入一组学生成绩(姓名和分数)，输出：平均成绩、最高分和最低分。当然，也要能输出所有学生信息*/#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;vector&gt;using namespace std;struct student&#123; string name; double score; void print();&#125;;void student::print() &#123; cout &lt;&lt; name &lt;&lt; " " &lt;&lt; score &lt;&lt; endl;&#125;int main() &#123;#if 0 student stu; stu.name = "Li Ping"; stu.score = 78.5; stu.print();#endif vector&lt;student&gt; students; while (1) &#123; student stu; cout &lt;&lt; "请输入姓名 分数:\n"; cin &gt;&gt; stu.name &gt;&gt; stu.score; if (stu.score &lt; 0) break; students.push_back(stu); &#125; for (int i = 0; i &lt; students.size(); i++) students[i].print(); double min = 100, max=0, average = 0; for (int i = 0; i &lt; students.size(); i++) &#123; if (students[i].score &lt; min) min = students[i].score; if (students[i].score &gt; max) max = students[i].score; average += students[i].score; &#125; average /= students.size(); cout &lt;&lt; "平均分、最高分、最低分："&lt;&lt; average &lt;&lt; " " &lt;&lt; max &lt;&lt; " " &lt;&lt; min &lt;&lt; endl;&#125; 8. this指针， 访问控制，构造函数 this指针 12345678910111213141516171819/*this指针: 成员函数实际上隐含一个this指针。*/#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;struct student &#123; string name; double score; void print() &#123; cout &lt;&lt; this-&gt;name &lt;&lt; " " &lt;&lt; this-&gt;score &lt;&lt; endl; &#125;&#125;;int main() &#123; student stu; stu.name = "Li Ping"; stu.score = 78.5; stu.print(); // print(&amp;stu);&#125; 访问控制 123456789101112131415161718192021222324252627282930313233/*struct和class区别：struct里的成员默认是public(公开的)class里的成员默认是private(私有的)*/#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class student&#123;public: //接口 void print() &#123; cout &lt;&lt; this-&gt;name &lt;&lt; " " &lt;&lt; this-&gt;score &lt;&lt; endl;&#125; string get_name() &#123; return name; &#125; double get_score() &#123; return score; &#125; void set_name(string n) &#123; name = n; &#125; void set_score(double s) &#123; score = s; &#125;private: string name; double score;&#125;;int main() &#123; student stu;// stu.name = "Li Ping";// stu.score = 78.5; stu.set_name("Li Ping"); stu.set_score(78.5); stu.print(); // print(&amp;stu); cout &lt;&lt; stu.get_name() &lt;&lt; " " &lt;&lt; stu.get_score() &lt;&lt; endl;&#125; 构造函数 123456789101112131415161718192021222324252627282930/*构造函数： 函数名和类名相同且无返回类型的成员函数。*/#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class student&#123; string name; double score;public: // student()&#123;&#125; // 默认构造函数 student()&#123; name = "NO"; score = 0; cout &lt;&lt; "构造函数\n"; &#125; student(string n,double s)&#123; //不是默认构造函数 name = n; score = s; cout &lt;&lt; "构造函数\n"; &#125; void print() &#123; cout &lt;&lt; this-&gt;name &lt;&lt; " " &lt;&lt; this-&gt;score &lt;&lt; endl; &#125;&#125;;int main() &#123; //student stu;//在创建一个类对象时会自动调用称为“构造函数”的成员函数 student stu("LiPing",80.5); stu.print(); //student students[3]; 数组必须有默认构造函数 &#125; 9. 运算重载符12345678910111213141516171819202122232425262728293031/*运算符重载：针对用户定义类型重新定义运算符函数*/#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class student &#123; string name; double score;public: student(string n, double s) &#123; name = n; score = s; &#125; //友元函数 friend ostream&amp; operator&lt;&lt;(ostream &amp;o, student s); friend istream&amp; operator&gt;&gt;(istream &amp;in, student &amp;s);&#125;;ostream&amp; operator&lt;&lt;(ostream &amp;o, student s) &#123; cout &lt;&lt; s.name &lt;&lt; "," &lt;&lt; s.score &lt;&lt; endl; return o; &#125;istream&amp; operator&gt;&gt;(istream &amp;in, student &amp;s) &#123; in &gt;&gt; s.name &gt;&gt; s.score; return in;&#125;int main() &#123; student stu("LiPing", 80.5); cin &gt;&gt; stu; //operator&gt;&gt;(cin,stu) cout &lt;&lt; stu; //operator&lt;&lt;(cout,stu)&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class Point&#123; double x, y;public: double operator[](int i) const&#123; //const函数 if (i == 0) return x; else if (i == 1) return y; else throw "下标非法!"; //抛出异常 &#125; double&amp; operator[](int i) &#123; if (i == 0) return x; else if (i == 1) return y; else throw "下标非法!"; //抛出异常 &#125; Point(double x_,double y_) &#123; x = x_; y = y_; &#125; Point operator+(const Point q) &#123; return Point(this-&gt;x+q[0],this-&gt;y + q[1]); &#125; //友元函数 friend ostream &amp; operator&lt;&lt;(ostream &amp;o, Point p); friend istream &amp; operator&gt;&gt;(istream &amp;i, Point &amp;p);&#125;;ostream &amp; operator&lt;&lt;(ostream &amp;o, Point p) &#123; o &lt;&lt;p.x &lt;&lt; " " &lt;&lt; p.y&lt;&lt; endl; return o;&#125;istream &amp; operator&gt;&gt;(istream &amp;i, Point &amp;p) &#123; i &gt;&gt; p.x &gt;&gt; p.y; return i;&#125;#if 0Point operator+(const Point p,const Point q) &#123; return Point(p[0] + q[0], p[1] + q[1]);&#125;#endifint main() &#123; Point p(3.5, 4.8),q(2.0,3.0);#if 0// cin &gt;&gt; p; cout &lt;&lt; p; cout &lt;&lt; p[0] &lt;&lt; "-" &lt;&lt; p[1] &lt;&lt; endl; //p.operator[](0) p[0] = 3.45; p[1] = 5.67; cout &lt;&lt; p;#endif cout &lt;&lt; p&lt;&lt;q; Point s = p + q; //p.operator+(q) vs operator+(p,q) cout &lt;&lt; s;&#125; 10. String类、拷贝构造函数、析构函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include &lt;iostream&gt;using namespace std;class String &#123; char *data; //C风格的字符串 int n;public: ~String() &#123; //析构函数，析构的顺序与创建的相反顺序 cout &lt;&lt;n&lt;&lt; " 析构函数\n"; if(data) delete[] data; &#125;#if 1 String(const String &amp;s) &#123; //硬拷贝 cout &lt;&lt; "拷贝构造函数\n"; data = new char[s.n + 1]; n = s.n; for (int i = 0; i &lt; n; i++) data[i] = s.data[i]; data[n] = '\0'; &#125;#endif String(const char *s=0) &#123; //构造函数 cout &lt;&lt; "构造函数\n"; if (s == 0) &#123; cout &lt;&lt; "s==0\n"; data = 0; n = 0; return; &#125; const char *p = s; while (*p) p++; n = p - s; //String长 data = new char[n + 1]; //分配内存空间 for (int i = 0; i &lt; n; i++)//存储String data[i] = s[i]; data[n] = '\0'; &#125; int size() &#123; return n; &#125; char operator[](int i)const &#123; //下标运算符，不可以修改 if (i&lt;0 || i&gt;=n ) throw "下标非法"; return data[i]; &#125; char&amp; operator[](int i) &#123; //下标运算符，可以修改 if (i &lt; 0 || i &gt;= n) throw "下标非法"; return data[i]; &#125;&#125;;ostream &amp; operator&lt;&lt;(ostream &amp;o, String s) &#123; for (int i = 0; i &lt; s.size(); i++) cout &lt;&lt; s[i]; return o;&#125;void f() &#123; String str,str2("hello world"); str2[1] = 'E';// cout &lt;&lt; str2 &lt;&lt; endl;#if 1 String s3 = str2; //拷贝构造函数 cout &lt;&lt; s3 &lt;&lt; endl; s3[3] = 'L'; cout &lt;&lt; s3 &lt;&lt; endl; cout &lt;&lt; str2 &lt;&lt; endl;#endif&#125;int main() &#123; f();&#125; 11. 类模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/*类模拟vector&lt;int&gt;的类Vector*/#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;class student &#123; string name; double score;public: student(string n="no", double s=0) &#123; //申请了空间必须要默认 name = n; score = s; &#125; friend ostream&amp; operator&lt;&lt;(ostream &amp;o, student s);&#125;;ostream&amp; operator&lt;&lt;(ostream &amp;o, student s) &#123; cout &lt;&lt; s.name &lt;&lt; "," &lt;&lt; s.score &lt;&lt; endl; return o;&#125;//类模板template&lt;typename T&gt; //加这一句，改数据类型 class Vector &#123; T *data; int capacity; int n;public: Vector(int cap=3) &#123; data = new T[cap]; if (data == 0) &#123; cap = 0; n = 0; return; &#125; capacity = cap; n = 0; &#125; void push_back(T e) &#123; if (n == capacity) &#123;//空间已经满 cout &lt;&lt; "增加容量！\n"; T *p = new T[2 * capacity]; if (p) &#123; for (int i = 0; i &lt; n; i++) p[i] = data[i]; delete[] data; data = p; capacity = 2*capacity; &#125; else &#123; return; &#125; &#125; data[n] = e; n++; &#125; T operator[](int i) const&#123; if (i &lt; 0 || i &gt;= n) throw "下标非法!"; return data[i]; &#125; int size() &#123; return n; &#125;&#125;;int main() &#123; Vector&lt;student&gt; v; v.push_back(student("Li",45.7)); v.push_back(student("Wang", 45.7)); v.push_back(student("zhao", 45.7)); for (int i = 0; i &lt; v.size(); i++) cout &lt;&lt; v[i] ; cout &lt;&lt; endl; v.push_back(student("zhang", 45.7)); v.push_back(student("Liu", 45.7)); for (int i = 0; i &lt; v.size(); i++) cout &lt;&lt; v[i]; cout &lt;&lt; endl;#if 0#if 1 Vector&lt;int&gt; v; v.push_back(3); v.push_back(4); v.push_back(5); for(int i = 0 ; i&lt;v.size();i++) cout&lt;&lt;v[i]&lt;&lt;'\t'; cout &lt;&lt; endl; v.push_back(6); v.push_back(7); for (int i = 0; i &lt; v.size(); i++) cout &lt;&lt; v[i] &lt;&lt; '\t'; cout &lt;&lt; endl;#else Vector&lt;string&gt; v; v.push_back("hello"); v.push_back("world"); v.push_back("sdfasdf"); for (int i = 0; i &lt; v.size(); i++) cout &lt;&lt; v[i] &lt;&lt; '\t'; cout &lt;&lt; endl; v.push_back("ggg"); v.push_back("hhh"); for (int i = 0; i &lt; v.size(); i++) cout &lt;&lt; v[i] &lt;&lt; '\t'; cout &lt;&lt; endl;#endif#endif&#125; 学习来源：b站：从C到C++快速入门(2019版)]]></content>
      <tags>
        <tag>C/C++</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习的数学基础]]></title>
    <url>%2F2019%2F04%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[预备了一下学习机器学习需要的数学基础知识，补充了一些不清楚的数学知识 概览 算法或理论 用到的数学知识 贝叶斯分类器 随机变量，贝叶斯公式，随机变量独立性，正太分布，最大似然估计 决策树 概率，熵，Gini系数 KNN算法 距离函数 主成分分析 协方差矩阵，散布矩阵，拉格朗日乘数法，特征值与特征向量 流形学习 流形，最优化，测地线，测地距离，图，特征值与特征向量 线性判别分析 散度矩阵，逆矩阵，拉格朗日乘数法，特征值与特征向量 支持向量机 点到平面的距离，Slater条件，强对偶，拉格朗日对偶，KKT条件，凸优化，核函数，Mercer条件 ]logistics回归 概率、随机变量，最大似然估计，梯度下降法，凸优化，牛顿法 随机森林 抽样，方差 AdaBoost算法 概率，随机变量，最大似然估计，梯度下降法，凸优化，牛顿法 隐马尔科夫链 概率，离散型随机变量，条件概率，随机变量独立性，拉格朗日乘数法，最大似然估计 条件随机场 条件概率，数学期望，最大似然估计 高斯混合模型 正态分布，最大似然估计，Jensen不等式 人工神经网络 梯度下降法，链式法则 卷积神经网络 梯度下降法，链式法则 循环神经网络 梯度下降法，链式法则 生成对抗网络 梯度下降法，链式法则，极值定理，Kullback-Leibler散度，Jensen-Shannon散度，测地距离，条件分布，互信息 K-means算法 距离函数 强化学习 数学期望，贝尔曼方程 贝叶斯网络 条件概率，贝叶斯公式，图 VC维 Hoeffding不等式 微积分 导数与求导公式 一阶导数与函数的单调性 一元函数极值判定法则 高阶导数 二阶导数与函数的凹凸性 一元函数泰勒展开 偏导数 梯度$\nabla f(\mathrm{x})=\left(\frac{\partial f}{\partial x_{1}}, \ldots, \frac{\partial f}{\partial x_{n}}\right)^{\mathrm{T}}$ 雅可比矩阵多元函数的一阶偏导数组成$\left[ \begin{array}{cccc}{\frac{\partial y_{1}}{\partial x_{1}}} &amp; {\frac{\partial y_{1}}{\partial x_{2}}} &amp; {\dots} &amp; {\frac{\partial y_{1}}{\partial x_{n}}} \\ {\frac{\partial y_{2}}{\partial x_{1}}} &amp; {\frac{\partial y_{2}}{\partial x_{2}}} &amp; {\dots} &amp; {\frac{\partial y_{2}}{\partial x_{n}}} \\ {\cdots} &amp; {\cdots} &amp; {\cdots} &amp; {\cdots} \\ {\frac{\partial y_{m}}{\partial x_{1}}} &amp; {\frac{\partial y_{m}}{\partial x_{2}}} &amp; {\ldots} &amp; {\frac{\partial y_{m}}{\partial x_{n}}}\end{array}\right]$ Hessian矩阵多元函数的二阶导数组成$\left[ \begin{array}{cccc}{\frac{\partial^{2} f}{\partial x_{1}^{2}}} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} &amp; {\dots} &amp; {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{2}^{2}}} &amp; {\dots} &amp; {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}}\\ {\cdots} &amp; {\cdots} &amp; {\cdots} &amp; {\cdots}\\ {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} &amp; {\frac{\partial^{2} f}{\partial x_{n} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f}{\partial x_{n}^{2}}}\end{array}\right]$ 多元函数泰勒展开$f(\mathrm{x})=f\left(\mathrm{x}_{0}\right)+\left(\nabla f\left(\mathrm{x}_{0}\right)\right)^{\mathrm{T}}\left(\mathrm{x}-\mathrm{x}_{0}\right)+\frac{1}{2}\left(\mathrm{x}-\mathrm{x}_{0}\right)^{\mathrm{T}} \mathrm{H}\left(\mathrm{x}-\mathrm{x}_{0}\right)+o\left(\left\Vert \mathrm{x}-\mathrm{x}_{0}\right\Vert^{2}\right)$ 多元函数极值判断法则 如果Hessian矩阵正定，函数在该点有极小值 如果Hessian矩阵负定，函数在该点有极大值 如果Hessian矩阵不定，还需要看更高阶的导数 线性代数 向量及其运算 向量的范数$\Vert\mathrm{x}\Vert_{p}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}$$\Vert\mathrm{x}\Vert_{1}=\sum_{i=1}^{n}\left|x_{i}\right|$$\Vert\mathrm{x}\Vert_{2}=\sqrt{\sum_{i=1}^{n}\left(x_{i}\right)^{2}}$ 矩阵及其运算 张量 行列式 $|\mathrm{A}|=\sum_{\sigma \in S_{n}} \operatorname{sgn}(\sigma) \prod_{i=1}^{n} a_{i, \sigma(i)}$ 逆序数 二次型 特征值与特征向量 奇异值分解（SVD）$\mathrm{A}=\mathrm{U} \Sigma \mathrm{V}^{\mathrm{T}}$U：$AA^{T}$ 正交矩阵 mxmV：$A^{T}A$ 正交矩阵 nxn$\Sigma$ 对角阵 mxn 常用的矩阵与向量求导公式$\nabla \mathrm{w}^{\mathrm{T}} \mathrm{x}=\mathrm{w}$$\nabla \mathrm{x}^{\mathrm{T}} \mathrm{Ax}=\left(\mathrm{A}+\mathrm{A}^{\mathrm{T}}\right) \mathrm{x}$$\nabla^{2} \mathrm{x}^{\mathrm{T}} \mathrm{Ax}=\mathrm{A}+\mathrm{A}^{\mathrm{T}}$ （$\nabla^{2}$ 相当于H ） 概率论 随机事件与概率 条件概率与贝叶斯公式 随机变量 数学期望与方差 常用概率分布 随机向量 协方差与协方差矩阵协方差反应随机变量线性相关的程度 $\operatorname{cov}\left(x_{1}, x_{2}\right)=\mathrm{E}\left(\left(x_{1}-\mathrm{E}\left(x_{1}\right)\right)\left(x_{2}-\mathrm{E}\left(x_{2}\right)\right)\right)$ $\operatorname{cov}\left(x_{1}, x_{2}\right)=\mathrm{E}\left(x_{1} x_{2}\right)-\mathrm{E}\left(x_{1}\right) \mathrm{E}\left(x_{2}\right)$ 最大似然估计（MLE）用来估计概率密度的参数 最优化方法 最优化的基本概念 最优化问题 目标函数 优化变量 可行域 等式约束 不等式约束 局部最小值 全局最小值 迭代法 $\lim _{k \rightarrow+\infty} \nabla f\left(\mathrm{x}_{k}\right)=0$ $\mathrm{x}_{k+1}=h\left(\mathrm{x}_{k}\right)$ 梯度下降法$\mathrm{x}_{k+1}=\mathrm{x}_{k}-\gamma \nabla f\left(\mathrm{x}_{k}\right)$ 牛顿法$\mathrm{x}_{k+1}=\mathrm{x}_{k}-\mathrm{H}_{k}^{-1} \mathrm{g}_{k}$ 坐标下降法$\min f(x), x=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$$\min _{x_{i}} f(x)$ 优化算法面临的问题 局部极值点问题 鞍点问题（H不定） 拉格朗日乘数法 凸优化简介 凸集$\theta \mathrm{x}+(1-\theta) \mathrm{y} \in C$ 凸函数 凸函数定义$f(\theta \mathrm{x}+(1-\theta) \mathrm{y})&lt;\theta f(\mathrm{x})+(1-\theta) f(\mathrm{y})$ 一阶判别法 二阶判别法（H半正定，正定严格凸函数） 凸优化的性质 局部最优解一定是全局最优解$\mathrm{z}=\theta \mathrm{y}+(1-\theta) \mathrm{x} \quad \theta=\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}}$$\begin{aligned}\Vert\mathrm{x}-\mathrm{z}\Vert_{2} &amp;=\Vert \mathrm{x}-\left(\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}} \mathrm{y}+\left(1-\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}}\right) \mathrm{x}\left\Vert_{2}\right.\right.\\ &amp;=\left\Vert\frac{\delta}{2\Vert\mathrm{x}-\mathrm{y}\Vert_{2}}(\mathrm{x}-\mathrm{y})\right\Vert_{2} \\ &amp;=\frac{\delta}{2} \leq \delta \end{aligned}$$f(z)=f(\theta y+(1-\theta) x) \leq \theta f(y)+(1-\theta) f(x)&lt;f(x)$ 拉格朗日对偶 $\min f(\mathrm{x})$$\mathrm{g}_{i}(\mathrm{x}) \leq 0 \quad \mathrm{i}=1, \ldots, m$$h_{i}(\mathrm{x})=0 \quad \mathrm{i}=1, \ldots, p$ $L(\mathrm{x}, \lambda, v)=f(\mathrm{x})+\sum_{i=1}^{m} \lambda_{1} g_{i}(\mathrm{x})+\sum_{i=1}^{p} v_{i} h_{i}(\mathrm{x})$ 原问题$\begin{aligned} p^{ \ast } &amp;=\min _{x} \max _{\lambda, v, \lambda_{i} \geq 0} L(\mathrm{x}, \lambda, v) &amp; \min _{\mathrm{x}} \theta_{P}(\mathrm{x})=\min _{\mathrm{x}} \max _{\lambda, v, \lambda_{i} \geq 0} L(\mathrm{x}, \lambda, v) \\ &amp;=\min _{\mathrm{x}} \theta_{P}(x) \end{aligned}$ 对偶问题$d^{ \ast }=\max _{\lambda, v, \lambda_{i} \geq 0} \min _{x} L(\mathrm{x}, \lambda, v)=\max _{\lambda, v, \lambda_{i} \geq 0} \theta_{D}(\lambda, v)$ 弱对偶问题$d^{ \ast }=\max _{\lambda, v, \lambda_{i} \geq 0} \min _{x} L(\mathrm{x}, \lambda, v) \leq \min _{x} \max _{\lambda, v, \lambda_{i} \geq 0} L(\mathrm{x}, \lambda, v)=p^{ \ast }$ 强对偶Slater条件 凸函数 不等式严格成立（不取等号） KKT条件$\min f(\mathrm{x})$$g_{i}(\mathrm{x}) \leq 0 \quad \mathrm{i}=1, \ldots, q$$h_{i}(\mathrm{x})=0 \quad \mathrm{i}=1, \ldots, p$ $L(\mathrm{x}, \lambda, \mu)=f(\mathrm{x})+\sum_{j=1}^{p} \lambda_{i} h_{j}(\mathrm{x})+\sum_{k=1}^{q} \mu_{i} g_{k}(\mathrm{x})$ $\nabla_{\mathrm{x}} L\left(\mathrm{x}^{\ast}\right)=0$$\mu_{k} \geq 0$$\mu_{k} g_{k}\left(\mathrm{x}^{\ast}\right)=0$$h_{j}\left(\mathrm{x}^{\ast}\right)=0$$g_{k}\left(\mathrm{x}^{\ast}\right) \leq 0$]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>基础</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周 Logistic回归、SVM]]></title>
    <url>%2F2019%2F04%2F06%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%20Logistic%E5%9B%9E%E5%BD%92%E3%80%81SVM%2F</url>
    <content type="text"><![CDATA[1、Logistic回归基本原理 分类 给定训练数据$D =\{\mathbf x_i, y_i\}^N_{i=1}$，分类任务学习一个从输入x到输出y的映射f ：$\hat y = f(\mathbf x) = \underset{c}{arg\ max}\ p(y = c \mid \mathbf x, D)$ 其中y为离散值，其取值范围称为标签空间:$Y =\{1,2,…,C\}$ 当C=2时，为两类分类问题，计算出$p(y = 1 \mid \mathbf x)$即可。此时分布为Bernoulli分布: p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))其中$\mu (\mathbf x) = \mathbb{E}(y \mid \mathbf x) = p(y = 1 \mid \mathbf x)$ Recall:Bernouili分布 Bernoulli分布又名两点分布或者0-1分布。若Bernoulli试验成功，则Bernoulli随机变量X取值为1，否则X为0。记试验成功概率为θ， 我们称X服从参数为θ的Bernoulli分布，记为: 𝑋~𝐵𝑒𝑟(θ), 概率函数（pmf）为：p(x) = \theta ^x(1- \theta)^{1-x} = \begin{cases} \theta & if\ x = 1\\ 1 - \theta & if\ x = 0 \end{cases} Bernoulli分布的均值：$\mu = \theta $ 方差：$\sigma^2 = \theta \times (1-\theta)$ Logistic回归模型 Logistic回归模型同线性回归模型类似，也是一个线性模型，只是条件概率𝑝(𝑦|𝐱)的形式不同：p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))\mu (\mathbf x) = \sigma(\mathbf w^T\mathbf x) 其中sigmoid函数（S形函数）定义为\sigma(a) = \frac{1}{1+exp(-a)} = \frac{exp(-a)}{exp(-a)+1} 亦被称为logistic函数或logit函数，将实数a变换到[0,1]区间。 因为概率取值在[0,1]区间 Logistic回归亦被称为logit回归 为什么用logistic函数？ 在神经科学中 神经元对其输入进行加权和：$f(x) = w^Tx$ 如果该和大于某阈值 $f(x) &gt; \tau $，神经元发放脉冲 在Logistic回归，定义Log Odds Ratio:\begin{eqnarray} LOR(x) &=& log \frac {p(y=1 \mid \mathbf x, \mathbf f w)}{p(y = 0 \mid \mathbf x, \mathbf w)} &=& log [\frac{1}{1+exp(-\mathbf w^T\mathbf x)} \frac {1+exp(-\mathbf w^T\mathbf x)}{exp(-\mathbf w^T\mathbf x)}] \\ &=& log [exp(\mathbf w^T \mathbf x)]\\ &=& \mathbf w^T\mathbf x \end{eqnarray} 因此，$iff LOR(\mathbf x) = \mathbf w^T \mathbf x &gt; 0$神经元发放脉冲，即$p(y=1 \mid \mathbf x, \mathbf w) &gt; p(y=0 \mid \mathbf x, \mathbf w)$ 线性决策函数 在Logistic回归中 $LOR(\bf x) = w^Tx &gt; 0, \hat y = 1$ $LOR(\bf x) = w^Tx &lt;0, \hat y = 0$ $\bf w^T \bf x = 0$:决策面 因此$a(\bf x) = w^Tx$分类决策面 因此Logistic回归是一个线性分类器 极大似然估计 Logistic回归：$p(y \mid \mathbf {x,w}) = Ber(y \mid \mu (x)), \mu (\mathbf x) = \sigma (\mathbf w^T\mathbf x)$ 令$\mu_i = \mu(\mathbf x_i)$，则负log似然为$\begin{eqnarray} J(w) = NLL(\mathbf w) &amp;=&amp; - \sum_{i=1}^N log \left[(\mu_i)^{y_i} \times (1-\mu_i)^{1-y_i}\right]\\ &amp;=&amp; \sum_{i=1}^N- \left[y_i log(\mu_i)+(1-y_i)log(1-u_i) \right] \end{eqnarray}$ 极大似然估计等价于最小Logistic损失 优化求解：梯度下降／牛顿法 梯度 目标函数为$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$ 梯度为$\begin{eqnarray} g(\bf w) &amp;=&amp; \frac{\partial J(\bf w)}{\partial \bf w} = \frac{\partial}{\partial \bf w}[\sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]]\\&amp;=&amp; \sum_{i=1}^N[\mu(\mathbf x_i) - y_i] \mathbf x_i \\&amp;=&amp; \bf X^T(\mu - y)\end{eqnarray}$ 二阶Hessian矩阵为$\begin{eqnarray} H(w) &amp;=&amp; \frac{\partial}{\partial \mathbf w}[\mathbf g( \mathbf w)^T] = \sum_{i=1}^N(\frac {\partial}{\partial \mathbf w}\mu_i) \mathbf x_i^T\\&amp;=&amp; \sum_{i=1}{N}\mu_i(1-\mu_i)\bf x_ix_i^T = X^T\underset{S}{ \underbrace{diag(\mu_i(1-\mu_i)}}X = X^TSX\end{eqnarray}$ 牛顿法 亦称牛顿-拉夫逊（ Newton-Raphson ）方法 牛顿在17世纪提出的一种近似求解方程的方法 使用函数f(x)的泰勒级数的前面几项来寻找方程f(x) = 0的根 在求极值问题中，求$g(\mathbf w) = \frac {\partial J(\mathbf w)}{\partial w} = 0$的根 对应$J(\mathbf w)$处取极值 将导数$g(\mathbf w)$在 $w^t$处进行Taylor展开：$0 = \bf g(\hat w) = g(w^t)+(\hat w - w^t)H(w^t) + Op(\hat w - w^t)$ 去掉高阶无穷小$Op(\bf \hat w - w^t)$，从而得到$g(\bf w^t)+(\hat w - w^t)H(w^t) = 0 \Rightarrow \hat w = w^t - H^{-1}(w^t)g(w^t)$ 因此迭代机制为：$\bf w^{t+1} = w^t - H^{-1}(w^t)g(w^t)$ 也被称为二阶梯度下降法，移动方向:$\bf d = -(H(w^t))^{-1}g(w^t)$ Vs. 一阶梯度法，移动方向:$\bf d = -g(w^t)$移动 Iteratively Reweighted Least Squares（IRLS） 引入记号：$\bf g^t(w) = X^T(\mu^t - y), \mu_i^t = \sigma((w^t)^Tx_i)$$\bf H^t(w) = X^TS^tX$,$ S^t:diag(\mu_i^t(1-\mu_1^t),…,\mu_N^t(1-\mu_N^t))$ 根据牛顿法的结果：$w^{t+1} = w^t - (H^t)^{-1}g^t = (X^TS^tX)^{-1}X^TS^tz$ 回忆最小二乘问题： 目标函数：$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T(y - Xw)$ 解：$\hat w = (X^TX)^{-1}X^Ty$ 回忆加权最小二乘问题： 目标函数:$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T\Sigma^{-1}(y - Xw)$ 解：$\hat w = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}y$ IRLS中，$\bf w^{t+1} = (X^TS^tX)^{-1}X^TS^t[Xw^t + (S^t)^{-1}(y - \mu ^t)]$ 相当于权重矩阵为 $\Sigma^{-1} = \bf S^t$ 由于$S^t$是对角阵，$S^t$相当于给每个样本的权重$S_{ii}^t = \mu_i^t(1-\mu_i^t), \mathbf z_i^t = (\mathbf w^t)^T\mathbf x_i + \frac {y_i - \mu_i^t}{S_{ii}^t}$ 拟牛顿法 牛顿法比一般的梯度下降法收敛速度快，但是在高维情况下，计算目标函数的二阶偏导数的复杂度很大，而且有时候目标函数的海森矩阵无法保持正定，不存在逆矩阵，此时牛顿法将不再能使用。 因此，人们提出了拟牛顿法。其基本思想是：不用二阶偏导数而构造出可以近似Hessian矩阵(或Hessian矩阵的逆矩阵)的正定对称矩阵，进而再逐步优化目标函数。不同的构造方法就产生了不同的拟牛顿法（Quasi-Newton Methods） BFGS／LBFGS／Newton-CG 正则化的Logistic回归 若损失函数取logistic损失，则Logistic回归的目标函数为$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$ 同线性回归类似，Logistic回归亦可加上L2正则$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \Vert \mathbf w\Vert ^2_2$ 或L1正则$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \vert \mathbf w\vert $ L2正则的Logistic回归求解 梯度为:$g_{I 2}(\mathbf{w})=g(\mathbf{w})+\lambda \mathbf{w}=\sum_{i=1}^{N}\left(\mu\left(\mathbf{x}_{i}\right)-y_{i}\right) \mathbf{x}_{i}+\lambda \mathbf{w}=\mathbf{X}^{T}(\mathbf{\mu}-\mathbf{y})+\lambda \mathbf{w}$ Hessian矩阵为：$\mathbf{H}_{L 2}(\mathbf{w})=\mathbf{H}(\mathbf{w})+\lambda \mathbf{I}=\mathbf{X}^{T} \mathbf{S} \mathbf{X}+\lambda \mathbf{I}$ 类似不带正则的Logistic回归，可采用（随机）梯度下降、牛顿法或拟牛顿法求解。 L1正则的Logistic回归求解 L1正则项的在0处不可导 在此我们L1正则的Logistic回归的牛顿法（IRLS）求解 随机梯度下降（在线学习）在CTR预估部分讲解 Recall：IRLS\mathbf{w}^{t+1}=\left(\mathbf{X}^{T} \mathbf{S}^{t} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{S}^{t} \mathbf{z}=\underset{\mathbf{w}}{\arg \min }\left\|\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{X} \mathbf{w}-\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{z}\right\|_{2}^{2} L1正则的Logistic回归在每次迭代中可视为一个再加权的Lasso问题：\mathbf{w}^{t+1}=\underset{\mathbf{w}}{\arg \min }\left\|\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{X} \mathbf{w}-\left(\mathbf{S}^{t}\right)^{1 / 2} \mathbf{z}\right\|_{2}^{2}, s . t .\|\mathbf{w}\|_{1}]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>人工智能</tag>
        <tag>Logistic回归</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + GitHub Pages + Next在windows下搭建个人博客]]></title>
    <url>%2F2019%2F04%2F01%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[才搭好博客，发现在博客发布文章确实比微信公众号方便很多，这里简略说下用 Hexo + GitHub Pages + Next搭建个人博客的课程，大部分经验都是来自于网络，我会在整个过程后面附上参考的文章，一来总结搭建博客的过程，二来减少后来人踩坑。 整个过程： 1、注册Github账号及创建仓库 2、安装Git for Windows 3、配置Git 4、安装node.js 5、安装Hexo 6、使用next设计个性化博客 7、连接Hexo和Github Pages及部署博客 8、购买域名并解析以上就是全部的过程，当然具体还有很多细节，比如更换配置、设置文章字数的单位，阅读时常的单位，设置评论区，具体的东西还是要依据个人的喜好调整，但是next主题里面基本都集成了这些功能，只要稍微调整下就行。 参考的文章： 参考的整个过程 各种个性化小功能 给统计量添加单位 各种个性化设置 文章发布 GitHub/Coding双线部署 小书匠Markdown使用手册 在Markdown中输入数学公式(MathJax) Hexo 的 Next 主题中渲染 MathJax 数学公式 报错：hexo fs.SyncWriteStream is deprecated Hexo Next主题博客功能完善 MathJax语法 MathJax与LaTex介绍 MathJax(Markdown中的公式)的基本使用语法]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周 机器学习简介与线性回归]]></title>
    <url>%2F2019%2F03%2F31%2F%E7%AC%AC%E4%B8%80%E5%91%A8%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.1 一个Kaggle竞赛优胜解决方案 一个Kaggle竞赛优胜解决方案 任务：Avazu点击率预估竞赛 Rank 2nd Owen Zhang的解法 优胜算法的特点 特征工程 融合大法 多层 多种不同模型的组合 所以： 基础模型很重要（线性模型） 集成学习模型单模型性能好（GBDT） 特定问题的模型贡献大（FM） 模型融合很重要 课程内容安排 基本模型 线性模型： 线性回归， logistic回归， SVM 非线性模型： （线性模型核化）、分类回归树 集成学习模型（随机森林、GBDT） 数据预处理：数据清洗，特征工程，降维，聚类 模型融合 推荐系统/点击率预估问题特定解决方案 1.2 机器学习任务类型 定义 数据 数据通常以二维数据表形式给出 每一行： 一个样本 每一列：一个属性/特征 例：Boston房价预测数据，根据某地区房屋属性，预测该地区预测房价 506行， 506个样本 14列 机器学习任务类型 监督学习（Supervised Learning） 分类（classfication） 回归（regression） 排序（ranking） 非监督学习（unsupervised learning） 聚类（clustering） 降维（dimensionality reduction） 概率密度估计（density estimation） 增强学习（reinforcement learning） 半监督学习（semi-supervised learning） 迁移学习（transfer learning） …… 监督学习 学习一个x-&gt;y 的映射f, 从而对新输入的x进行预测f（x）D = \{X_i,y_i\}^N_{i=1} D：训练数据集 N：训练样本数目 $X_i$: 第i个样本的输入，亦被称为特征、属性或协变量 $y_i$: 第i个训练样本的输出，亦被称为响应，如类别标签、序号或数值 例：波士顿房价预测 回归 若输出y∈R为连续值，则我们称之为一个回归（regression）任务例： 房价预测，预测二手车的价格 假设回归模型：$y = f(\mathbf x|\theta)$ 如在线性回归中，$f(\mathbf x|w) = \mathbf w^T \mathbf x$ 训练：根据训练数据 $D = \{\mathbf X_i,y_i\}^N_{i=1}$ 学习映射 预测：对新的测试数据x进行预测：$\hat f = f(x)$ y带帽表示预测 学习目标：训练集上预测值与真值之间的差异最小 损失函数：度量模型预测值与真值之间的差异，如L(f(\mathbf x),y) = \frac 12(f(x) - y)^2 目标函数为：$J(\mathbf \theta) = \frac1N \sum_{i = 1}^N L(f(\mathbf x_i|\mathbf \theta), y_i)$ 分类 若输出y为离散值，则我们称之为一个分类，标签空间y = {1,2, … C} 例：信用评分 分类： 学习从输入x到输出y的映射f:概率问题$\hat y = f(\mathbf x) = \underset{c} {arg\ max} \ p(y = c\mid \mathbf x, D)$ 学习目标： 损失函数：01损失 l_{0/1}(y, \hat y) = \begin {cases} 0 & y = \hat y \\ 1 & otherwise \end{cases} 需要预测的概率： 预测：最大后验估计（Maximum a Posteriori, MAP）$\hat y = f(\mathbf x) = \underset{c} {arg\ max}\ p(y = c\mid \mathbf x, D)$ 排序（Rank） 排序学习是推荐、搜素、广告的核心方法 排序学习中需要首先根据查询q及其文档集合进行标注（data labeling） 和提取特征（feature extraction） 才能得到D = {….} 非监督学习 发现数据中的“有意义的模式”， 亦被称为知识发现 训练数据不包含标签 标签在训练数据中为隐含变量 $ D = \{ \bf X_i\}_{ i= 1}^ N $ 聚类例：人的“类型”分多少类？ 模型选择$ K^* = arg\ max _K\ p(K \mid D)$某个样本属于哪个类？ 降维多维特征，有些特征之间会相关而存在冗余很多算法中，降维算法成为了数据预处理的一部分， 如主成分分析（Principal Components Analysis, PCA） 半监督学习 当标注数据“昂贵”时有用 例：标注3D姿态、 蛋白质功能等等 多标签学习 有歧义标签学习 多实例学习 增强学习从行为的反馈(奖励或惩罚)中学习 设计一个回报函数（reward function）， 如果learning agent(如机器人、围棋ai程序)，在决定一步之后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负 增强学习的任务：找到一条回报值最大的路径 1.3 一个典型的机器学习案例-对鱼进行分类 根据一些光学传感器对传送带上的鱼进行分类 形式化为机器学习问题 训练数据 每条鱼的测量向量 每条鱼的标签 测试 给定一个新的特征向量x 预测对应的标签y 将长度作为特征进行分类（直方图） 需要先做一个决策边界 最小化平均损失 将亮度作为特征进行分类 （直方图） 将长度和亮度一起作为特征（二维散点图） 线性决策函数 二次决策函数 更复杂的决策边界训练集上的误差 ≠ 测试集上的误差数据过拟合（overfitting）推广性（generalization）差 小结：设计一个鱼的分类器 选择特征 可能是最重要的步骤！（收集训练数据） 选择模型（如决策边界的形状） 根据训练数据估计模型 利用模型对新样本进行分类 1.4 机器学习算法的组成部分 机器学习任务的一般步骤 确定特征 可能是最重要的步骤！（收集训练数据） 确定模型 目标函数/决策边界形状 模型训练：根据训练数据估计模型参数 优化计算 模型评估：在校验集上评估模型预测性能 模型应用/预测 模型 监督学习任务：$D = \{X_i, y_i\} _{i = 1} ^ N $ 模型：对给定的输入x, 如何预测其标签$ \hat y$ 不同模型对数据的假设不同 最简单的模型：线性模型$ f(x) = \sum_j w_j x_j = \bf w^T \bf x$ 确定模型类别后，模型训练转化为求解模型参数 如对线性模型参数为$\theta = \{w_j \mid j = 1,…, D\}$,其中D为特征维数 求解模型参数：目标函数最小化 非线性模型 基函数： $x^2$, log, exp, 样条函数，决策树…. 核化：将原问题转化为对偶问题，将对偶问题中的向量积$\langle x_i, x_j\rangle$ 换成核函数$k(x_i,x_j)$ 目标函数：通常包含两项：损失函数和正则项J(\theta) = \frac 1N \sum_{i=1}^N\ L(f(x_i; \theta), y_i) + R(\theta) 损失函数 损失函数 - 回归 损失函数：度量模型预测值与真值之间的差异 对回归问题：令残差 $r = f(\bf x) - y$ L2损失：连续，但对噪声敏感L_2 (r) = \frac 12 r ^2 L1损失：不连续，对噪声不敏感L_1(r) = |r| Huber 损失： 连续，对噪声不敏感L_\delta (r) = \begin{cases} \frac 12 r^2 & if|r| \le \delta\\ \delta |r| - \frac 12 \delta^2 & if|r| \ge \delta\end{cases} 损失函数 - 分类 损失函数：度量模型预测值与真值之间的差异 对分类问题 0-1损失：$l_{0/1}(y,f(x)) = \begin{cases} 1 &amp; yf(x) \lt 0 \\ 0 &amp; othereise\end{cases}$ logistic损失：亦称负log似然损失 $l_{log}(y,f(x)) = log(1 + exp(-yf(x)))$ 指数损失：$l_{exp}(y,f(x)) = exp(-yf(x))$ 合页损失：$l_{hinge}(y,f(x)) = max(0, 1 - yf(x))$ 正则项 复杂模型（预测）不稳定：方差大 正则项对复杂模型施加惩罚 正则项的必要性例：sin曲线拟合 增加L2正则岭回归：最小化RSS 欠拟合：模型太简单/对复杂性惩罚太多 样本数目增多时，可以考虑更复杂的模型 常见正则项 L2正则: $R(\theta) = \lambda ||\theta||^2_2 = \lambda \sum^D_{j=1} \theta_j^2$ L1正则: $R(\theta) = \lambda |\theta| = \lambda \sum ^D_{j=1}|\theta_j|$ L0正则: $R(\theta) = \lambda||\theta||_ 0$ 非0参数的数目 不好优化，通常用L1正则近似 常见线性模型的损失和正则项组合 L2损失 L1损失 Huber损失 Logistic损失 合页损失 e-insensitive损失 L2正则 岭回归 L2正则 Logistic回归 SVM SVR L1正则 LASSO L1正则 Logistic回归 L2+L1正则 Elastic 模型训练 在训练数据上求目标函数极小值：优化 简单目标函数直接求解 如小数据集上的线性回归 更复杂问题：凸优化 （随机）梯度下降 牛顿法/拟牛顿法 … 梯度下降（Gradient Descent）算法 梯度下降/最速下降算法：快速寻找函数局部极小值 梯度下降算法：求函数J（θ）的最小值 给定初始值$θ^0$ 更新θ，使得J（θ）越来越小 $θ^t = θ^{t-1} - \eta\nabla J(θ)$ ( $\eta$ : 学习率 ) 直到收敛到 / 达到预先设定的最大迭代次数 下降的步伐太小（学习率）非常重要：如果太小，收敛速度慢； 如果太大，可能会出现overshoot the minimum的现象 梯度下降求得的只是局部最小值 二阶导数 &gt; 0, 则目标函数为凸函数，局部极小值即为全局最小值 随机选择多个初始值，得到函数的多个局部极小值点。多个局部极小值点的最小值为函数的全局最小值 梯度下降算法每次学习都使用整个训练集，这样对大的训练数据集合，每次学习时间过长，对大的训练集需要消耗大量的内存。此时可采用随机梯度下降（Stochastic gradient descent, SGD), 每次从训练集中随机选择一部分样本进行学习。 更多（随机）梯度下降算法的改进版 动量（Momentum） Nesterov accelerated gradient (NAG) Adagrad RMSprop Adaptive Moment Estimation (Adam)… 模型选择与模型评估 同一个问题有不同的解决方案 如线性回归 vs. 决策树 哪个更好？ 模型评估与模型选择 在新数据点的预测误差最小 模型评估：已经选定最终的模型，估计它在新数据上的预测误差 模型选择：估计不同模型的性能，选出最好的模型 样本足够多：训练集和校验集 样本不够多：重采样技术来模拟校验集：交叉验证和bootstrap K-折交叉验证 交叉验证（Cross Validation, CV）： 将训练数据分成容量大致相等的K份（通常K = 5/10） 交叉验证估计的误差为：CV(M)= \frac1K \sum ^K_{k = 1} E_k(M) 模型选择 对多个不同模型，计算其对应的误差CV（M）， 最佳模型为CV（M）最小的模型 模型复杂度和泛化误差的关系通常是U形曲线： 1.5 学习环境简介 编程语言 Python 数据处理工具包 Numpy SciPy pandas 数据可视化工具包 Matplotlib Seaborn 机器学习工具包 scikit learn 示例代码：INotebook NumPy NumPy(Numeric Python)是Python的开源数值计算扩展，可用来存储和处理大型矩阵 Numpy包括： N维数组(ndarray) 实用的线性代数、傅里叶变换和随机数生成函数 Numpy和稀疏矩阵运算包SciPy配合使用更加方便 SciPy SciPy是建立在NumPy的基础上、是科学和工程设计的Python工具包，提供统计、优化和数值微积分计算等功能 NumPy 处理$10^6$级别的数据通常没有大问题，但当数据量达到$10^7$级别时速度开始发慢，内存受到限制（具体情况取决于实际内存的大小） 当处理超大规模数据集，比如$10^{10}$级别，且数据中包含大量的0时，可采用稀疏矩阵可显著的提高速度和效率 Pandas(Pandel data structures) Pandas是Python语言的“关系型数据库”数据结构和数据分析工具，非常高效且易于使用 基于NumPy补充了大量数据操作功能，能实现统计、分组、排序、透视表(SQL语句的大部分功能) Pandas主要有2种重要的数据类型 series：一维序列 DataFrame：二维表(机器学习数据的常用数据结构) Matplotlib Matplotlib是Python语言的2D图形绘制工具 Seaborn Seaborn是一个基于Matplotlib的Python可视化工具包，提供更高层次的用户接口，可以给出漂亮的数据统计 Scikit - Learn Machine Learning in Python Scikit-Learn是基于Python的开源机器学习模块，最早于2007年由David Cournapeau发起 基本功能有六部分：分类（Classification），回归（Regression），聚类（Clustering），数据降维（Dimensionality reduction），模型选择（Model Selection），数据预处理（Preprocessing） 对于具体的机器学习问题，通常可以分为三个步骤 数据准备与预处理（Preprocessing, Dimensionality reduction） 模型选择与训练（Classification, Regression, Clustering） 模型验证与参数调优（Model Selection） 各种机器学习模型有统一的接口 模型既有默认参数，也提供多种参数调优方法 卓越的文档 丰富的随附任务功能集合 活跃的社区提供开发和支持 1.6 线性回归模型 目标函数通常包含两项：损失函数和正则项J(\bf \theta) = \frac1N \sum_{i = 1}^N L(f(\bf x_i|\bf \theta), y_i) + \lambda R(\bf \theta) 对回归问题，损失函数可以采用L2损失，得到\begin{eqnarray}J(\theta) &=&\sum_{i=1}^NL(y_i,\hat y_i) \\ &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\ &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 残差平方和（residual sum of squares, RSS） 由于线性模型比较简单，实际应用中有时正则项为空，得到最小二乘线性回归（Ordinary Least Square, OLS）\begin{eqnarray}J(\theta) &=&\sum_{i=1}^NL(y_i,\hat y_i) &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\ &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 正则项可以为L2正则，得到岭回归（Ridge Regression）J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda ||w||^2_2 正则项也可以选L1正则，得到Lasso模型： J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda |w| 当$\lambda$取合适值时，Lasso（Least absolute shrinkage and selection operator）的结果是稀疏的（w的某些元素系数为0），起到特征选择作用 为什么L1正则的解是稀疏的？ 线性回归模型的概率解释 最小二乘（线性）回归等价于极大似然估计 假设：$ y = f(\bf x) + \epsilon = w^Tx + \epsilon $其中$\epsilon$为线性预测和真值之间的残差我们通常假设残差的分布为$\epsilon \sim N(0,\sigma ^2)$,因此线性回归可写成：$p(y|x,\theta) \sim N(y| \bf w^T \bf x, \sigma^2)$,其中$ \bf \theta = (\bf w, \sigma ^2)$ 正则（线性）回归等价于高斯先验（L2正则）或Laplace先验下（L1正则）的贝叶斯估计 Recall：极大似然估计 极大似然估计（Maximize Likelihood Estimator, MLE）定义为\hat \theta = \underset {\theta} {arg\ max}\ log\ p(D\mid \theta) 其中（log）似然函数为l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) 表示在参数为$\theta$的情况下，数据$D ={\bf x_i,y_i}^N_{i=1}$ 极大似然：选择数据出现概率最大的参数 线性回归的MLEp(y_i|x_i,\bf w,\sigma ^2) \sim N(y_i\mid \bf w^T \bf x_i, \sigma^2) = \frac 1{\sqrt{2\pi}\sigma} exp(-\frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2)) OLS的似然函数为l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) 极大似然可等价地写成极小负log似然损失（negative log likelihood, NLL） \begin{eqnarray}{NLL(\bf \theta)} &=& \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) \\ &=& - \sum_{i=1}^N log ((\frac 1{2 \pi \sigma^2})^ \frac 12 exp(- \frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))) \\ &=& \frac N2 log(2\pi \sigma ^2) + \frac 1{2 \sigma^2} \sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 正则回归等价于贝叶斯估计 假设残差的分布为$\epsilon \sim N(0, \sigma ^2)$,线性回归可写成：$p(y_i \mid \bf x_i, \theta) \sim N(y_i \mid \bf w^T \bf x_i，\sigma ^2)$$p(y\mid \bf X, \bf w, \sigma ^2) = N(\bf y \mid \bf X \bf w, \sigma ^2 \bf I_N) \propto exp(- \frac 1{2\sigma ^2}((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w)))$ 若假设参数为w的先验分布为 $w_j \sim N(0, \tau ^2)$ 偏向较小的系数值，从而得到的曲线也比较平滑$p(\bf w) =\prod_{j=1}^{D} N(w_j \mid 0, \tau ^2) \propto exp(- \frac 1{2\tau^2} \sum_{j=1}^D \bf w_j^2 = exp(- \frac 1{2\tau^2} ( \bf w^T \bf w ) )) $ 其中$1/\tau ^2$控制先验的强度 根据贝叶斯公式，得到参数的后验分布为$p(y\mid \bf X, \bf w, \sigma ^2) = \propto exp(- \frac 1{2\sigma ^2} ((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w) ) - \frac 1{2 \tau^2} ( w^Tw ) )$ 则最大后验估计(MAP)等价于最小目标函数$J(\bf w) = (\bf y - \bf X\bf w)^T(\bf y - \bf X\bf w) + \frac {\sigma ^2}{\tau^2} \bf w^T \bf w $ 对比岭回归的目标函数$J(\bf w) = \sum_{i=1}^N(y_i -\bf w^T\bf x_i)^2 + \lambda \Vert \bf w\Vert ^2_2$ 小结 线性回归模型可以放到机器学习一般框架 损失函数：L2损失，… 正则：无正则， L2正则，L1正则… 正则回归模型可视为先验为正则、似然为高斯分布的贝叶斯估计 L2正则：先验分布为高斯分布 L1正则：先验分布为Laplace分布 1.7 线性回归模型-优化算法 线性回归的目标函数 无正则的最小二乘线性回归（Ordinary Least Square, OLS）：J(w) = \sum_{i=1}^N(y_i - w^Tx_i)^2 L2正则的岭回归（Ridge Regression）模型：J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D w_j^2 L1正则的Lasso模型：J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D |w_j| 模型训练： 根据训练数据求目标函数取极小值的参数： $\hat w = \underset {w} {arg\ min} J(\bf w)$ 目标函数的最小值： 一阶的导数为0：$\frac{\partial J(w)} {\partial w}$ 二阶导数&gt;0：$\frac{\partial J^2(w)} {\partial w^2}$ OLS的优化求解： OLS的优化求解 OLS的目标函数写成矩阵形式：$J(w) = \sum ^N_{i=1}(y_i - w^Tx_i)^2 = (y - Xw)^T(y - Xw)$ 只取与w有关的项，得到$J(w) = w^T(X^TX)w - 2w^T(X^Ty)$ 求导 $\frac{\partial J(w)} {\partial w} = 2X^TXw - 2X^Ty = 0 \Rightarrow X^TXw = X^Ty$`$\hat w_{OLS} = (X^TX)^{-1}X^Ty$ OLS的优化求解 ——SVD OLS目标函数：$J(w) = \Vert y - Xw\Vert_2^2$ 相当于求 $y = Xw$ 如果X为方阵，可求逆：$w = X^{-1}y$ 如果𝐗不是方阵，可求Moore-Penrose广义逆：$𝐰 = 𝐗^{\dagger }𝐲$。 Moore-Penrose广义逆可采用奇异值分解(Singular Value Decomposition)实现：奇异值分解：$X = U \Sigma V^T$$X^{\dagger } = V \Sigma ^{\dagger} U^T$其中 $\Sigma = \begin{pmatrix}{\sigma_1}&amp;{0}&amp;{\cdots}&amp;{0}\\{0}&amp;{\sigma_2}&amp;{\cdots}&amp;{0}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{0}&amp;{0}&amp;{\cdots}&amp;{0}\\\end{pmatrix}$,$\Sigma ^{\dagger} = \begin{pmatrix}{\frac {1}{\sigma_1}}&amp;{0}&amp;{\cdots}&amp;{0}\\{0}&amp;{\frac{1}{\sigma_2}}&amp;{\cdots}&amp;{0}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{0}&amp;{0}&amp;{\cdots}&amp;{0}\\\end{pmatrix}$ OLS的优化求解——梯度下降 OLS目标函数：$J(w) = (y - Xw)^T(y - Xw)$梯度：$\nabla_w = - 2X^T(y - Xw^t)$参数更新：$w^{t+1} = w^t - \eta\nabla_w = w^t + 2\eta X^T(y - Xw^t)$ 岭回归的优化求解 岭回归的目标函数与OLS只相差一个正则项（也是w的二次函数） 岭回归的优化求解——SVD Lasso的优化条件 软&amp; 硬阈值 Lasso的优化求解——坐标轴下降法 为了找到一个函数的局部极小值，在每次迭代中可以在当前点处沿一个坐标方向进行一维搜索。 整个过程中循环使用不同的坐标方向。一个周期的一维搜索迭代过程相当于一个梯度迭代。 注意： 梯度下降方法是利用目标函数的导数（梯度）来确定搜索方向的，而该梯度方向可能不与任何坐标轴平行。 而坐标轴下降法是利用当前坐标系统进行搜索，不需要求目标函数的导数，只按照某一坐标方向进行搜索最小值。（在稀疏矩阵上的计算速度非常快，同时也是Lasso回归最快的解法） 小结 线性回归模型比较简单 当数据规模比较小时，可直接解析求解 scikit learn中的实现采用SVD分解实现 当数据规模较大时，可采用随机梯度下降 scikit learn提供一个SGDRegression类 岭回归求解类似OLS，采用SVD分解实现 Lasso优化求解采用坐标轴下降法 1.8 线性回归模型-模型选择 模型评估与模型选择 模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果 校验集划分（train_test_split、交叉验证） 评价指标（sklearn.metrics） 模型选择： 模型中通常有一些超参数，需要通过模型选择来确定 线性回归模型中的正则参数 OLS中的特征的数目 参数搜索范围：网格搜索（GridSearch） Scikit learn将交叉验证与网格搜索合并为一个函数 评价准则 模型训练好后，可用一些度量准则检查模型拟合的效果 开方均方误差（rooted mean squared error，RMSE）:$RMSE = \sqrt{\frac 1N \sum_{i=1}^N(\hat y_i - y_i)^2}$` 平均绝对误差（mean absolute error，MAE）：$MAE = \frac 1N \sum_{i=1}^N|\hat y_i - y_i|$ R2 score：既考虑了预测值与真值之间的差异，也考虑了问题本身真值之间的差异（ scikit learn 线性回归模型的缺省评价准则）$SS_{res} = \sum_{i=1}^N(\hat y_i - y_i)^2, SStot = \sum_{i=1}^N(y_i - \bar{y})^2, R^2 = 1 - \frac {SS_{res}}{SS_{tot}})$ 也可以检查残差的分布 还可以打印预测值与真值的散点图 线性回归中的模型选择Scikit learn中的model selection模块提供模型选择功能 对于线性模型，留一交叉验证（N折交叉验证，亦称为leave-oneout cross-validation，LOOCV）有更简便的计算方式，因此Scikit learn提供了RidgeCV类和LassoCV类实现了这种方式 后续课程将讲述一般模型的交叉验证和参数调优GridSearchCV RidgeCV RidgeCV中超参数λ用alpha表示 RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False) LassoCV LassoCV的使用与RidgeCV类似 Scikit learn还提供一个与Lasso类似的LARS（least angle regression，最小角回归），二者仅仅是优化方法不同，目标函数相同。 当数据集中特征维数很多且存在共线性时，LassoCV更合适。 小结：线性回归之模型选择 采用交叉验证评估模型预测性能，从而选择最佳模型 回归性能的评价指标 线性模型的交叉验证通常直接采用广义线性模型的留一交叉验证进行快速模型评估 Scikit learn中对RidgeCV和LassoCV实现该功能 1.9 波士顿房价预测案例详解——数据探索 第一步：理解任务，准备数据 数据读取 Pandas支持多种格式的数据 数据探索&amp;特征工程 数据规模 确定数据类型，是否需要进一步编码 特征编码 数据是否有缺失值 数据填补 查看数据分布，是否有异常数据点 离群点处理 查看两两特征之间的关系，看数据是否有冗余/相关 降维 数据概览 pandas:DataFrame Head():数据前5行，可查看每一列的名字及数据类型 Info(): 数据规模：行数&amp;列数 每列的数据类型、是否有空值 占用存储量 shape:行数&amp;列数 各属性的统计特性 直方图 每个取值在数据集中出现的样本数目 离群点 离群点：奇异点（outlier）,指远离大多数样本的样本点。通常认为这些点是噪声，对模型有坏影响 相关性 相关性：相关性可以通过计算相关系数或打印散点图来发现 相关系数： 散点图 可以通过两个变量之间的散点图直观感受二者的相关性 数据预处理 数据标准化（ Standardization ） 某个特征的所有样本取值为0均值、1方差 数据归一化（ Scaling ） 某个特征的所有样本取值在规定范围内 数据正规化（ Normalization ） 每个样本模长为1 数据二值化 根据特征值取值是否大于阈值将特征值变为0或1，可用类Binarizer 实现 数据缺失 数据类型变换 有些模型只能处理数值型数据。如果给定的数据是不同的类型，必须先将数据变成数值型。 第二步：模型确定和模型训练 1、确定模型类型 目标函数（损失函数、正则） 2、模型训练 优化算法（解析法，梯度下降、随机梯度下降…） 第三步：模型评估与模型选择 模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果 校验集划分（train_test_split、交叉验证） 评价指标 （sklearn.metics） 也可以检查残差的分布 还可以打印预测值与真值的散点图 模型选择：选择预测性能最好的模型 模型中通常有一些超参数，需要通过模型选择来确定 参数搜索范围：网格搜索（GridSearch） 1.10 波士顿房价预测-数据探索代码python 3.712345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns# 读入数据data = pd.read_csv("boston_housing.csv")# 数据探索print(data.head())data.info()print(data.isnull().sum())print(data.describe())# 目标y(房屋价格)的直方图/分布fig = plt.figure()sns.distplot(data.MEDV.values, bins=30, kde=True)plt.xlabel('Median value of owner_occupied homes', fontsize=12)plt.show()# 单个特征散点图plt.scatter(range(data.shape[0]), data["MEDV"].values, color='purple')plt.title("Distribution of Price")plt.show()# 删除y大于50的样本data = data[data.MEDV &lt; 50]print(data.shape)# 输入属性的直方图／分布# 犯罪率特征fig = plt.figure()sns.distplot(data.CRIM.values, bins=30, kde=False)plt.xlabel('crime rate', fontsize=12)plt.show()# 是否靠近charles riversns.countplot(data.CHAS, order=[0, 1]);plt.xlabel('Charles River');plt.ylabel('Number of occurrences');plt.show()# 靠近高速sns.countplot(data.RAD)plt.xlabel('index of accessibility to radial highways')plt.show()# 两两特征之间的相关性# 获得所有列的名字cols = data.columns# 计算相关性data_corr = data.corr().abs()# 相关性热图plt.subplots(figsize=(13, 9))sns.heatmap(data_corr, annot=True)sns.heatmap(data_corr, mask=data_corr &lt; 1, cbar=False)plt.savefig('house_coor.png')plt.show()# 输出强相关对threshold = 0.5corr_list = []size = data_corr.shape[0]for i in range(0, size): for j in range(i+1, size): if (data_corr.iloc[i,j] &gt;= threshold and data_corr.iloc[i, j] &lt; 1) or (data_corr.iloc[i, j] &lt; 0 and data_corr.iloc &lt;= -threshold): corr_list.append([data_corr.iloc[i, j], i, j])s_corr_list = sorted(corr_list, key=lambda x: -abs(x[0]))for v, i, j in s_corr_list: print("%s and %s = %.2f" % (cols[i], cols[j], v))for v, i, j in s_corr_list: sns.pairplot(data, height=6, x_vars=cols[i], y_vars=cols[j]) plt.show() 1.11 波士顿房价预测案例详解1.12 波士顿房价预测案例详解-代码讲解python 3.7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174# 波士顿房价预测案例——线性回归分析import numpy as np # 矩阵操作import pandas as pd # SQL数据处理from sklearn.metrics import r2_score # 评价回归预测模型的性能import matplotlib.pyplot as plt # 画图import seaborn as sns# 读入数据data = pd.read_csv("boston_housing.csv")# 1、数据准备# 从原始数据中分离输入特征x和输出yy = data['MEDV'].valuesX = data.drop('MEDV', axis=1)# 用于后续显示权重系数对应的特征columns = X.columns# 数据较少，将数据分割训练数据from sklearn.model_selection import train_test_split# 随机采样20%的数据构建测试样本，其余作为训练样本X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33,test_size=0.2)# print(X_train.shape)# 2、数据预处理/特征工程# 数据标准化from sklearn.preprocessing import StandardScaler# 分别初始化对特征和目标值的标准化器ss_X = StandardScaler()ss_y = StandardScaler()# 分别对训练和测试数据的特征以及目标值进行标准化处理X_train = ss_X.fit_transform(X_train)X_test = ss_X.transform(X_test)# 对y标准化不是必须# 对y标准化的好处是不同的问题的w差异不太大，同时正则参数的范围也有限y_train = ss_y.fit_transform(y_train.reshape(-1, 1))y_test = ss_y.transform(y_test.reshape(-1, 1))# 3、确定模型类型# 3.1 尝试缺省参数的线性回归# 线性回归from sklearn.linear_model import LinearRegression# 使用默认配置初始化lr = LinearRegression()# 训练模型参数lr.fit(X_train, y_train)# 预测y_test_pred_lr = lr.predict(X_test)y_train_pred_lr = lr.predict(X_train)# 看看各特征的权重系数，系数的绝对值大小可视为该特征的重要性fs = pd.DataFrame(&#123;"columns": list(columns), "coef": list((lr.coef_.T))&#125;)fs.sort_values(by=['coef'], ascending=False)print(fs)# 模型评价# 测试集print('The r2 score of LinearRegression on test is', r2_score(y_test, y_test_pred_lr))# 训练集print('The r2 score of LinearRegression on train is', r2_score(y_train, y_train_pred_lr))# 在训练集上观察残差的分布，看是否符合模型假设：噪声为0均值的高斯噪声f, ax = plt.subplots(figsize=(7, 5))f.tight_layout()ax.hist(y_train - y_train_pred_lr, bins=40, label='Residuals Linear', color='b', alpha=.5)ax.set_title("Histogram of Residuals")ax.legend(loc='best')plt.show()# 还可以观察预测值与真值的散点图plt.figure(figsize=(4, 3))plt.scatter(y_train, y_train_pred_lr)plt.plot([-3, 3],[-3, 3], '--k')plt.axis('tight')plt.xlabel('True price')plt.ylabel('Predicted price')plt.tight_layout()plt.show()# 线性模型，随机梯度下降优化模型参数# 随机梯度下降一般在大数据集上应用，其实本项目不适合用from sklearn.linear_model import SGDRegressor# 使用默认配置初始化线sgdr = SGDRegressor(max_iter=1000)# 训练：参数估计sgdr.fit(X_train, y_train)# 预测sgdr.coef_print('The value of default measurement of SGDRegressor on test is', sgdr.score(X_test, y_test))print('The value of default measurement of SGDRegressor on train is', sgdr.score(X_train, y_train))# 3.2 正则化的线性回归（L2正则--&gt;岭回归）from sklearn.linear_model import RidgeCV# 设置超参数（正则参数）范围alphas = [0.01, 0.1, 1, 10, 100]# 生成一个RidgeCVridge = RidgeCV(alphas=alphas, store_cv_values=True)# 模型训练ridge.fit(X_train, y_train)# 预测y_test_pred_ridge = ridge.predict(X_test)y_train_pred_ridge = ridge.predict(X_train)# 评估，使用r2_score评价模型在测试集和训练集上的性能print('The r2 score of RidgeCV on test is', r2_score(y_test, y_test_pred_ridge))print('The r2 score of RidgeCV on test is', r2_score(y_train, y_train_pred_ridge))# 可视化mse_mean = np.mean(ridge.cv_values_, axis=0)plt.plot(np.log10(alphas), mse_mean.reshape(len(alphas), 1))# plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])plt.xlabel('log(alpha)')plt.ylabel('mse')plt.show()print('alpha is:', ridge.alpha_)# 看看各特征的权重系数，系数的绝对值大小可视为该特制的重要性fs = pd.DataFrame(&#123;"columns": list(columns), "coef_lr": list(lr.coef_.T), "coef_ridge": list(ridge.coef_.T)&#125;)fs.sort_values(by=['coef_lr'], ascending=False)print(fs)# 3.3 正则化的线性回归（L1正则--&gt;Lasso）from sklearn.linear_model import LassoCV# 生成一个LassoCV实例lasso = LassoCV()# 训练（内含CV）lasso.fit(X_train, y_train)# 测试y_test_pred_lasso = lasso.predict(X_test)y_train_pred_lasso = lasso.predict(X_train)# 评估， 使用r2_score评价模型在测试集和训练集上的性能print('The r2 score of LassoCV on test is', r2_score(y_test, y_test_pred_lasso))print('The r2 score of LassoCV on train is', r2_score(y_train, y_train_pred_lasso))# 可视化mses = np.mean(lasso.mse_path_, axis=1)plt.plot(np.log10(lasso.alphas_), mses)# plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])plt.xlabel('log(alpha)')plt.ylabel('mse')plt.show()print('alpha is:', lasso.alpha_)# 看看各特征的权重系数，系数的绝对值大小可视为该特制的重要性fs = pd.DataFrame(&#123;"columns": list(columns), "coef_lr": list(lr.coef_.T), "coef_ridge": list(lasso.coef_.T)&#125;)fs.sort_values(by=['coef_lr'], ascending=False)print(fs)]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>学习笔记</tag>
        <tag>人工智能</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉基础入门 学习笔记]]></title>
    <url>%2F2019%2F03%2F30%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[一、 计算机视觉和深度学习概述 计算机视觉回顾 计算机视觉（computer vision）定义 数据（静态图片，视频） 算法（机器学习算法，神经网络）本质上是一个回归+分类 计算机视觉的重要性 三大任务：图像识别（image classification）车牌识别，人脸识别 三大任务：目标检测（object detection = classification + localization）行人检测和车辆检测 三大任务：图像分割图像语义分割个体分割 = 检测 + 分割 视觉目标跟踪（tracking） 视频分割 图像风格迁移 生成对抗网络（GAN） 视频生成 深度学习介绍 2006 Hinton bp(反向传播) 2012 Krizhevsky A 深度学习 深度卷积 RNN LSTM 持续信息 视觉识别，语音识别，DeepMind, AlphaGo 人脸识别：LFW 错误率5% -&gt; 0.5% 图像分割 VGGNet, GoogleNet, ResNet, DenseNet 常见的深度学习开发平台 Torch, TensorFlow, MatConvNetTheano, Caffe 课程介绍 图像识别：Alexnet, VGGnet, GoogleNet, ResNet, DenseNet 目标检测Fast-rcnn, faster-rcnn, Yolo, Retina-Net 图像分割FCN, Mask-Rcnn 目标跟踪GORURN， ECO 图像生成GAN， WGAN 光流FlowNet 视频分割Segnet 二、 图像分类与深度卷积网络的模型 图像分类 图像分类的挑战光照变化形变类内变化 图像分类定义 目标分类框架 泛化能力如何提高泛化能力？ 需要用图像特征来描述图像 训练和测试的流程 图像特征 color: Qutantize RGB values global shape: PCA space local shape: shape context texture: Filter banks SIFT, Hog, LBP, Harr 支持向量机（SVM） 超平面与支持向量 最大化间隔 svm分类（python）以lris兰花分类为例 程序实现 更好的特征 CNN特征 学习出来的 如何学习？ 构造神经网络 神经网络原理 神经网络做图像分类 神经网络搭建 神经网络的基本单元：神经元 激励函数 Sigmoid、tanh、ReLU、Leaky ReLU、Maxout、ELU 卷积层 卷积滤波的计算 卷积层可视化 池化层（pooling layer） 特征表达更加紧凑，同时具有位移不变性 全连接层 损失函数 交叉熵损失函数（SIGMOID_CROSS_ENTROPY_LOSS) 应用于二分类问题 Softmax 损失函数（SOFTMAX_LOSS) 多分类问题 欧式距离损失函数（EUCLIDEAN_LOSS）回归问题 对比损失函数（Contrastive loss）用来计算两个图像之间的相似度 Triplet loss 训练网络 网络训练和测试 卷积神经网络介绍 Alexnet, VGGnet, GoogleNet, ResNet, DenseNet 训练技巧， 防止过拟合（泛化能力不强） 数据增强（Data augmentation） 水平翻转， 随机裁剪和平移变换，颜色、光照变换 Dropout 其他有助于训练的手段 L1， L2正则化 Batch Normalization 利用caffe搭建深度网络做图像分类]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>计算机视觉</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
</search>
