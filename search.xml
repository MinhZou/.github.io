<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[第二周 Logistic回归、SVM]]></title>
    <url>%2F2019%2F04%2F06%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%20Logistic%E5%9B%9E%E5%BD%92%E3%80%81SVM%2F</url>
    <content type="text"><![CDATA[1、Logistic回归基本原理 分类 给定训练数据$D =\{\mathbf x_i, y_i\}^N_{i=1}$，分类任务学习一个从输入x到输出y的映射f ：$\hat y = f(\mathbf x) = \underset{c}{arg\ max}\ p(y = c \mid \mathbf x, D)$ 其中y为离散值，其取值范围称为标签空间:$Y =\{1,2,…,C\}$ 当C=2时，为两类分类问题，计算出$p(y = 1 \mid \mathbf x)$即可。此时分布为Bernoulli分布: p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))其中$\mu (\mathbf x) = \mathbb{E}(y \mid \mathbf x) = p(y = 1 \mid \mathbf x)$ Recall:Bernouili分布 Bernoulli分布又名两点分布或者0-1分布。若Bernoulli试验成功，则Bernoulli随机变量X取值为1，否则X为0。记试验成功概率为θ， 我们称X服从参数为θ的Bernoulli分布，记为: 𝑋~𝐵𝑒𝑟(θ), 概率函数（pmf）为：p(x) = \theta ^x(1- \theta)^{1-x} = \begin{cases} \theta & if\ x = 1\\ 1 - \theta & if\ x = 0 \end{cases} Bernoulli分布的均值：$\mu = \theta $ 方差：$\sigma^2 = \theta \times (1-\theta)$ Logistic回归模型 Logistic回归模型同线性回归模型类似，也是一个线性模型，只是条件概率𝑝(𝑦|𝐱)的形式不同：p(y \mid \mathbf x) = Ber(y \mid \mu (\mathbf x))\mu (\mathbf x) = \sigma(\mathbf w^T\mathbf x) 其中sigmoid函数（S形函数）定义为\sigma(a) = \frac{1}{1+exp(-a)} = \frac{exp(-a)}{exp(-a)+1} 亦被称为logistic函数或logit函数，将实数a变换到[0,1]区间。 因为概率取值在[0,1]区间 Logistic回归亦被称为logit回归 为什么用logistic函数？ 在神经科学中 神经元对其输入进行加权和：$f(x) = w^Tx$ 如果该和大于某阈值 $f(x) &gt; \tau $，神经元发放脉冲 在Logistic回归，定义Log Odds Ratio:\begin{eqnarray} LOR(x) &=& log \frac {p(y=1 \mid \mathbf x, \mathbf f w)}{p(y = 0 \mid \mathbf x, \mathbf w)} &=& log [\frac{1}{1+exp(-\mathbf w^T\mathbf x)} \frac {1+exp(-\mathbf w^T\mathbf x)}{exp(-\mathbf w^T\mathbf x)}] \\ &=& log [exp(\mathbf w^T \mathbf x)]\\ &=& \mathbf w^T\mathbf x \end{eqnarray} 因此，$iff LOR(\mathbf x) = \mathbf w^T \mathbf x &gt; 0$神经元发放脉冲，即$p(y=1 \mid \mathbf x, \mathbf w) &gt; p(y=0 \mid \mathbf x, \mathbf w)$ 线性决策函数 在Logistic回归中 $LOR(\bf x) = w^Tx &gt; 0, \hat y = 1$ $LOR(\bf x) = w^Tx &lt;0, \hat y = 0$ $\bf w^T \bf x = 0$:决策面 因此$a(\bf x) = w^Tx分类决策面$ 因此Logistic回归是一个线性分类器 极大似然估计 Logistic回归：$p(y \mid \mathbf {x,w}) = Ber(y \mid \mu (x)), \mu (\mathbf x) = \sigma (\mathbf w^T\mathbf x)$ 令$\mu_i = \mu(\mathbf x_i)$，则负log似然为$\begin{eqnarray} J(w) = NLL(\mathbf w) &amp;=&amp; - \sum_{i=1}^N log \left[(\mu_i)^{y_i} \times (1-\mu_i)^{1-y_i}\right]\\ &amp;=&amp; \sum_{i=1}^N- \left[y_i log(\mu_i)+(1-y_i)log(1-u_i) \right] \end{eqnarray}$ 极大似然估计等价于最小Logistic损失 优化求解：梯度下降／牛顿法 梯度 目标函数为$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$ 梯度为$\begin{eqnarray} g(\bf w) &amp;=&amp; \frac{\partial J(\bf w)}{\partial \bf w} = \frac{\partial}{\partial \bf w}[\sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]]\\&amp;=&amp; \sum_{i=1}^N[\mu(\mathbf x_i) - y_i] \mathbf x_i \\&amp;=&amp; \bf X^T(\mu - y)\end{eqnarray}$ 二阶Hessian矩阵为$\begin{eqnarray} H(w) &amp;=&amp; \frac{\partial}{\partial \mathbf w}[\mathbf g( \mathbf w)^T] = \sum_{i=1}^N(\frac {\partial}{\partial \mathbf w}\mu_i) \mathbf x_i^T\\&amp;=&amp; \sum_{i=1}{N}\mu_i(1-\mu_i)\bf x_ix_i^T = X^T\underset{S}{ \underbrace{diag(\mu_i(1-\mu_i)}}X = X^TSX\end{eqnarray}$ 牛顿法 亦称牛顿-拉夫逊（ Newton-Raphson ）方法 牛顿在17世纪提出的一种近似求解方程的方法 使用函数f(x)的泰勒级数的前面几项来寻找方程f(x) = 0的根 在求极值问题中，求$g(\mathbf w) = \frac {\partial J(\mathbf w)}{\partial w} = 0$的根 对应$J(\mathbf w)$处取极值 将导数$g(\mathbf w)$在 $w^t$处进行Taylor展开：$0 = \bf g(\hat w) = g(w^t)+(\hat w - w^t)H(w^t) + Op(\hat w - w^t)$ 去掉高阶无穷小$Op(\bf \hat w - w^t)$，从而得到$g(\bf w^t)+(\hat w - w^t)H(w^t) = 0 \Rightarrow \hat w = w^t - H^{-1}(w^t)g(w^t)$ 因此迭代机制为：$\bf w^{t+1} = w^t - H^{-1}(w^t)g(w^t)$ 也被称为二阶梯度下降法，移动方向:$\bf d = -(H(w^t))^{-1}g(w^t)$ Vs. 一阶梯度法，移动方向:$\bf d = -g(w^t)$移动 Iteratively Reweighted Least Squares（IRLS） 引入记号：$\bf g^t(w) = X^T(\mu^t - y), \mu_i^t = \sigma((w^t)^Tx_i)$$\bf H^t(w) = X^TS^tX$, S^t:diag(\mu_i^t(1-\mu_1^t),…,\mu_N^t(1-\mu_N^t)) 根据牛顿法的结果：$w^{t+1} = w^t - (H^t)^{-1}g^t = (X^TS^tX)^{-1}X^TS^tz$ 回忆最小二乘问题： 目标函数：$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T(y - Xw)$ 解：$\hat w = (X^TX)^{-1}X^Ty$ 回忆加权最小二乘问题： 目标函数:$J(\bf w) = \sum_{i=1}^N(y_i - w^Tx)^2 = (y - Xw)^T\Sigma^{-1}(y - Xw)$ 解：$\hat w = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}y$ IRLS中，$\bf w^{t+1} = (X^TS^tX)^{-1}X^TS^t[Xw^t + (S^t)^{-1}(y - \mu ^t)]$ 相当于权重矩阵为 \Sigma^{-1} = \bf S^t 由于$S^t$是对角阵，$S^t$相当于给每个样本的权重$S_{ii}^t = \mu_i^t(1-\mu_i^t), \mathbf z_i^t = (\mathbf w^t)^T\mathbf x_i + \frac {y_i - \mu_i^t}{S_{ii}^t}$ 拟牛顿法 牛顿法比一般的梯度下降法收敛速度快，但是在高维情况下，计算目标函数的二阶偏导数的复杂度很大，而且有时候目标函数的海森矩阵无法保持正定，不存在逆矩阵，此时牛顿法将不再能使用。 因此，人们提出了拟牛顿法。其基本思想是：不用二阶偏导数而构造出可以近似Hessian矩阵(或Hessian矩阵的逆矩阵)的正定对称矩阵，进而再逐步优化目标函数。不同的构造方法就产生了不同的拟牛顿法（Quasi-Newton Methods） BFGS／LBFGS／Newton-CG 正则化的Logistic回归 若损失函数取logistic损失，则Logistic回归的目标函数为$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]$ 同线性回归类似，Logistic回归亦可加上L2正则$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \Vert \mathbf w\Vert ^2_2$ 或L1正则$J(\mathbf w) = \sum_{i=1}^N-[y_i log(\mu_i)+(1-y_i)log(1-u_i)]+\lambda \vert \mathbf w\vert $ L2正则的Logistic回归求解 梯度为: Hessian矩阵为： 类似不带正则的Logistic回归，可采用（随机）梯度下降、牛顿法或拟牛顿法求解。 L1正则的Logistic回归求解 L1正则项的在0处不可导 在此我们L1正则的Logistic回归的牛顿法（IRLS）求解 随机梯度下降（在线学习）在CTR预估部分讲解 Recall：IRLS L1正则的Logistic回归在每次迭代中可视为一个再加权的Lasso问题： 小结 Logistic回归： 损失函数：负log似然损失 正则：L2/L1正则 优化：梯度下降／牛顿法／拟牛顿法 2、Softmax分类器3、Scikit learn中的Logistic回归实现4、不平衡数据分类学习5、分类模型的评价6、Logistic回归之模型选择_参数调优7、Logistic回归-Otto商品分类代码8、支持向量机9、带松弛因子的C-SVM10、核方法11、支持向量回归（SVR）12、sklearn中的SVM实现13、SVM-Otto]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>学习笔记</tag>
        <tag>Logistic回归</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + GitHub Pages + Next在windows下搭建个人博客]]></title>
    <url>%2F2019%2F04%2F01%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[才搭好博客，发现在博客发布文章确实比微信公众号方便很多，这里简略说下用 Hexo + GitHub Pages + Next搭建个人博客的课程，大部分经验都是来自于网络，我会在整个过程后面附上参考的文章，一来总结搭建博客的过程，二来减少后来人踩坑。 整个过程： 1、注册Github账号及创建仓库 2、安装Git for Windows 3、配置Git 4、安装node.js 5、安装Hexo 6、使用next设计个性化博客 7、连接Hexo和Github Pages及部署博客 8、购买域名并解析以上就是全部的过程，当然具体还有很多细节，比如更换配置、设置文章字数的单位，阅读时常的单位，设置评论区，具体的东西还是要依据个人的喜好调整，但是next主题里面基本都集成了这些功能，只要稍微调整下就行。 参考的文章： 参考的整个过程 各种个性化小功能 给统计量添加单位 各种个性化设置 文章发布 GitHub/Coding双线部署 小书匠Markdown使用手册 在Markdown中输入数学公式(MathJax) Hexo 的 Next 主题中渲染 MathJax 数学公式 报错：hexo fs.SyncWriteStream is deprecated Hexo Next主题博客功能完善 MathJax语法 MathJax与LaTex介绍 MathJax(Markdown中的公式)的基本使用语法]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周 机器学习简介与线性回归]]></title>
    <url>%2F2019%2F03%2F31%2F%E7%AC%AC%E4%B8%80%E5%91%A8%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.1 一个Kaggle竞赛优胜解决方案 一个Kaggle竞赛优胜解决方案 任务：Avazu点击率预估竞赛 Rank 2nd Owen Zhang的解法 优胜算法的特点 特征工程 融合大法 多层 多种不同模型的组合 所以： 基础模型很重要（线性模型） 集成学习模型单模型性能好（GBDT） 特定问题的模型贡献大（FM） 模型融合很重要 课程内容安排 基本模型 线性模型： 线性回归， logistic回归， SVM 非线性模型： （线性模型核化）、分类回归树 集成学习模型（随机森林、GBDT） 数据预处理：数据清洗，特征工程，降维，聚类 模型融合 推荐系统/点击率预估问题特定解决方案 1.2 机器学习任务类型 定义 数据 数据通常以二维数据表形式给出 每一行： 一个样本 每一列：一个属性/特征 例：Boston房价预测数据，根据某地区房屋属性，预测该地区预测房价 506行， 506个样本 14列 机器学习任务类型 监督学习（Supervised Learning） 分类（classfication） 回归（regression） 排序（ranking） 非监督学习（unsupervised learning） 聚类（clustering） 降维（dimensionality reduction） 概率密度估计（density estimation） 增强学习（reinforcement learning） 半监督学习（semi-supervised learning） 迁移学习（transfer learning） …… 监督学习 学习一个x-&gt;y 的映射f, 从而对新输入的x进行预测f（x）D = \{X_i,y_i\}^N_{i=1} D：训练数据集 N：训练样本数目 $X_i$: 第i个样本的输入，亦被称为特征、属性或协变量 $y_i$: 第i个训练样本的输出，亦被称为响应，如类别标签、序号或数值 例：波士顿房价预测 回归 若输出y∈R为连续值，则我们称之为一个回归（regression）任务例： 房价预测，预测二手车的价格 假设回归模型：$y = f(\mathbf x|\theta)$ 如在线性回归中，$f(\mathbf x|w) = \mathbf w^T \mathbf x$ 训练：根据训练数据 $D = \{\mathbf X_i,y_i\}^N_{i=1}$ 学习映射 预测：对新的测试数据x进行预测：$\hat f = f(x)$ y带帽表示预测 学习目标：训练集上预测值与真值之间的差异最小 损失函数：度量模型预测值与真值之间的差异，如L(f(\mathbf x),y) = \frac 12(f(x) - y)^2 目标函数为：$J(\mathbf \theta) = \frac1N \sum_{i = 1}^N L(f(\mathbf x_i|\mathbf \theta), y_i)$ 分类 若输出y为离散值，则我们称之为一个分类，标签空间y = {1,2, … C} 例：信用评分 分类： 学习从输入x到输出y的映射f:概率问题$\hat y = f(\mathbf x) = \underset{c} {arg\ max} \ p(y = c\mid \mathbf x, D)$ 学习目标： 损失函数：01损失 l_{0/1}(y, \hat y) = \begin {cases} 0 & y = \hat y \\ 1 & otherwise \end{cases} 需要预测的概率： 预测：最大后验估计（Maximum a Posteriori, MAP）$\hat y = f(\mathbf x) = \underset{c} {arg\ max}\ p(y = c\mid \mathbf x, D)$ 排序（Rank） 排序学习是推荐、搜素、广告的核心方法 排序学习中需要首先根据查询q及其文档集合进行标注（data labeling） 和提取特征（feature extraction） 才能得到D = {….} 非监督学习 发现数据中的“有意义的模式”， 亦被称为知识发现 训练数据不包含标签 标签在训练数据中为隐含变量 $ D = \{ \bf X_i\}_{ i= 1}^ N $ 聚类例：人的“类型”分多少类？ 模型选择$ K^* = arg\ max _K\ p(K \mid D)$某个样本属于哪个类？ 降维多维特征，有些特征之间会相关而存在冗余很多算法中，降维算法成为了数据预处理的一部分， 如主成分分析（Principal Components Analysis, PCA） 半监督学习 当标注数据“昂贵”时有用 例：标注3D姿态、 蛋白质功能等等 多标签学习 有歧义标签学习 多实例学习 增强学习从行为的反馈(奖励或惩罚)中学习 设计一个回报函数（reward function）， 如果learning agent(如机器人、围棋ai程序)，在决定一步之后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负 增强学习的任务：找到一条回报值最大的路径 1.3 一个典型的机器学习案例-对鱼进行分类 根据一些光学传感器对传送带上的鱼进行分类 形式化为机器学习问题 训练数据 每条鱼的测量向量 每条鱼的标签 测试 给定一个新的特征向量x 预测对应的标签y 将长度作为特征进行分类（直方图） 需要先做一个决策边界 最小化平均损失 将亮度作为特征进行分类 （直方图） 将长度和亮度一起作为特征（二维散点图） 线性决策函数 二次决策函数 更复杂的决策边界训练集上的误差 ≠ 测试集上的误差数据过拟合（overfitting）推广性（generalization）差 小结：设计一个鱼的分类器 选择特征 可能是最重要的步骤！（收集训练数据） 选择模型（如决策边界的形状） 根据训练数据估计模型 利用模型对新样本进行分类 1.4 机器学习算法的组成部分 机器学习任务的一般步骤 确定特征 可能是最重要的步骤！（收集训练数据） 确定模型 目标函数/决策边界形状 模型训练：根据训练数据估计模型参数 优化计算 模型评估：在校验集上评估模型预测性能 模型应用/预测 模型 监督学习任务：$D = \{X_i, y_i\} _{i = 1} ^ N $ 模型：对给定的输入x, 如何预测其标签$ \hat y$ 不同模型对数据的假设不同 最简单的模型：线性模型$ f(x) = \sum_j w_j x_j = \bf w^T \bf x$ 确定模型类别后，模型训练转化为求解模型参数 如对线性模型参数为$\theta = \{w_j \mid j = 1,…, D\}$,其中D为特征维数 求解模型参数：目标函数最小化 非线性模型 基函数： $x^2$, log, exp, 样条函数，决策树…. 核化：将原问题转化为对偶问题，将对偶问题中的向量积$\langle x_i, x_j\rangle$ 换成核函数$k(x_i,x_j)$ 目标函数：通常包含两项：损失函数和正则项J(\theta) = \frac 1N \sum_{i=1}^N\ L(f(x_i; \theta), y_i) + R(\theta) 损失函数 损失函数 - 回归 损失函数：度量模型预测值与真值之间的差异 对回归问题：令残差 $r = f(\bf x) - y$ L2损失：连续，但对噪声敏感L_2 (r) = \frac 12 r ^2 L1损失：不连续，对噪声不敏感L_1(r) = |r| Huber 损失： 连续，对噪声不敏感L_\delta (r) = \begin{cases} \frac 12 r^2 & if|r| \le \delta\\ \delta |r| - \frac 12 \delta^2 & if|r| \ge \delta\end{cases} 损失函数 - 分类 损失函数：度量模型预测值与真值之间的差异 对分类问题 0-1损失：$l_{0/1}(y,f(x)) = \begin{cases} 1 &amp; yf(x) \lt 0 \\ 0 &amp; othereise\end{cases}$ logistic损失：亦称负log似然损失 $l_{log}(y,f(x)) = log(1 + exp(-yf(x)))$ 指数损失：$l_{exp}(y,f(x)) = exp(-yf(x))$ 合页损失：$l_{hinge}(y,f(x)) = max(0, 1 - yf(x))$ 正则项 复杂模型（预测）不稳定：方差大 正则项对复杂模型施加惩罚 正则项的必要性例：sin曲线拟合 增加L2正则岭回归：最小化RSS 欠拟合：模型太简单/对复杂性惩罚太多 样本数目增多时，可以考虑更复杂的模型 常见正则项 L2正则: $R(\theta) = \lambda ||\theta||^2_2 = \lambda \sum^D_{j=1} \theta_j^2$ L1正则: $R(\theta) = \lambda |\theta| = \lambda \sum ^D_{j=1}|\theta_j|$ L0正则: $R(\theta) = \lambda||\theta||_ 0$ 非0参数的数目 不好优化，通常用L1正则近似 常见线性模型的损失和正则项组合 L2损失 L1损失 Huber损失 Logistic损失 合页损失 e-insensitive损失 L2正则 岭回归 L2正则 Logistic回归 SVM SVR L1正则 LASSO L1正则 Logistic回归 L2+L1正则 Elastic 模型训练 在训练数据上求目标函数极小值：优化 简单目标函数直接求解 如小数据集上的线性回归 更复杂问题：凸优化 （随机）梯度下降 牛顿法/拟牛顿法 … 梯度下降（Gradient Descent）算法 梯度下降/最速下降算法：快速寻找函数局部极小值 梯度下降算法：求函数J（θ）的最小值 给定初始值$θ^0$ 更新θ，使得J（θ）越来越小 $θ^t = θ^{t-1} - \eta\nabla J(θ)$ ( $\eta$ : 学习率 ) 直到收敛到 / 达到预先设定的最大迭代次数 下降的步伐太小（学习率）非常重要：如果太小，收敛速度慢； 如果太大，可能会出现overshoot the minimum的现象 梯度下降求得的只是局部最小值 二阶导数 &gt; 0, 则目标函数为凸函数，局部极小值即为全局最小值 随机选择多个初始值，得到函数的多个局部极小值点。多个局部极小值点的最小值为函数的全局最小值 梯度下降算法每次学习都使用整个训练集，这样对大的训练数据集合，每次学习时间过长，对大的训练集需要消耗大量的内存。此时可采用随机梯度下降（Stochastic gradient descent, SGD), 每次从训练集中随机选择一部分样本进行学习。 更多（随机）梯度下降算法的改进版 动量（Momentum） Nesterov accelerated gradient (NAG) Adagrad RMSprop Adaptive Moment Estimation (Adam)… 模型选择与模型评估 同一个问题有不同的解决方案 如线性回归 vs. 决策树 哪个更好？ 模型评估与模型选择 在新数据点的预测误差最小 模型评估：已经选定最终的模型，估计它在新数据上的预测误差 模型选择：估计不同模型的性能，选出最好的模型 样本足够多：训练集和校验集 样本不够多：重采样技术来模拟校验集：交叉验证和bootstrap K-折交叉验证 交叉验证（Cross Validation, CV）： 将训练数据分成容量大致相等的K份（通常K = 5/10） 交叉验证估计的误差为：CV(M)= \frac1K \sum ^K_{k = 1} E_k(M) 模型选择 对多个不同模型，计算其对应的误差CV（M）， 最佳模型为CV（M）最小的模型 模型复杂度和泛化误差的关系通常是U形曲线： 1.5 学习环境简介 编程语言 Python 数据处理工具包 Numpy SciPy pandas 数据可视化工具包 Matplotlib Seaborn 机器学习工具包 scikit learn 示例代码：INotebook NumPy NumPy(Numeric Python)是Python的开源数值计算扩展，可用来存储和处理大型矩阵 Numpy包括： N维数组(ndarray) 实用的线性代数、傅里叶变换和随机数生成函数 Numpy和稀疏矩阵运算包SciPy配合使用更加方便 SciPy SciPy是建立在NumPy的基础上、是科学和工程设计的Python工具包，提供统计、优化和数值微积分计算等功能 NumPy 处理$10^6$级别的数据通常没有大问题，但当数据量达到$10^7$级别时速度开始发慢，内存受到限制（具体情况取决于实际内存的大小） 当处理超大规模数据集，比如$10^{10}$级别，且数据中包含大量的0时，可采用稀疏矩阵可显著的提高速度和效率 Pandas(Pandel data structures) Pandas是Python语言的“关系型数据库”数据结构和数据分析工具，非常高效且易于使用 基于NumPy补充了大量数据操作功能，能实现统计、分组、排序、透视表(SQL语句的大部分功能) Pandas主要有2种重要的数据类型 series：一维序列 DataFrame：二维表(机器学习数据的常用数据结构) Matplotlib Matplotlib是Python语言的2D图形绘制工具 Seaborn Seaborn是一个基于Matplotlib的Python可视化工具包，提供更高层次的用户接口，可以给出漂亮的数据统计 Scikit - Learn Machine Learning in Python Scikit-Learn是基于Python的开源机器学习模块，最早于2007年由David Cournapeau发起 基本功能有六部分：分类（Classification），回归（Regression），聚类（Clustering），数据降维（Dimensionality reduction），模型选择（Model Selection），数据预处理（Preprocessing） 对于具体的机器学习问题，通常可以分为三个步骤 数据准备与预处理（Preprocessing, Dimensionality reduction） 模型选择与训练（Classification, Regression, Clustering） 模型验证与参数调优（Model Selection） 各种机器学习模型有统一的接口 模型既有默认参数，也提供多种参数调优方法 卓越的文档 丰富的随附任务功能集合 活跃的社区提供开发和支持 1.6 线性回归模型 目标函数通常包含两项：损失函数和正则项J(\bf \theta) = \frac1N \sum_{i = 1}^N L(f(\bf x_i|\bf \theta), y_i) + \lambda R(\bf \theta) 对回归问题，损失函数可以采用L2损失，得到\begin{eqnarray}J(\theta) &=&\sum_{i=1}^NL(y_i,\hat y_i) \\ &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\ &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 残差平方和（residual sum of squares, RSS） 由于线性模型比较简单，实际应用中有时正则项为空，得到最小二乘线性回归（Ordinary Least Square, OLS）\begin{eqnarray}J(\theta) &=&\sum_{i=1}^NL(y_i,\hat y_i) &=&\sum_{i=1}^N(y_i - \hat y_i)^2\\ &=&\sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 正则项可以为L2正则，得到岭回归（Ridge Regression）J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda ||w||^2_2 正则项也可以选L1正则，得到Lasso模型： J(\bf w) = \sum_{i=1}^N(y_i - \bf w^Tx_i)^2 + \lambda |w| 当$\lambda$取合适值时，Lasso（Least absolute shrinkage and selection operator）的结果是稀疏的（w的某些元素系数为0），起到特征选择作用 为什么L1正则的解是稀疏的？ 线性回归模型的概率解释 最小二乘（线性）回归等价于极大似然估计 假设：$ y = f(\bf x) + \epsilon = w^Tx + \epsilon $其中$\epsilon$为线性预测和真值之间的残差我们通常假设残差的分布为$\epsilon \sim N(0,\sigma ^2)$,因此线性回归可写成：$p(y|x,\theta) \sim N(y| \bf w^T \bf x, \sigma^2)$,其中$ \bf \theta = (\bf w, \sigma ^2)$ 正则（线性）回归等价于高斯先验（L2正则）或Laplace先验下（L1正则）的贝叶斯估计 Recall：极大似然估计 极大似然估计（Maximize Likelihood Estimator, MLE）定义为\hat \theta = \underset {\theta} {arg\ max}\ log\ p(D\mid \theta) 其中（log）似然函数为l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) 表示在参数为$\theta$的情况下，数据$D ={\bf x_i,y_i}^N_{i=1}$ 极大似然：选择数据出现概率最大的参数 线性回归的MLEp(y_i|x_i,\bf w,\sigma ^2) \sim N(y_i\mid \bf w^T \bf x_i, \sigma^2) = \frac 1{\sqrt{2\pi}\sigma} exp(-\frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2)) OLS的似然函数为l(\bf \theta) = log\ p(D\mid \bf \theta) = \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) 极大似然可等价地写成极小负log似然损失（negative log likelihood, NLL） \begin{eqnarray}{NLL(\bf \theta)} &=& \sum_{i=1}^N log\ p(y_i \mid x_i, \bf \theta) \\ &=& - \sum_{i=1}^N log ((\frac 1{2 \pi \sigma^2})^ \frac 12 exp(- \frac 1{2 \sigma ^2}((y_i - \bf w^T \bf x_i)^2))) \\ &=& \frac N2 log(2\pi \sigma ^2) + \frac 1{2 \sigma^2} \sum_{i=1}^N(y_i - \bf w^T \bf x_i)^2 \end{eqnarray} 正则回归等价于贝叶斯估计 假设残差的分布为$\epsilon \sim N(0, \sigma ^2)$,线性回归可写成：$p(y_i \mid \bf x_i, \theta) \sim N(y_i \mid \bf w^T \bf x_i，\sigma ^2)$$p(y\mid \bf X, \bf w, \sigma ^2) = N(\bf y \mid \bf X \bf w, \sigma ^2 \bf I_N) \propto exp(- \frac 1{2\sigma ^2}((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w)))$ 若假设参数为w的先验分布为 $w_j \sim N(0, \tau ^2)$ 偏向较小的系数值，从而得到的曲线也比较平滑$p(\bf w) =\prod_{j=1}^{D} N(w_j \mid 0, \tau ^2) \propto exp(- \frac 1{2\tau^2} \sum_{j=1}^D \bf w_j^2 = exp(- \frac 1{2\tau^2} ( \bf w^T \bf w ) )) $ 其中$1/\tau ^2$控制先验的强度 根据贝叶斯公式，得到参数的后验分布为$p(y\mid \bf X, \bf w, \sigma ^2) = \propto exp(- \frac 1{2\sigma ^2} ((\bf y - \bf X \bf w)^T(\bf y - \bf X \bf w) ) - \frac 1{2 \tau^2} ( w^Tw ) )$ 则最大后验估计(MAP)等价于最小目标函数$J(\bf w) = (\bf y - \bf X\bf w)^T(\bf y - \bf X\bf w) + \frac {\sigma ^2}{\tau^2} \bf w^T \bf w $ 对比岭回归的目标函数$J(\bf w) = \sum_{i=1}^N(y_i -\bf w^T\bf x_i)^2 + \lambda \Vert \bf w\Vert ^2_2$ 小结 线性回归模型可以放到机器学习一般框架 损失函数：L2损失，… 正则：无正则， L2正则，L1正则… 正则回归模型可视为先验为正则、似然为高斯分布的贝叶斯估计 L2正则：先验分布为高斯分布 L1正则：先验分布为Laplace分布 1.7 线性回归模型-优化算法 线性回归的目标函数 无正则的最小二乘线性回归（Ordinary Least Square, OLS）：J(w) = \sum_{i=1}^N(y_i - w^Tx_i)^2 L2正则的岭回归（Ridge Regression）模型：J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D w_j^2 L1正则的Lasso模型：J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D |w_j| 模型训练： 根据训练数据求目标函数取极小值的参数： $\hat w = \underset {w} {arg\ min} J(\bf w)$ 目标函数的最小值： 一阶的导数为0：$\frac{\partial J(w)} {\partial w}$ 二阶导数&gt;0：$\frac{\partial J^2(w)} {\partial w^2}$ OLS的优化求解： OLS的优化求解 OLS的目标函数写成矩阵形式：$J(w) = \sum ^N_{i=1}(y_i - w^Tx_i)^2 = (y - Xw)^T(y - Xw)$ 只取与w有关的项，得到$J(w) = w^T(X^TX)w - 2w^T(X^Ty)$ 求导 $\frac{\partial J(w)} {\partial w} = 2X^TXw - 2X^Ty = 0 \Rightarrow X^TXw = X^Ty$`$\hat w_{OLS} = (X^TX)^{-1}X^Ty$ OLS的优化求解 ——SVD OLS目标函数：$J(w) = \Vert y - Xw\Vert_2^2$ 相当于求 $y = Xw$ 如果X为方阵，可求逆：$w = X^{-1}y$ 如果𝐗不是方阵，可求Moore-Penrose广义逆：$𝐰 = 𝐗^{\dagger }𝐲$。 Moore-Penrose广义逆可采用奇异值分解(Singular Value Decomposition)实现：奇异值分解：$X = U \Sigma V^T$$X^{\dagger } = V \Sigma ^{\dagger} U^T$其中 $\Sigma = \begin{pmatrix}{\sigma_1}&amp;{0}&amp;{\cdots}&amp;{0}\\{0}&amp;{\sigma_2}&amp;{\cdots}&amp;{0}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{0}&amp;{0}&amp;{\cdots}&amp;{0}\\\end{pmatrix}$,$\Sigma ^{\dagger} = \begin{pmatrix}{\frac {1}{\sigma_1}}&amp;{0}&amp;{\cdots}&amp;{0}\\{0}&amp;{\frac{1}{\sigma_2}}&amp;{\cdots}&amp;{0}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{0}&amp;{0}&amp;{\cdots}&amp;{0}\\\end{pmatrix}$ OLS的优化求解——梯度下降 OLS目标函数：$J(w) = (y - Xw)^T(y - Xw)$梯度：$\nabla_w = - 2X^T(y - Xw^t)$参数更新：$w^{t+1} = w^t - \eta\nabla_w = w^t + 2\eta X^T(y - Xw^t)$ 岭回归的优化求解 岭回归的目标函数与OLS只相差一个正则项（也是w的二次函数） 岭回归的优化求解——SVD Lasso的优化条件 软&amp; 硬阈值 Lasso的优化求解——坐标轴下降法 为了找到一个函数的局部极小值，在每次迭代中可以在当前点处沿一个坐标方向进行一维搜索。 整个过程中循环使用不同的坐标方向。一个周期的一维搜索迭代过程相当于一个梯度迭代。 注意： 梯度下降方法是利用目标函数的导数（梯度）来确定搜索方向的，而该梯度方向可能不与任何坐标轴平行。 而坐标轴下降法是利用当前坐标系统进行搜索，不需要求目标函数的导数，只按照某一坐标方向进行搜索最小值。（在稀疏矩阵上的计算速度非常快，同时也是Lasso回归最快的解法） 小结 线性回归模型比较简单 当数据规模比较小时，可直接解析求解 scikit learn中的实现采用SVD分解实现 当数据规模较大时，可采用随机梯度下降 scikit learn提供一个SGDRegression类 岭回归求解类似OLS，采用SVD分解实现 Lasso优化求解采用坐标轴下降法 1.8 线性回归模型-模型选择 模型评估与模型选择 模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果 校验集划分（train_test_split、交叉验证） 评价指标（sklearn.metrics） 模型选择： 模型中通常有一些超参数，需要通过模型选择来确定 线性回归模型中的正则参数 OLS中的特征的数目 参数搜索范围：网格搜索（GridSearch） Scikit learn将交叉验证与网格搜索合并为一个函数 评价准则 模型训练好后，可用一些度量准则检查模型拟合的效果 开方均方误差（rooted mean squared error，RMSE）:$RMSE = \sqrt{\frac 1N \sum_{i=1}^N(\hat y_i - y_i)^2}$` 平均绝对误差（mean absolute error，MAE）：$MAE = \frac 1N \sum_{i=1}^N|\hat y_i - y_i|$ R2 score：既考虑了预测值与真值之间的差异，也考虑了问题本身真值之间的差异（ scikit learn 线性回归模型的缺省评价准则）$SS_{res} = \sum_{i=1}^N(\hat y_i - y_i)^2, SStot = \sum_{i=1}^N(y_i - \bar{y})^2, R^2 = 1 - \frac {SS_{res}}{SS_{tot}})$ 也可以检查残差的分布 还可以打印预测值与真值的散点图 线性回归中的模型选择Scikit learn中的model selection模块提供模型选择功能 对于线性模型，留一交叉验证（N折交叉验证，亦称为leave-oneout cross-validation，LOOCV）有更简便的计算方式，因此Scikit learn提供了RidgeCV类和LassoCV类实现了这种方式 后续课程将讲述一般模型的交叉验证和参数调优GridSearchCV RidgeCV RidgeCV中超参数λ用alpha表示 RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False) LassoCV LassoCV的使用与RidgeCV类似 Scikit learn还提供一个与Lasso类似的LARS（least angle regression，最小角回归），二者仅仅是优化方法不同，目标函数相同。 当数据集中特征维数很多且存在共线性时，LassoCV更合适。 小结：线性回归之模型选择 采用交叉验证评估模型预测性能，从而选择最佳模型 回归性能的评价指标 线性模型的交叉验证通常直接采用广义线性模型的留一交叉验证进行快速模型评估 Scikit learn中对RidgeCV和LassoCV实现该功能 1.9 波士顿房价预测案例详解——数据探索 第一步：理解任务，准备数据 数据读取 Pandas支持多种格式的数据 数据探索&amp;特征工程 数据规模 确定数据类型，是否需要进一步编码 特征编码 数据是否有缺失值 数据填补 查看数据分布，是否有异常数据点 离群点处理 查看两两特征之间的关系，看数据是否有冗余/相关 降维 数据概览 pandas:DataFrame Head():数据前5行，可查看每一列的名字及数据类型 Info(): 数据规模：行数&amp;列数 每列的数据类型、是否有空值 占用存储量 shape:行数&amp;列数 各属性的统计特性 直方图 每个取值在数据集中出现的样本数目 离群点 离群点：奇异点（outlier）,指远离大多数样本的样本点。通常认为这些点是噪声，对模型有坏影响 相关性 相关性：相关性可以通过计算相关系数或打印散点图来发现 相关系数： 散点图 可以通过两个变量之间的散点图直观感受二者的相关性 数据预处理 数据标准化（ Standardization ） 某个特征的所有样本取值为0均值、1方差 数据归一化（ Scaling ） 某个特征的所有样本取值在规定范围内 数据正规化（ Normalization ） 每个样本模长为1 数据二值化 根据特征值取值是否大于阈值将特征值变为0或1，可用类Binarizer 实现 数据缺失 数据类型变换 有些模型只能处理数值型数据。如果给定的数据是不同的类型，必须先将数据变成数值型。 第二步：模型确定和模型训练 1、确定模型类型 目标函数（损失函数、正则） 2、模型训练 优化算法（解析法，梯度下降、随机梯度下降…） 第三步：模型评估与模型选择 模型训练好后，需要在校验集上采用一些度量准则检查模型预测的效果 校验集划分（train_test_split、交叉验证） 评价指标 （sklearn.metics） 也可以检查残差的分布 还可以打印预测值与真值的散点图 模型选择：选择预测性能最好的模型 模型中通常有一些超参数，需要通过模型选择来确定 参数搜索范围：网格搜索（GridSearch） 1.10 波士顿房价预测-数据探索代码python 3.712345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns# 读入数据data = pd.read_csv("boston_housing.csv")# 数据探索print(data.head())data.info()print(data.isnull().sum())print(data.describe())# 目标y(房屋价格)的直方图/分布fig = plt.figure()sns.distplot(data.MEDV.values, bins=30, kde=True)plt.xlabel('Median value of owner_occupied homes', fontsize=12)plt.show()# 单个特征散点图plt.scatter(range(data.shape[0]), data["MEDV"].values, color='purple')plt.title("Distribution of Price")plt.show()# 删除y大于50的样本data = data[data.MEDV &lt; 50]print(data.shape)# 输入属性的直方图／分布# 犯罪率特征fig = plt.figure()sns.distplot(data.CRIM.values, bins=30, kde=False)plt.xlabel('crime rate', fontsize=12)plt.show()# 是否靠近charles riversns.countplot(data.CHAS, order=[0, 1]);plt.xlabel('Charles River');plt.ylabel('Number of occurrences');plt.show()# 靠近高速sns.countplot(data.RAD)plt.xlabel('index of accessibility to radial highways')plt.show()# 两两特征之间的相关性# 获得所有列的名字cols = data.columns# 计算相关性data_corr = data.corr().abs()# 相关性热图plt.subplots(figsize=(13, 9))sns.heatmap(data_corr, annot=True)sns.heatmap(data_corr, mask=data_corr &lt; 1, cbar=False)plt.savefig('house_coor.png')plt.show()# 输出强相关对threshold = 0.5corr_list = []size = data_corr.shape[0]for i in range(0, size): for j in range(i+1, size): if (data_corr.iloc[i,j] &gt;= threshold and data_corr.iloc[i, j] &lt; 1) or (data_corr.iloc[i, j] &lt; 0 and data_corr.iloc &lt;= -threshold): corr_list.append([data_corr.iloc[i, j], i, j])s_corr_list = sorted(corr_list, key=lambda x: -abs(x[0]))for v, i, j in s_corr_list: print("%s and %s = %.2f" % (cols[i], cols[j], v))for v, i, j in s_corr_list: sns.pairplot(data, height=6, x_vars=cols[i], y_vars=cols[j]) plt.show() 1.11 波士顿房价预测案例详解1.12 波士顿房价预测案例详解-代码讲解python 3.7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174# 波士顿房价预测案例——线性回归分析import numpy as np # 矩阵操作import pandas as pd # SQL数据处理from sklearn.metrics import r2_score # 评价回归预测模型的性能import matplotlib.pyplot as plt # 画图import seaborn as sns# 读入数据data = pd.read_csv("boston_housing.csv")# 1、数据准备# 从原始数据中分离输入特征x和输出yy = data['MEDV'].valuesX = data.drop('MEDV', axis=1)# 用于后续显示权重系数对应的特征columns = X.columns# 数据较少，将数据分割训练数据from sklearn.model_selection import train_test_split# 随机采样20%的数据构建测试样本，其余作为训练样本X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33,test_size=0.2)# print(X_train.shape)# 2、数据预处理/特征工程# 数据标准化from sklearn.preprocessing import StandardScaler# 分别初始化对特征和目标值的标准化器ss_X = StandardScaler()ss_y = StandardScaler()# 分别对训练和测试数据的特征以及目标值进行标准化处理X_train = ss_X.fit_transform(X_train)X_test = ss_X.transform(X_test)# 对y标准化不是必须# 对y标准化的好处是不同的问题的w差异不太大，同时正则参数的范围也有限y_train = ss_y.fit_transform(y_train.reshape(-1, 1))y_test = ss_y.transform(y_test.reshape(-1, 1))# 3、确定模型类型# 3.1 尝试缺省参数的线性回归# 线性回归from sklearn.linear_model import LinearRegression# 使用默认配置初始化lr = LinearRegression()# 训练模型参数lr.fit(X_train, y_train)# 预测y_test_pred_lr = lr.predict(X_test)y_train_pred_lr = lr.predict(X_train)# 看看各特征的权重系数，系数的绝对值大小可视为该特征的重要性fs = pd.DataFrame(&#123;"columns": list(columns), "coef": list((lr.coef_.T))&#125;)fs.sort_values(by=['coef'], ascending=False)print(fs)# 模型评价# 测试集print('The r2 score of LinearRegression on test is', r2_score(y_test, y_test_pred_lr))# 训练集print('The r2 score of LinearRegression on train is', r2_score(y_train, y_train_pred_lr))# 在训练集上观察残差的分布，看是否符合模型假设：噪声为0均值的高斯噪声f, ax = plt.subplots(figsize=(7, 5))f.tight_layout()ax.hist(y_train - y_train_pred_lr, bins=40, label='Residuals Linear', color='b', alpha=.5)ax.set_title("Histogram of Residuals")ax.legend(loc='best')plt.show()# 还可以观察预测值与真值的散点图plt.figure(figsize=(4, 3))plt.scatter(y_train, y_train_pred_lr)plt.plot([-3, 3],[-3, 3], '--k')plt.axis('tight')plt.xlabel('True price')plt.ylabel('Predicted price')plt.tight_layout()plt.show()# 线性模型，随机梯度下降优化模型参数# 随机梯度下降一般在大数据集上应用，其实本项目不适合用from sklearn.linear_model import SGDRegressor# 使用默认配置初始化线sgdr = SGDRegressor(max_iter=1000)# 训练：参数估计sgdr.fit(X_train, y_train)# 预测sgdr.coef_print('The value of default measurement of SGDRegressor on test is', sgdr.score(X_test, y_test))print('The value of default measurement of SGDRegressor on train is', sgdr.score(X_train, y_train))# 3.2 正则化的线性回归（L2正则--&gt;岭回归）from sklearn.linear_model import RidgeCV# 设置超参数（正则参数）范围alphas = [0.01, 0.1, 1, 10, 100]# 生成一个RidgeCVridge = RidgeCV(alphas=alphas, store_cv_values=True)# 模型训练ridge.fit(X_train, y_train)# 预测y_test_pred_ridge = ridge.predict(X_test)y_train_pred_ridge = ridge.predict(X_train)# 评估，使用r2_score评价模型在测试集和训练集上的性能print('The r2 score of RidgeCV on test is', r2_score(y_test, y_test_pred_ridge))print('The r2 score of RidgeCV on test is', r2_score(y_train, y_train_pred_ridge))# 可视化mse_mean = np.mean(ridge.cv_values_, axis=0)plt.plot(np.log10(alphas), mse_mean.reshape(len(alphas), 1))# plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])plt.xlabel('log(alpha)')plt.ylabel('mse')plt.show()print('alpha is:', ridge.alpha_)# 看看各特征的权重系数，系数的绝对值大小可视为该特制的重要性fs = pd.DataFrame(&#123;"columns": list(columns), "coef_lr": list(lr.coef_.T), "coef_ridge": list(ridge.coef_.T)&#125;)fs.sort_values(by=['coef_lr'], ascending=False)print(fs)# 3.3 正则化的线性回归（L1正则--&gt;Lasso）from sklearn.linear_model import LassoCV# 生成一个LassoCV实例lasso = LassoCV()# 训练（内含CV）lasso.fit(X_train, y_train)# 测试y_test_pred_lasso = lasso.predict(X_test)y_train_pred_lasso = lasso.predict(X_train)# 评估， 使用r2_score评价模型在测试集和训练集上的性能print('The r2 score of LassoCV on test is', r2_score(y_test, y_test_pred_lasso))print('The r2 score of LassoCV on train is', r2_score(y_train, y_train_pred_lasso))# 可视化mses = np.mean(lasso.mse_path_, axis=1)plt.plot(np.log10(lasso.alphas_), mses)# plt.plot(np.log10(ridge.alpha_)*np.ones(3), [0.28, 0.29, 0.30])plt.xlabel('log(alpha)')plt.ylabel('mse')plt.show()print('alpha is:', lasso.alpha_)# 看看各特征的权重系数，系数的绝对值大小可视为该特制的重要性fs = pd.DataFrame(&#123;"columns": list(columns), "coef_lr": list(lr.coef_.T), "coef_ridge": list(lasso.coef_.T)&#125;)fs.sort_values(by=['coef_lr'], ascending=False)print(fs)]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>学习笔记</tag>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉基础入门 学习笔记]]></title>
    <url>%2F2019%2F03%2F30%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[一、 计算机视觉和深度学习概述 计算机视觉回顾 计算机视觉（computer vision）定义 数据（静态图片，视频） 算法（机器学习算法，神经网络）本质上是一个回归+分类 计算机视觉的重要性 三大任务：图像识别（image classification）车牌识别，人脸识别 三大任务：目标检测（object detection = classification + localization）行人检测和车辆检测 三大任务：图像分割图像语义分割个体分割 = 检测 + 分割 视觉目标跟踪（tracking） 视频分割 图像风格迁移 生成对抗网络（GAN） 视频生成 深度学习介绍 2006 Hinton bp(反向传播) 2012 Krizhevsky A 深度学习 深度卷积 RNN LSTM 持续信息 视觉识别，语音识别，DeepMind, AlphaGo 人脸识别：LFW 错误率5% -&gt; 0.5% 图像分割 VGGNet, GoogleNet, ResNet, DenseNet 常见的深度学习开发平台 Torch, TensorFlow, MatConvNetTheano, Caffe 课程介绍 图像识别：Alexnet, VGGnet, GoogleNet, ResNet, DenseNet 目标检测Fast-rcnn, faster-rcnn, Yolo, Retina-Net 图像分割FCN, Mask-Rcnn 目标跟踪GORURN， ECO 图像生成GAN， WGAN 光流FlowNet 视频分割Segnet 二、 图像分类与深度卷积网络的模型 图像分类 图像分类的挑战光照变化形变类内变化 图像分类定义 目标分类框架 泛化能力如何提高泛化能力？ 需要用图像特征来描述图像 训练和测试的流程 图像特征 color: Qutantize RGB values global shape: PCA space local shape: shape context texture: Filter banks SIFT, Hog, LBP, Harr 支持向量机（SVM） 超平面与支持向量 最大化间隔 svm分类（python）以lris兰花分类为例 程序实现 更好的特征 CNN特征 学习出来的 如何学习？ 构造神经网络 神经网络原理 神经网络做图像分类 神经网络搭建 神经网络的基本单元：神经元 激励函数 Sigmoid、tanh、ReLU、Leaky ReLU、Maxout、ELU 卷积层 卷积滤波的计算 卷积层可视化 池化层（pooling layer） 特征表达更加紧凑，同时具有位移不变性 全连接层 损失函数 交叉熵损失函数（SIGMOID_CROSS_ENTROPY_LOSS) 应用于二分类问题 Softmax 损失函数（SOFTMAX_LOSS) 多分类问题 欧式距离损失函数（EUCLIDEAN_LOSS）回归问题 对比损失函数（Contrastive loss）用来计算两个图像之间的相似度 Triplet loss 训练网络 网络训练和测试 卷积神经网络介绍 Alexnet, VGGnet, GoogleNet, ResNet, DenseNet 训练技巧， 防止过拟合（泛化能力不强） 数据增强（Data augmentation） 水平翻转， 随机裁剪和平移变换，颜色、光照变换 Dropout 其他有助于训练的手段 L1， L2正则化 Batch Normalization 利用caffe搭建深度网络做图像分类]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>计算机视觉</tag>
        <tag>基础</tag>
      </tags>
  </entry>
</search>
